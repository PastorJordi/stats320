{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9S5M08pFAJxy"
   },
   "source": [
    "# Lab 8: Latent Variable Models, Variational EM, and Worm Brains\n",
    "\n",
    "**STATS320: Machine Learning Methods for Neural Data Analysis**\n",
    "\n",
    "_Stanford University. Winter, 2021._\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/slinderman/stats320/blob/main/labs/STAT220_320_Lab_8_Latent_Variable_Models,_Variational_EM,_and_Worm_Brains.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Team Name:** _Your team name here_\n",
    "\n",
    "**Team Members:** _Names of everyone on your team here_\n",
    "\n",
    "*Due: 11:59pm Thursday, March 12, 2021 via GradeScope*\n",
    "\n",
    "---\n",
    "![](https://els-jbs-prod-cdn.jbs.elsevierhealth.com/cms/attachment/09c7213d-d3ef-4c84-8321-07ab84a365af/fx1.jpg)\n",
    "\n",
    "In our last lab of the course, we'll implement a variational expectation maximization algorithm to fit a latent variable model with both discrete and continuous states. We'll use a mean field approximation, which we'll fit using coordinate ascent variational inference (CAVI). Then we'll test it out on neural activity traces extracted from calcium imaging of the worm _C. elegans_ by Kato et al (2015), in their paper on low dimensional dynamics of whole brain activity. \n",
    "\n",
    "We won't implement variational EM for full-blown hierachical, recurrent, switching linear dynamical systems (Linderman et al, 2019). Instead, we'll work on a simpler model without time dependencies, which reduces to a mixture of factor analysis models. Once we've done so, you'll understand how the main fitting algorithms underlying [SSM](https://github.com/lindermanlab/ssm) work under the hood!\n",
    "\n",
    "\n",
    "## References\n",
    "Kato, Saul, Harris S. Kaplan, Tina Schrödel, Susanne Skora, Theodore H. Lindsay, Eviatar Yemini, Shawn Lockery, and Manuel Zimmer. 2015. “Global Brain Dynamics Embed the Motor Command Sequence of Caenorhabditis Elegans.” Cell 163 (3): 656–69.\n",
    "\n",
    "Linderman, Scott W., Annika L. A. Nichols, David M. Blei, Manuel Zimmer, and Liam Paninski. 2019. “Hierarchical Recurrent State Space Models Reveal Discrete and Continuous Dynamics of Neural Activity in C. Elegans.” bioRxiv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBtV5wMQDDfP"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5lkccXmMxZJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.io import loadmat\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.auto import trange\n",
    "\n",
    "# PyTorch \n",
    "import torch\n",
    "from torch.distributions import Categorical, MultivariateNormal, Normal, \\\n",
    "    LowRankMultivariateNormal, kl_divergence\n",
    "\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.float32\n",
    "\n",
    "# Helper function to convert between numpy arrays and tensors\n",
    "to_t = lambda array: torch.tensor(array, device=device, dtype=dtype)\n",
    "from_t = lambda tensor: tensor.to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fDxAL0XkRVGR"
   },
   "outputs": [],
   "source": [
    "#@title Helper Functions (run this cell!)\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "kato_files = [\"TS20140715e_lite-1_punc-31_NLS3_2eggs_56um_1mMTet_basal_1080s.mat\",\n",
    "              \"TS20140715f_lite-1_punc-31_NLS3_3eggs_56um_1mMTet_basal_1080s.mat\",\n",
    "              \"TS20140905c_lite-1_punc-31_NLS3_AVHJ_0eggs_1mMTet_basal_1080s.mat\",\n",
    "              \"TS20140926d_lite-1_punc-31_NLS3_RIV_2eggs_1mMTet_basal_1080s.mat\",\n",
    "              \"TS20141221b_THK178_lite-1_punc-31_NLS3_6eggs_1mMTet_basal_1080s.mat\"]\n",
    "\n",
    "kato_dir = '.'\n",
    "\n",
    "# Set notebook plotting defaults\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "# initialize a color palette for plotting\n",
    "palette = sns.xkcd_palette([\"light blue\",   # forward\n",
    "                            \"navy\",         # slow\n",
    "                            \"orange\",       # dorsal turn\n",
    "                            \"yellow\",       # ventral turn\n",
    "                            \"red\",          # reversal 1\n",
    "                            \"pink\",         # reversal 2\n",
    "                            \"green\",        # sustained reversal\n",
    "                            \"greyish\"])     # no state\n",
    "\n",
    "def load_kato_labels():\n",
    "    zimmer_state_labels = \\\n",
    "        loadmat(os.path.join(\n",
    "            kato_dir,\n",
    "            \"sevenStateColoring.mat\"))\n",
    "    return zimmer_state_labels\n",
    "\n",
    "def load_kato_key():\n",
    "    data = load_kato_labels()\n",
    "    key = data[\"sevenStateColoring\"][\"key\"][0,0][0]\n",
    "    key = [str(k)[2:-2] for k in key]\n",
    "    return key\n",
    "\n",
    "\n",
    "def _get_neuron_names(neuron_ids_1, neuron_ids_2, worm_name):\n",
    "    # Remove the neurons that are not uniquely identified\n",
    "    def check_label(neuron_name):\n",
    "        if neuron_name is None:\n",
    "            return False\n",
    "        if neuron_name == \"---\":\n",
    "            return False\n",
    "\n",
    "        neuron_index = np.where(neuron_ids_1 == neuron_name)[0]\n",
    "        if len(neuron_index) != 1:\n",
    "            return False\n",
    "\n",
    "        if neuron_ids_2[neuron_index[0]] is not None:\n",
    "            return False\n",
    "\n",
    "        # Make sure it doesn't show up in the second neuron list\n",
    "        if len(np.where(neuron_ids_2 == neuron_name)[0]) > 0:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    final_neuron_names = []\n",
    "    for i, neuron_name in enumerate(neuron_ids_1):\n",
    "        if check_label(neuron_name):\n",
    "            final_neuron_names.append(neuron_name)\n",
    "        else:\n",
    "            final_neuron_names.append(\"{}_neuron{}\".format(worm_name, i))\n",
    "\n",
    "    return final_neuron_names\n",
    "\n",
    "\n",
    "def load_kato(index, sample_rate=3, name=\"unnamed\"):\n",
    "    filename = os.path.join(kato_dir, kato_files[index])\n",
    "    zimmer_data = loadmat(filename)\n",
    "\n",
    "    # Get the neuron names\n",
    "    neuron_ids = zimmer_data[\"wbData\"]['NeuronIds'][0, 0][0]\n",
    "    neuron_ids_1 = np.array(\n",
    "        list(map(lambda x: None if len(x[0]) == 0\n",
    "                            else str(x[0][0][0]),\n",
    "            neuron_ids)))\n",
    "\n",
    "    neuron_ids_2 = np.array(\n",
    "        list(map(lambda x: None if x.size < 2 or x[0, 1].size == 0\n",
    "                            else str(x[0, 1][0]),\n",
    "            neuron_ids)))\n",
    "\n",
    "    all_neuron_names = _get_neuron_names(neuron_ids_1, neuron_ids_2, name)\n",
    "\n",
    "    # Get the calcium trace (corrected for bleaching)\n",
    "    t_smpl = np.ravel(zimmer_data[\"wbData\"]['tv'][0, 0])\n",
    "    t_start = t_smpl[0]\n",
    "    t_stop = t_smpl[-1]\n",
    "    tt = np.arange(t_start, t_stop, step=1./sample_rate)\n",
    "    def interp_data(xx, kind=\"linear\"):\n",
    "        f = interp1d(t_smpl, xx, axis=0, kind=kind)\n",
    "        return f(tt)\n",
    "        # return np.interp(tt, t_smpl, xx, axis=0)\n",
    "\n",
    "    dff = interp_data(zimmer_data[\"wbData\"]['deltaFOverF'][0, 0])\n",
    "    dff_bc = interp_data(zimmer_data[\"wbData\"]['deltaFOverF_bc'][0, 0])\n",
    "    dff_deriv = interp_data(zimmer_data[\"wbData\"]['deltaFOverF_deriv'][0, 0])\n",
    "\n",
    "    # Kato et al smoothed the derivative.  Let's just work with the first differences\n",
    "    # of the bleaching corrected and normalized dF/F\n",
    "    dff_bc_zscored = (dff_bc - dff_bc.mean(0)) / dff_bc.std(0)\n",
    "    dff_diff = np.vstack((np.zeros((1, dff_bc_zscored.shape[1])),\n",
    "                                np.diff(dff_bc_zscored, axis=0)))\n",
    "\n",
    "    # Get the state sequence as labeled in Kato et al\n",
    "    # Interpolate to get at new time points\n",
    "    labels = load_kato_labels()\n",
    "    labels = labels[\"sevenStateColoring\"][\"dataset\"][0, 0]['stateTimeSeries']\n",
    "    states = interp_data(labels[0, index].ravel() - 1, kind=\"nearest\").astype(int)\n",
    "    \n",
    "    # Only keep the neurons with names\n",
    "    has_name = np.array([not name.startswith(\"unnamed\") for name in all_neuron_names])\n",
    "    y = dff_bc[:, has_name]\n",
    "    neuron_names = [name for name, valid in zip(all_neuron_names, has_name) if valid]\n",
    "\n",
    "    # Load the state names from Kato et al\n",
    "    state_names=load_kato_key()\n",
    "    return dict(neuron_names=neuron_names, \n",
    "                y=to_t(y), \n",
    "                z_kato=states, \n",
    "                state_names=state_names,\n",
    "                fps=3)\n",
    "\n",
    "\n",
    "def gradient_cmap(colors, nsteps=256, bounds=None):\n",
    "    # Make a colormap that interpolates between a set of colors\n",
    "    ncolors = len(colors)\n",
    "    if bounds is None:\n",
    "        bounds = np.linspace(0,1,ncolors)\n",
    "\n",
    "    reds = []\n",
    "    greens = []\n",
    "    blues = []\n",
    "    alphas = []\n",
    "    for b,c in zip(bounds, colors):\n",
    "        reds.append((b, c[0], c[0]))\n",
    "        greens.append((b, c[1], c[1]))\n",
    "        blues.append((b, c[2], c[2]))\n",
    "        alphas.append((b, c[3], c[3]) if len(c) == 4 else (b, 1., 1.))\n",
    "\n",
    "    cdict = {'red': tuple(reds),\n",
    "             'green': tuple(greens),\n",
    "             'blue': tuple(blues),\n",
    "             'alpha': tuple(alphas)}\n",
    "\n",
    "    cmap = LinearSegmentedColormap('grad_colormap', cdict, nsteps)\n",
    "    return cmap\n",
    "\n",
    "\n",
    "def states_to_changepoints(z):\n",
    "    assert z.ndim == 1\n",
    "    return np.concatenate(([0], 1 + np.where(np.diff(z))[0], [z.size - 1]))\n",
    "\n",
    "\n",
    "def plot_2d_continuous_states(x, z, \n",
    "                              colors=palette,\n",
    "                              ax=None,\n",
    "                              inds=(0,1),\n",
    "                              figsize=(2.5, 2.5),\n",
    "                              **kwargs):\n",
    "\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "    cps = states_to_changepoints(z)\n",
    "\n",
    "    # Color denotes our inferred latent discrete state\n",
    "    for cp_start, cp_stop in zip(cps[:-1], cps[1:]):\n",
    "        ax.plot(x[cp_start:cp_stop + 1, inds[0]],\n",
    "                x[cp_start:cp_stop + 1, inds[1]],\n",
    "                 '-', color=colors[z[cp_start]],\n",
    "                **kwargs)\n",
    "\n",
    "cmap = gradient_cmap(palette)\n",
    "\n",
    "\n",
    "def plot_elbos(avg_elbos, marginal_ll=None):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    if marginal_ll is not None:\n",
    "        axs[0].hlines(marginal_ll, 0, len(avg_elbos), \n",
    "                colors='k', linestyles=':', label=\"$\\log p(y \\mid \\Theta)$\")\n",
    "    axs[0].plot(avg_elbos, label=\"$\\mathcal{L}[q, \\Theta]$\")\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_xlabel(\"Iteration\")\n",
    "    axs[0].set_ylabel(\"ELBO\")\n",
    "\n",
    "    if marginal_ll is not None:\n",
    "        axs[1].hlines(marginal_ll, 1, len(avg_elbos), \n",
    "                colors='k', linestyles=':', label=\"$\\log p(y \\mid \\Theta)$\")\n",
    "    axs[1].plot(torch.arange(1, len(avg_elbos)), avg_elbos[1:], \n",
    "                label=\"$\\mathcal{L}[q, \\Theta]$\")\n",
    "    axs[1].set_xlabel(\"Iteration\")\n",
    "    axs[1].set_ylabel(\"ELBO\")\n",
    "\n",
    "    axs[2].plot(avg_elbos[1:] - avg_elbos[:-1])\n",
    "    axs[2].set_xlabel(\"Iteration\")\n",
    "    axs[2].set_ylabel(\"Change in ELBO\")\n",
    "    axs[2].hlines(0, 0, len(avg_elbos) - 1, \n",
    "                colors='k', linestyles=':')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIBIRr2QFzN3"
   },
   "source": [
    "# Warm up: Build the generative model\n",
    "\n",
    "To start, we'll consider a model that has both discrete and continuous latent variables, just like a switching linear dynamical system, but we'll get rid of the time dependencies. Let $z_t \\in \\{1,\\ldots, K\\}$ denote a discrete latent state, $x_t \\in \\mathbb{R}^D$ denote a continuous latent state, and $y_t \\in \\mathbb{R}^N$ denote an observed data point. The model is,\n",
    "\\begin{align}\n",
    "p(z, x, y \\mid \\Theta) &= \\prod_{t=1}^T p(z_t \\mid \\Theta) \\, p(x_t \\mid z_t, \\Theta) \\, p(y_t \\mid x_t, \\Theta) \\\\\n",
    "&= \\prod_{t=1}^T \\mathrm{Cat}(z_t \\mid \\pi) \\, \\mathcal{N}(x_t \\mid b_{z_t}, Q_{z_t}) \\, \\mathcal{N}(y_t \\mid C x_t + d, R) \n",
    "\\end{align}\n",
    "where the parameters $\\Theta$ consist of,\n",
    "- $\\pi \\in \\Delta_K$, a distribution on discrete states\n",
    "- $b_k \\in \\mathbb{R}^D$, a mean for each discrete state\n",
    "- $Q_k \\in \\mathbb{R}^{D \\times D}$, a covariance for each discrete state\n",
    "- $C \\in \\mathbb{R}^{N \\times D}$, an _observation matrix_\n",
    "- $d \\in \\mathbb{R}^{N}$, an _observation bias_\n",
    "- $R = \\mathrm{diag}([r_1^2, \\ldots, r_N^2])$, a diagonal observation coariance matrix.\n",
    "\n",
    "This is called a **mixture of factor analyzers** since each $p(y, x \\mid z, \\Theta)$ is a factor analysis model. We also recognize it as an analogue of the switching linear dynamical system without any temporal dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6FsPCGYD59F"
   },
   "source": [
    "## Make a Linear Regression Distribution object\n",
    "\n",
    "We'll be using PyTorch Distributions for this lab. PyTorch doesn't include conditional distributions like $p(y \\mid x)$, so we've written a lightweight object to encapsulate the parameters of the linear Gaussian observation model as well. We call it an `IndependentLinearRegression` because the observation covariance $R$ is a diagonal matrix, which implies independent noise across each output dimension.  This is similar to what you wrote in Lab 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDSxDTLiSLGU"
   },
   "outputs": [],
   "source": [
    "class IndependentLinearRegression(object):\n",
    "    \"\"\"\n",
    "    An object that encapsulates the weights and covariance of a linear \n",
    "    regression. It has an interface similar to that of PyTorch Distributions.\n",
    "    \"\"\"\n",
    "    def __init__(self, weights, bias, diag_covariance):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: N x D tensor of regression weights\n",
    "        bias: N tensor of regression bias\n",
    "        diag_covariance: N tensor of non-negative variances\n",
    "        \"\"\"\n",
    "        self.data_dim, self.covariate_dim = weights.shape[-2:]\n",
    "        assert bias.shape[-1] == self.data_dim\n",
    "        assert diag_covariance.shape[-1] == self.data_dim\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.diag_covariance = diag_covariance\n",
    "\n",
    "    def log_prob(self, data, covariates):\n",
    "        \"\"\"\n",
    "        Compute the log probability of the data given the covariates using the \n",
    "        model parameters. Note that this function's signature is slightly \n",
    "        different from what you implemented in Lab 7.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: a tensor with lagging dimension $N$, the dimension of the data.\n",
    "        covariates: a tensor with lagging dimension $D$, the covariate dimension\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        lp: a tensor of log likelihoods for each data point and covariate pair.\n",
    "        \"\"\"\n",
    "        predictions = torch.einsum('...d,nd->...n', covariates, self.weights) \n",
    "        predictions += self.bias\n",
    "        lkhd = Normal(predictions, torch.sqrt(self.diag_covariance))\n",
    "        return lkhd.log_prob(data).sum(axis=-1)\n",
    "\n",
    "    def sample(self, covariates):\n",
    "        \"\"\"\n",
    "        Sample data points given covariates.\n",
    "        \"\"\"\n",
    "        predictions = torch.einsum('...d,nd->...n', covariates, self.weights) \n",
    "        predictions += self.bias\n",
    "        lkhd = Normal(predictions, torch.sqrt(self.diag_covariance))\n",
    "        return lkhd.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAdg8J2fSDEw"
   },
   "source": [
    "## Make a mixture of factor analyzers object\n",
    "\n",
    "To get you started, we've written a `MixtureOfFactorAnalyzers` object that encapsulates the generative model. It's built out of `torch.distributions.Distribution` objects, which represent the distributions in the generative model. You're already familiar with the `MultivariateNormal` distribution object, which we will use to represent both $p(x \\mid z)$. We also use the `Categorical` distribution object to represent $p(z)$. We'll take advantage of the distribution objects' broadcasting capability to combine all the conditional distributions $p(x \\mid z=k)$ into one object by using a batch of means and covariances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijbVXU12nYXh"
   },
   "outputs": [],
   "source": [
    "class MixtureOfFactorAnalyzers(object):\n",
    "    def __init__(self, num_states, latent_dim, data_dim, scale=1):\n",
    "        self.num_states = num_states\n",
    "        self.latent_dim = latent_dim\n",
    "        self.data_dim = data_dim\n",
    "        \n",
    "        # Initialize the discrete state prior p(z)\n",
    "        self.p_z = Categorical(logits=torch.zeros(num_states))\n",
    "\n",
    "        # Initialize the conditional distributions p(x | z)\n",
    "        self.p_x = MultivariateNormal(\n",
    "            scale * torch.randn(num_states, latent_dim), \n",
    "            torch.eye(latent_dim).repeat(num_states, 1, 1))\n",
    "\n",
    "        # Initialize the observation model p(y | x)\n",
    "        self.p_y = IndependentLinearRegression(\n",
    "            torch.randn(data_dim, latent_dim),\n",
    "            torch.randn(data_dim),\n",
    "            torch.ones(data_dim)\n",
    "        )\n",
    "\n",
    "    # Write property to get the parameters from the underlying objects\n",
    "    # These variable names correspond to the math above.\n",
    "    @property\n",
    "    def pi(self):\n",
    "        return self.p_z.probs\n",
    "\n",
    "    @property\n",
    "    def log_pi(self):\n",
    "        return self.p_z.logits\n",
    "\n",
    "    @property\n",
    "    def bs(self):\n",
    "        return self.p_x.mean\n",
    "\n",
    "    @property\n",
    "    def Qs(self):\n",
    "        return self.p_x.covariance_matrix\n",
    "\n",
    "    @property\n",
    "    def Js(self):\n",
    "        return self.p_x.precision_matrix\n",
    "\n",
    "    @property\n",
    "    def hs(self):\n",
    "        # linear natural paramter h = Q^{-1} b = J b\n",
    "        return torch.einsum('kij,kj->ki', self.Js, self.bs)\n",
    "\n",
    "    @property\n",
    "    def C(self):\n",
    "        return self.p_y.weights\n",
    "\n",
    "    @property\n",
    "    def d(self):\n",
    "        return self.p_y.bias\n",
    "\n",
    "    @property\n",
    "    def R_diag(self):\n",
    "        return self.p_y.diag_covariance\n",
    "\n",
    "    def sample(self, sample_shape=(100,)):\n",
    "        \"\"\"\n",
    "        Draw a sample of the latent variables and data under the MFA model.\n",
    "        \"\"\"\n",
    "        z = self.p_z.sample(sample_shape)\n",
    "        x = MultivariateNormal(self.bs[z], self.Qs[z]).sample()\n",
    "        y = self.p_y.sample(x)\n",
    "        return dict(z=z, x=x, y=y)\n",
    "\n",
    "    def plot(self, data, spc=10):\n",
    "        # Unpack the arguments\n",
    "        z, x, y = data['z'], data['x'], data['y']\n",
    "        K = self.num_states\n",
    "        N = self.data_dim\n",
    "        T = len(y)\n",
    "        \n",
    "        # Plot the data\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        for k in range(K):\n",
    "            plt.plot(x[z == k, 0], x[z == k, 1], 'o', color=palette[k], mec='k')\n",
    "        plt.xlabel(\"continuous latente dim 0\")\n",
    "        plt.ylabel(\"continuous latente dim 1\")\n",
    "\n",
    "        # Sort the data by their discrete states for nicer visualization\n",
    "        perm = torch.argsort(z)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(z[perm][None, :], extent=(0, T, -spc, spc * (N + 1)), \n",
    "                   aspect=\"auto\", cmap=cmap, vmin=0, vmax=len(palette)-1, \n",
    "                   alpha=0.5)\n",
    "        plt.plot(y[perm] + spc * torch.arange(N), 'wo', mec='k')\n",
    "        for n in range(N):\n",
    "            plt.plot([0, T], [spc * n, spc * n], ':k')\n",
    "\n",
    "        plt.xlim(0, T)\n",
    "        plt.xlabel(\"data index [sorted by discrete state]\")\n",
    "        plt.ylim(-spc, spc * (N + 1))\n",
    "        plt.yticks(spc * np.arange(N), np.arange(N))\n",
    "        plt.ylabel(\"data dimension\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VP5MOoa6GI1b"
   },
   "source": [
    "## Sample data from the generative model\n",
    "\n",
    "Now we will sample small training and testing datasets from an MFA model with random parameters. We plot the data in two ways: as points in the continuous latent space color coded by discrete label, and then as points in the data space. Don't be fooled by the ordering of the second plot: the samples are arbitrarily ordered, but we've permuted their order to see how different states give rise to better see the distribution corresponding to each discrete state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUzFqgP0ncio"
   },
   "outputs": [],
   "source": [
    "# Construct a model instance.\n",
    "# The scale keyword determines how separated the clusters are in the continuous\n",
    "# latent space.\n",
    "torch.manual_seed(0)\n",
    "num_states = 7\n",
    "latent_dim = 2\n",
    "data_dim = 10\n",
    "model = MixtureOfFactorAnalyzers(num_states, latent_dim, data_dim, scale=3)\n",
    "\n",
    "# Sample from the model\n",
    "num_data = 1000\n",
    "train_data = model.sample(sample_shape=(num_data,))\n",
    "test_data = model.sample(sample_shape=(num_data,))\n",
    "\n",
    "# Plot the data\n",
    "model.plot(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iW3_2hfHnFCT"
   },
   "source": [
    "## Pause\n",
    "\n",
    "Take a second to make sure you understand the model and the plots above..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6iEEwHUj7tF"
   },
   "source": [
    "# Part 1: Coordinate Ascent Variational Inference (CAVI)\n",
    "\n",
    "First, we'll implement coordinate ascent variational inference (CAVI) for the mixture of factor analyzers model. We'll use a mean field posterior approximation\n",
    "\\begin{align}\n",
    "p(z, x \\mid y, \\Theta) \\approx \\prod_{t=1}^T q(z_t) \\, q(x_t)\n",
    "\\end{align}\n",
    "such that $\\mathrm{KL}\\big( q(z)q(x) \\, \\| \\, p(z, x \\mid y, \\Theta) \\big)$ is minimized. In class, we showed how to minimize the KL via coordinate ascent, iteratively optimizing $q(z)$ and $q(x)$, holding the other fixed. Here we will implement that algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkIjezggl8wy"
   },
   "source": [
    "## Problem 1a [Math]: Derive the expected log likelihood\n",
    "\n",
    "In class we derived the coordinate update for the discrete state factors,\n",
    "\\begin{align}\n",
    "\\log q(z_t) &= \\mathbb{E}_{q(x_t)} \\left[\\log p(z_t, x_t, y \\mid \\Theta) \\right] + \\mathrm{c} \\\\\n",
    "&= \\mathbb{E}_{q(x_t)} \\left[\\log p(z_t \\mid \\Theta) + \\log p(x_t \\mid z_t, \\Theta) + \\log p(y \\mid x_t, \\Theta) \\right] + \\mathrm{c} \\\\\n",
    "&= \\log \\mathrm{Cat}(z_t \\mid \\pi) + \\mathbb{E}_{q(x_t)} \\left[\\log \\mathcal{N}(x_t \\mid b_{z_t}, Q_{z_t}) \\right] + \\mathrm{c} \\\\\n",
    "&= \\sum_{k=1}^K \\mathbb{I}[z_t=k] \\left( \\log \\pi_k + \\mathbb{E}_{q(x_t)} \\left[\\log \\mathcal{N}(x_t \\mid b_k, Q_k) \\right] \\right) + \\mathrm{c} \\\\\n",
    "&= \\log \\mathrm{Cat}(z_t \\mid \\tilde{\\pi}_t)\n",
    "\\end{align}\n",
    "where\n",
    "\\begin{align}\n",
    "\\log \\tilde{\\pi}_{tk} = \\log \\pi_k + \\underbrace{\\mathbb{E}_{q(x_t)} \\left[\\log \\mathcal{N}(x_t \\mid b_k, Q_k) \\right]}_{\\text{expected log likelihood}} + \\mathrm{c}.\n",
    "\\end{align}\n",
    "However, we did not simplify the expected log likelihood expression.\n",
    "\n",
    "_Suppose $q(x_t) = \\mathcal{N}(x_t \\mid \\tilde{\\mu}_t, \\tilde{\\Sigma}_t)$. Show that the expected log likelihood is,\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{q(x_t)} \\left[\\log \\mathcal{N}(x_t \\mid b_k, Q_k) \\right]\n",
    "&= \\log \\mathcal{N}(\\tilde{\\mu}_t \\mid b_k, Q_k) - \\tfrac{1}{2} \\langle(Q_k^{-1}, \\tilde{\\Sigma}_t \\rangle\n",
    "\\end{align}\n",
    "Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjCRNLAAoWWL"
   },
   "source": [
    "## Problem 1b: Implement the discrete state update\n",
    "\n",
    "We will use `torch.distributions.Distribution` objects to represent the  approximate posterior distributions as well. We will use `MultivariateNormal` to represent $q(x)$ and `Categorical` to represent $q(z)$. We'll take advantage of the distribution objects' broadcasting capability to represent the variational posteriors for all time steps at once. \n",
    "\n",
    "_Implement a CAVI update for the discrete states posterior that takes in the model and the continuous state posterior `q_x` and outputs the optimal `q_z`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SphU492LomR0"
   },
   "outputs": [],
   "source": [
    "def cavi_update_q_z(model, q_x):\n",
    "    \"\"\"Compute the optimal discrete state posterior given the generative model\n",
    "    and the variational posterior on the continuous states.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: a MixtureOfFactorAnalyzers model instance.\n",
    "    \n",
    "    q_x: a `MultivariateNormal` object with a shape `TxD` parameter `mean` and a \n",
    "        shape `TxDxD` parameter `covariance matrix` representing the means and \n",
    "        covariances, respectively, for each data point under the variational \n",
    "        posterior.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    q_z: a `Categorical` object with a shape `TxK` parameter `logits` \n",
    "        representing the variational posterior on discrete states.\n",
    "    \"\"\"\n",
    "    K = model.num_states\n",
    "    T = q_x.mean.shape[0]\n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    logits = torch.zeros(T, K)\n",
    "    for k, (bk, Qk, Jk) in enumerate(zip(model.bs, model.Qs, model.Js)):\n",
    "        logits[:, k] += ... \n",
    "\n",
    "    q_z = Categorical(logits=logits)\n",
    "    #\n",
    "    ###\n",
    "    return q_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYnJYQoN_JlE"
   },
   "outputs": [],
   "source": [
    "def test_1b():\n",
    "    torch.manual_seed(0)\n",
    "    q_x = MultivariateNormal(torch.randn(num_data, latent_dim),\n",
    "                             torch.eye(latent_dim).repeat(num_data, 1, 1))\n",
    "    q_z = cavi_update_q_z(model, q_x)\n",
    "    assert q_z.probs.shape == (num_data, num_states)\n",
    "    assert np.isclose(q_z.probs.std(), 0.2576, atol=1e-8)\n",
    "test_1b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiOE-pwKsT3t"
   },
   "source": [
    "## Problem 1c: Implement the continuous state update\n",
    "\n",
    "In class we showed that the optimal continuous state posterior, holding the discrete posterior fixed, was a Gaussian distribution $q(x_t) = \\mathcal{N}(\\tilde{\\mu}_t, \\tilde{\\Sigma}_t)$ with\n",
    "\\begin{align}\n",
    "\\tilde{\\mu}_t &= \\tilde{J}_t^{-1} \\tilde{h}_t &\n",
    "\\tilde{\\Sigma}_t &= \\tilde{J}_t^{-1} \\\\\n",
    "\\tilde{h}_t &= \\mathbb{E}_{q({z_t})}[Q_{z_t}^{-1} b_{z_t}] + C^\\top R^{-1} (y_t-d) &\n",
    "\\tilde{J}_t &= \\mathbb{E}_{q({z_t})}[Q_{z_t}^{-1}] + C^\\top R^{-1} C \\\\\n",
    "&= \\sum_{k=1}^K \\left[ q({z_t}=k) Q_k^{-1} b_k \\right] + C^\\top R^{-1} (y_t-d) &\n",
    "&= \\sum_{k=1}^K \\left[ q({z_t}=k) Q_k^{-1} \\right] + C^\\top R^{-1} C\n",
    "\\end{align}\n",
    "\n",
    "_Implement a CAVI update for the continuous states posterior that takes in `p_x`, `p_y`, and `q_z` and outputs the optimal `q_x`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGDy9EZAtMwS"
   },
   "outputs": [],
   "source": [
    "def cavi_update_q_x(data, model, q_z):\n",
    "    \"\"\"Compute the optimal discrete state posterior given the generative model\n",
    "    and the variational posterior on the continuous states.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: a dictionary with a key `y` containing a `TxN` tensor of data.\n",
    "\n",
    "    model: a MixtureOfFactorAnalyzers model instance.\n",
    "\n",
    "    q_z: a `Categorical` object with a shape `TxK` parameter `logits` \n",
    "        representing the variational posterior on discrete states.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    q_x: a `MultivariateNormal` object with a shape `TxD` parameter `mean` and a \n",
    "        shape `TxDxD` parameter `covariance matrix` representing the means and \n",
    "        covariances, respectively, for each data point under the variational \n",
    "        posterior.\n",
    "    \"\"\"\n",
    "    y = data[\"y\"]\n",
    "    \n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    ...\n",
    "    q_x = ...\n",
    "    #\n",
    "    ###\n",
    "    return q_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osDlPzu8DpTT"
   },
   "outputs": [],
   "source": [
    "def test_1c():\n",
    "    torch.manual_seed(0)\n",
    "    q_z = Categorical(logits=torch.randn(num_data, num_states))\n",
    "    q_x = cavi_update_q_x(train_data, model, q_z)\n",
    "    assert q_x.mean.shape == (num_data, latent_dim)\n",
    "    assert q_x.covariance_matrix.shape == (num_data, latent_dim, latent_dim)\n",
    "    assert np.isclose(q_x.mean.mean(), -0.7204, atol=1e-4)\n",
    "    assert np.isclose(q_x.mean.std(), 2.9253, atol=1e-4)\n",
    "    assert np.isclose(q_x.covariance_matrix.mean(), 0.0271, atol=1e-4)\n",
    "    assert np.isclose(q_x.covariance_matrix.std(), 0.0623, atol=1e-4)\n",
    "test_1c()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl8jKsjtH8mu"
   },
   "source": [
    "## Problem 1d [Short Answer]: Intuition for the continuous updates\n",
    "\n",
    "Consider setting the discrete posterior $q(z)$ to be uniform over the $K$ states and then performing one update of the continuous states. The plot below shows the true values of $x$ and $z$ as color coded dots in 2D, and then it shows the means of the continuous state posterior $q(x)$ found using one step of CAVI. We see that the means of the continuous state posterior are all pulled toward the center. Why would you expect that to happen?\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YwWuyglQERCD"
   },
   "outputs": [],
   "source": [
    "def plot_data_and_q_x(data, q_x):\n",
    "    x, z, y = data['x'], data['z'], data['y']\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for k in range(model.num_states):\n",
    "        plt.plot(x[z == k, 0], x[z == k, 1], 'o', color=palette[k], mec='k')\n",
    "        plt.plot(q_x.mean[z == k, 0], q_x.mean[z == k, 1], 'o', \n",
    "                 color=palette[k], mfc='none', mec='r', ms=8)\n",
    "    plt.xlabel(\"continuous latente dim 0\")\n",
    "    plt.ylabel(\"continuous latente dim 1\")\n",
    "\n",
    "q_z = Categorical(logits=torch.zeros(num_data, model.num_states))\n",
    "q_x = cavi_update_q_x(train_data, model, q_z)\n",
    "plot_data_and_q_x(train_data, q_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nero-ljVw37-"
   },
   "source": [
    "## Problem 1e [Math]: Derive the evidence lower bound\n",
    "\n",
    "We will use the ELBO to track the convergence of our CAVI algorithm. In class we wrote the ELBO as, \n",
    "\\begin{align}\n",
    "\\mathcal{L}[q(z)q(x), \\Theta] &= \\mathbb{E}_{q(z)q(x)} \\left[ \\log p(z, x, y \\mid \\Theta) - \\log q(z)q(x) \\right] \n",
    "\\end{align}\n",
    "Show that this is equivalent to,\n",
    "\\begin{align}\n",
    "\\mathcal{L}[q(z)q(x), \\Theta]\n",
    "&= \\mathbb{E}_{q(x)}\\left[\\log p(y \\mid x, \\Theta) \\right] \n",
    "- \\mathrm{KL}\\big(q(z) \\, \\| \\, p(z \\mid \\Theta) \\big) - \\mathbb{E}_{q(z)}\\left[\\mathrm{KL}\\big( q(x) \\, \\| \\, p(x \\mid z, \\Theta) \\big) \\right].\n",
    "\\end{align}\n",
    "\n",
    "Then show that,\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{q(x)}\\left[\\log p(y \\mid x, \\Theta) \\right] \n",
    "&= \\sum_{t=1}^T \\log \\mathcal{N}(y_t \\mid C \\tilde{\\mu}_t + d, R) -\\tfrac{1}{2} \\langle C^\\top R^{-1} C, \\tilde{\\Sigma}_t \\rangle,\n",
    "\\end{align}\n",
    "whre $\\tilde{\\mu}_t$ and $\\tilde{\\Sigma}_t$ are the parameters of the variational posterior $q(x_t)$, as above.\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQyXC1CnzpHo"
   },
   "source": [
    "## Problem 1f: Implement the ELBO\n",
    "\n",
    "Use the `IndependentLinearRegression.log_prob` function and the `torch.distributions.kl_divergence` function imported at the top of the notebook to implement the ELBO calculation. Remember that the log probabilities and KL divergence functions broadcast nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVeuU5Mvzz6p"
   },
   "outputs": [],
   "source": [
    "def elbo(data, model, variational_posterior):\n",
    "    \"\"\"Compute the optimal discrete state posterior given the generative model\n",
    "    and the variational posterior on the continuous states.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: a dictionary with a key `y` containing a `TxN` tensor of data.\n",
    "\n",
    "    model: a MixtureOfFactorAnalyzers model instance\n",
    "    \n",
    "    variational_posterior: a tuple (q_z, q_x) where\n",
    "        q_z: a `Categorical` object with a shape `TxK` parameter `logits` \n",
    "            representing the variational posterior on discrete states.\n",
    "\n",
    "        q_x: a `MultivariateNormal` object with a shape `TxD` parameter `mean` \n",
    "            and a shape `TxDxD` parameter `covariance matrix` representing the \n",
    "            means and covariances, respectively, for each data point under the \n",
    "            variational posterior.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The evidence lower bound (ELBO) as derived above.\n",
    "    \"\"\"\n",
    "    y = data[\"y\"]\n",
    "    q_z, q_x = variational_posterior\n",
    "\n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    ...\n",
    "    elbo = ...\n",
    "    #\n",
    "    ###\n",
    "    return elbo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-iRqZgZpL4S"
   },
   "outputs": [],
   "source": [
    "def test_1f():\n",
    "    q_z = Categorical(logits=torch.zeros(num_data, model.num_states))\n",
    "    q_x = cavi_update_q_x(train_data, model, q_z)\n",
    "    assert np.isclose(elbo(train_data, model, (q_z, q_x)) / num_data,\n",
    "                    -32.3214, atol=1e-4)\n",
    "\n",
    "test_1f()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6Fq6PbKLUW2"
   },
   "source": [
    "## Problem 1g [Math]: Derive the exact marginal likelihood\n",
    "\n",
    "In this simple model, we can actually compute the marginal likelihood exactly. This gives us a way of seeing how tight the ELBO actually is. (Remember, the ELBO is a lower bound on the marginal likelihood!)\n",
    "\n",
    "To compute the marginal likelihood, we need two key facts about Gaussian random variables:\n",
    "\\begin{align}\n",
    "x \\sim \\mathcal{N}(b, Q) &\\implies C x + d \\sim \\mathcal{N}(Cb + d, CQ C^\\top) \\\\\n",
    "m \\sim \\mathcal{N}(\\mu_1, \\Sigma_1), \n",
    "\\epsilon \\sim \\mathcal{N}(\\mu_2, \\Sigma_2) &\\implies \n",
    "m + \\epsilon \\sim \\mathcal{N}(\\mu_1 + \\mu_2, \\Sigma_1 + \\Sigma_2)\n",
    "\\end{align}\n",
    "Use these two facts to show that \n",
    "\\begin{align}\n",
    "p(y_t \\mid z_t, \\Theta) &= \\mathcal{N}(C b_{z_t} + d, C Q_{z_t} C^\\top + R).\n",
    "\\end{align}\n",
    "Then show that \n",
    "\\begin{align}\n",
    "\\log p(y \\mid \\Theta) &= \\sum_{t=1}^T \\log \\left( \\sum_{k=1}^K \\mathcal{N}(y_t \\mid C b_{k} + d, C Q_{k} C^\\top + R) \\right).\n",
    "\\end{align}\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3K8IkfD9OVES"
   },
   "source": [
    "## Implement the exact marginal likelihood\n",
    "The code below implements the exact marginal likelihood according to the formula above using PyTorch's `LowRankMultivariateNormal` distribution. Note: this distribution takes in the square root of $C Q_k C^\\top$, which is $C Q_k^{1/2}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sO2Eoe8qOnYc"
   },
   "outputs": [],
   "source": [
    "def exact_marginal_lkhd(data, model):\n",
    "    \"\"\"\n",
    "    Compute the exact marginal likelihood. \n",
    "    Normalize by the number of datapoints.\n",
    "    \"\"\"\n",
    "    # Compute the marginal distributions \n",
    "    y = data[\"y\"]\n",
    "    T = y.shape[0]\n",
    "    K = model.num_states\n",
    "    \n",
    "    # Compute the marginal likelihood under each discrete state assignment\n",
    "    lls = torch.zeros(T, K)\n",
    "    for k, (bk, Qk) in enumerate(zip(model.bs, model.Qs)):\n",
    "        # log p(z = k)\n",
    "        lls[:, k] += model.log_pi[k]\n",
    "\n",
    "        # logp(y | z = k) = log N(y | C b_k + d, C Q_k C^T + diag(R))\n",
    "        Qk_sqrt = torch.cholesky(Qk)\n",
    "        p_yk = LowRankMultivariateNormal(model.C @ bk + model.d, \n",
    "                                         model.C @ Qk_sqrt, model.R_diag)\n",
    "        lls[:, k] += p_yk.log_prob(y)\n",
    "    \n",
    "    return torch.logsumexp(lls, axis=1).sum()\n",
    "\n",
    "marginal_ll = exact_marginal_lkhd(train_data, model) / num_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_7XiTJYHAAu"
   },
   "source": [
    "## Run CAVI\n",
    "\n",
    "That's all we need for CAVI! The code below simply alternates between updating $q(z)$ and $q(x)$. After each iteration, we compute the ELBO.We allow the user to pass in an initial posterior approximation (though only $q(z)$ is used since $q(x)$ is immediately updated). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1kpsIk9z2YS"
   },
   "outputs": [],
   "source": [
    "def cavi(data, model, initial_posterior=None, num_steps=10, pbar=None):\n",
    "    y = data[\"y\"]\n",
    "\n",
    "    # Initialize the discrete state posterior to uniform\n",
    "    if initial_posterior is None:\n",
    "        q_z = Categorical(logits=torch.zeros(len(y), model.num_states))\n",
    "        q_x = None\n",
    "    else:\n",
    "        q_z, _ = initial_posterior\n",
    "\n",
    "    # Optional progress bar\n",
    "    if pbar is not None: pbar.reset()\n",
    "\n",
    "    # Run CAVI\n",
    "    avg_elbos = []\n",
    "    for i in range(num_steps):\n",
    "        if pbar is not None: pbar.update()\n",
    "        q_x = cavi_update_q_x(data, model, q_z)\n",
    "        avg_elbos.append(elbo(data, model, (q_z, q_x)) / len(y))\n",
    "        q_z = cavi_update_q_z(model, q_x)\n",
    "\n",
    "    return torch.tensor(avg_elbos), (q_z, q_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7257IHdul92"
   },
   "outputs": [],
   "source": [
    "# Run CAVI and plot the ELBO over coordinate ascent iterations\n",
    "avg_elbos, (q_z, q_x) = cavi(train_data, model)\n",
    "plot_elbos(avg_elbos, marginal_ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mQnkN7AMBDN"
   },
   "source": [
    "## Re-examine the continuous state posterior after CAVI\n",
    "\n",
    "Now let's make the same plot from Problem 1d again. We should see that the continuous means are pulled toward their true values. Remember, these are inferences! The CAVI algorithm only sees the data $y$ and the model parameters $\\Theta$. After a few iterations (really, after about 2 iterations), it converges to a posterior approximation in which the mean of continuous latent states, $\\mathbb{E}_{q(x_t)}[x_t]$, are close to their true values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHoFi1NwH0Lc"
   },
   "outputs": [],
   "source": [
    "plot_data_and_q_x(train_data, q_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FURED-xnwUDz"
   },
   "source": [
    "# Part 2: Variational EM in a mixture of factor analysis models\n",
    "\n",
    "The CAVI algorithm we implemented in Part 1 will form the E step for variational EM. To complete the algorithm, we just need to compute the expected sufficient statistics under the variational posterior and use them to implement the M-step. Last week, in Lab 7, we derived the expected sufficient statistics needed to update the multivariate normal distribution and the weights of the linear regression. In this part, you'll write similar functions to compute the expected sufficient statistics using the variational posterior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM192bwhdD2J"
   },
   "source": [
    "## Problem 2a: Compute the expected sufficient statistics\n",
    "\n",
    "\n",
    "The sufficient statistics of the model are (with zero-indexing for Python friendliness),\n",
    "0. $\\sum_{t=1}^T \\mathbb{I}[z_t=k]$ for $k = 1, \\ldots, K$\n",
    "1. $\\sum_{t=1}^T \\mathbb{I}[z_t=k] \\, x_t$ for $k = 1, \\ldots, K$\n",
    "2. $\\sum_{t=1}^T \\mathbb{I}[z_t=k] \\, x_t x_t^\\top$ for $k = 1, \\ldots, K$\n",
    "3.  $\\sum_{t=1}^T x_t$\n",
    "4. $\\sum_{t=1}^T x_t x_t^\\top$\n",
    "5. $\\sum_{t=1}^T y_t x_t^\\top$\n",
    "6. $\\sum_{t=1}^T y_t$\n",
    "7. $\\sum_{t=1}^T y_t^2$\n",
    "7. $\\sum_{t=1}^T 1 = T$\n",
    "\n",
    "Write a function that computes the _expected_ sufficient statistics $\\mathbb{E}_{q(z)q(x)}[\\cdot]$ under the variational posterior distribution. In code, we'll call these variables `E_*`, for example `E_z` represents the length $K$ tensor for the sufficient statistic 0.\n",
    "\n",
    "**Note:** The expected outer product, $\\mathbb{E}_{q(x_t)}[x_t x_t^\\top]$, does _not_ equal the covariance matrix unless $\\mathbb{E}_{q(x_t)}[x_t]$ is zero (and here, it's not generally zero).\n",
    "\n",
    "**Note:** Statistics 3 and 1 are redundant, as are 4 and 2. We've split them out anyway, as they are used separately in updating the parameters of `p_x` and `p_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Uhe6Ve4e0he"
   },
   "outputs": [],
   "source": [
    "def compute_expected_suffstats(data, posterior):\n",
    "    \"\"\"\n",
    "    Compute the expected sufficient statistics of the data \n",
    "    under the variational posterior\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: a dictionary with a key `y` containing a `TxN` tensor of data.\n",
    "    posterior: a tuple (q_z, q_x) representing the variational posterior, as \n",
    "        computed in part 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of the 9 expected sufficient statistics in the order listed above.\n",
    "    \"\"\"\n",
    "    y = data[\"y\"]\n",
    "    q_z, q_x = posterior\n",
    "    \n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    E_z = ...\n",
    "    E_zx = ...\n",
    "    E_zxxT = ...\n",
    "    E_x = ...\n",
    "    E_xxT = ...\n",
    "    E_yxT = ...\n",
    "    E_y = ...\n",
    "    E_ysq = ...\n",
    "    T = ...\n",
    "    #\n",
    "    ###\n",
    "    return (E_z, E_zx, E_zxxT, E_x, E_xxT, E_yxT, E_y, E_ysq, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PdDJ4P4OhSyC"
   },
   "outputs": [],
   "source": [
    "def test_2a():\n",
    "    print(\"This test only checks the shapes, not the values!\")\n",
    "    stats = compute_expected_suffstats(train_data, (q_z, q_x))\n",
    "    assert len(stats) == 9\n",
    "    E_z, E_zx, E_zxxT, E_x, E_xxT, E_yxT, E_y, E_ysq, T = stats\n",
    "    assert E_z.shape == (num_states,)\n",
    "    assert E_zx.shape == (num_states, latent_dim)\n",
    "    assert E_zxxT.shape == (num_states, latent_dim, latent_dim)\n",
    "    assert E_x.shape == (latent_dim,)\n",
    "    assert E_xxT.shape == (latent_dim, latent_dim)\n",
    "    assert E_yxT.shape == (data_dim, latent_dim)\n",
    "    assert E_y.shape == (data_dim,)\n",
    "    assert E_ysq.shape == (data_dim,)\n",
    "    assert isinstance(T, (int, float))\n",
    "\n",
    "test_2a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FRcofx7oHJZ"
   },
   "source": [
    "## Problem 2b: Implement the M-step for the parameters of $p(z \\mid \\Theta)$\n",
    "\n",
    "Write a function to update the prior distribution on discrete states, $p(z \\mid \\Theta)$, using the expected sufficient statistics. This is part of the M-step for variational EM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWOBtncKoLKW"
   },
   "outputs": [],
   "source": [
    "def update_p_z(stats):\n",
    "    \"\"\"\n",
    "    Compute the parameters $\\pi$ of the $p(z \\mid \\Theta)$ and pack them into \n",
    "    a new Categorical distribution object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    stats: a tuple of the 9 sufficient statistics computed above\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A new Categorical object for p_z with a length K tensor of cluster \n",
    "        probabilities.\n",
    "    \"\"\"\n",
    "    E_z = stats[0]\n",
    "    ### \n",
    "    # YOUR CODE BELOW\n",
    "    p_z = ...\n",
    "    #\n",
    "    ###\n",
    "    return p_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NCwj0xVjaxG"
   },
   "source": [
    "## Problem 2c: Implement the M-step for parameters of $p(x \\mid z, \\Theta)$\n",
    "\n",
    "Perform an M-step on the parameters of $p(x \\mid z, \\Theta)$ using the expected sufficient statistics. As before, add a little to the diagonal of the covariance to ensure positive definiteness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxTpGfXHR0iY"
   },
   "outputs": [],
   "source": [
    "def update_p_x(stats):\n",
    "    \"\"\"\n",
    "    Compute the parameters $\\{b_k, Q_k\\}$ of the $p(x \\mid z, \\Theta)$ and pack \n",
    "    them into a new MultivariateNormal distribution object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    stats: a tuple of the 9 sufficient statistics computed above\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A new MultivariateNormal object with KxD mean and KxDxD covariance matrix.\n",
    "    \"\"\"\n",
    "    E_z, E_zx, E_zxxT = stats[:3]\n",
    "    K, D = E_zx.shape\n",
    "\n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    ...\n",
    "    p_x = ...\n",
    "    #\n",
    "    ###\n",
    "    return p_x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5DW2NirkX2W"
   },
   "source": [
    "## Problem 2d: Implement the M-step for parameters of $p(y \\mid x, \\Theta)$\n",
    "\n",
    "Following Lab 7, let $\\phi_t = (x_t, 1)$ denote the covariates that go into the linear model for data point $y_t$. Specifically,\n",
    "\\begin{align}\n",
    "p(y_t \\mid x_t, \\Theta) &= \\mathcal{N}(y_t \\mid W \\phi_t, R),\n",
    "\\end{align}\n",
    "where $W = (C, d) \\in \\mathbb{R}^{N \\times D+1}$ is an array containing both the weights and the bias of the linear regression model.\n",
    "\n",
    "To update the linear regression, we need the expected sufficient statistics:\n",
    "- The expected outer product of the data and covariates, \n",
    "\\begin{align}\n",
    "\\mathbb{E}_{q(x_t)}[ y \\phi_t^\\top] = \\mathbb{E}_{q(x_t)}[ y (x_t, 1)^\\top] \n",
    "= \\begin{bmatrix} \\mathbb{E}_{q(x_t)}[ y x_t^\\top], & y \\end{bmatrix} \n",
    "\\end{align}\n",
    "\n",
    "- The expected outer product of the covariates with themselves, \n",
    "\\begin{align}\n",
    "\\mathbb{E}_{q(x_t)}[ \\phi_t \\phi_t^\\top] = \\mathbb{E}_{q(x_t)}[ (x_t, 1) (x_t, 1)^\\top] \n",
    "= \\begin{bmatrix} \\mathbb{E}_{q(x_t)}[ x_t x_t^\\top], &  \\mathbb{E}_{q(x_t)}[ x_t] \\\\\n",
    "\\mathbb{E}_{q(x_t)}[x_t^\\top], & T \n",
    "\\end{bmatrix} \n",
    "\\end{align}\n",
    "\n",
    "These are $N \\times (D+1)$ and $(D+1) \\times (D+1)$ tensors, respectively.\n",
    "\n",
    "Since we are assuming a diagonal covariance matrix, we only need $y_{tn}^2$ instead of the full outer product $y_t y_t^\\top$.  As before, add a bit to the diagonal to ensure positive definiteness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJYP-MZS_uK8"
   },
   "outputs": [],
   "source": [
    "def update_p_y(stats):\n",
    "    \"\"\"\n",
    "    Compute the linear regression parameters given the expected \n",
    "    sufficient statistics.\n",
    "\n",
    "    Note: add a little bit to the diagonal of each covariance \n",
    "        matrix to ensure that the result is positive definite.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    stats: a tuple of the 8 sufficient statistics computed above\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A new IndependentLinearRegression object for p_y\n",
    "    \"\"\"\n",
    "    E_x, E_xxT, E_yxT, E_y, E_ysq, T = stats[3:]\n",
    "    N, D = E_yxT.shape\n",
    "    \n",
    "    # Initialize the big statistics tensors\n",
    "    E_yphiT = torch.zeros(N, D + 1)\n",
    "    E_phiphiT = torch.zeros(D + 1, D + 1)\n",
    "    \n",
    "    ###\n",
    "    # Use E_x, E_xxT, E_yxT, E_y, and T to fill in E_yphiT and\n",
    "    # E_phiphiT as described above. Then use them to compute the\n",
    "    # new p_y.    \n",
    "    #\n",
    "    # YOUR CODE BELOW\n",
    "    ...    \n",
    "    p_y = ...\n",
    "    #\n",
    "    ###\n",
    "\n",
    "    return p_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HlbbA5Fnmc0"
   },
   "source": [
    "## Put it all together\n",
    "\n",
    "From here it's smooth sailing! We just iterate between the variational E step, which involves running CAVI for some number of iterations, and then performing an M step using expected sufficient statistics. We'll track the ELBO throughout to monitor convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVsrfOAznntm"
   },
   "outputs": [],
   "source": [
    "def m_step(data, model, posterior):\n",
    "    \"\"\"\n",
    "    Perform an M-step to update the model parameters given the data and the \n",
    "    posterior from the variational E step.\n",
    "    \"\"\"\n",
    "    stats = compute_expected_suffstats(data, posterior)\n",
    "    model.p_z = update_p_z(stats)\n",
    "    model.p_x = update_p_x(stats)\n",
    "    model.p_y = update_p_y(stats)\n",
    "\n",
    "\n",
    "def variational_em(data, model, num_iters=100, num_cavi_steps=1):\n",
    "    \"\"\"\n",
    "    Fit the model parameters via variational EM.\n",
    "    \"\"\"\n",
    "    # Run CAVI\n",
    "    avg_elbos = []\n",
    "    posterior = None\n",
    "    for i in trange(num_iters):\n",
    "        # Variational E step with CAVI\n",
    "        these_elbos, posterior = cavi(data, model, posterior, \n",
    "                                      num_steps=num_cavi_steps)\n",
    "        avg_elbos.extend(these_elbos)\n",
    "\n",
    "        # M-step\n",
    "        m_step(data, model, posterior)\n",
    "\n",
    "    return torch.tensor(avg_elbos), posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vq_7Cqk9tDXe"
   },
   "outputs": [],
   "source": [
    "# Fit the synthetic data\n",
    "avg_elbos, posterior = variational_em(train_data, model)\n",
    "plot_elbos(avg_elbos, marginal_ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_k4BwLcUnGZ"
   },
   "source": [
    "## Problem 2e [Short Answer]: Interpret the results\n",
    "\n",
    "One perhaps counterintuitive aspect of the output is that the ELBO of the fitted model actually exceeds the marginal likeliood of the true model. How can that happen?\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTTtPjDOVv7H"
   },
   "source": [
    "## Problem 2f: Cross validation\n",
    "\n",
    "Fit the MFA model with variational EM for $D=1,\\ldots, 5$ (inclusive), keeping the number of discrete states fixed to $K=7$. For each model, evaluate the evidence lower bound on the test data, using ten steps of CAVI to approximate the posterior. Then compute the exact marginal likelihood using the true model and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OD4crz3MV0zP"
   },
   "outputs": [],
   "source": [
    "test_latent_dims = torch.arange(1, 11)\n",
    "test_elbos = []\n",
    "for d in test_latent_dims:\n",
    "    print(\"Fitting the MFA model with D =\", int(d), \n",
    "          \"dimensional continuous states.\")\n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    ...\n",
    "    #\n",
    "    ###\n",
    "\n",
    "# Compute the true marginal likelihood of the test dat\n",
    "true_test_elbo = exact_marginal_lkhd(test_data, model) / num_data\n",
    "\n",
    "# Plot as a function of continuous dimensionality\n",
    "plt.plot(test_latent_dims, test_elbos, '-o')\n",
    "plt.plot(test_latent_dims, true_test_elbo * np.ones_like(test_latent_dims), ':k')\n",
    "plt.xlabel(\"continuous dimension $D$\")\n",
    "plt.ylabel(\"Test ELBO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqxThbOlY1Lq"
   },
   "source": [
    "## Problem 2g [Short answer]: Interpret the results\n",
    "\n",
    "Woul you be surprised to see the fitted models achieve higher ELBOs on test data than the marginal likelihod of the true model? Can you think of any potential concerns with using the ELBO for model comparison; e.g. for selecting the latent state dimension $D$?\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37rNTRHwwDnp"
   },
   "source": [
    "# Part 3: Apply it to real data\n",
    "\n",
    "Finally, we'll apply the mixture of factor analyzers to calcium imaging data from immobilized worms studied by Kato et al (2015). They also segmented the time series into discrete states based on the neural activity and gave each state a name, using their knowledge of how different neurons correlate with different types of behavior. We'll try to recapitulate some of their results using the MFA model to infer discrete states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeZtzkFqDJhS"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget -nc https://www.dropbox.com/s/qnjslekm11pyuju/kato2015.zip\n",
    "!unzip -n kato2015.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ch-AtmZbbuoT"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "The data is stored in a dictionary with a few extra keys for the neuron names and the given discrete state labels and human-interpretable state names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJAVmVAmR3tv"
   },
   "outputs": [],
   "source": [
    "# Load the data for a single worm\n",
    "data = load_kato(index=4)\n",
    "\n",
    "# Extract key constants\n",
    "num_frames, num_neurons = data[\"y\"].shape\n",
    "times = np.arange(num_frames) / data[\"fps\"]\n",
    "\n",
    "# The data has the following keys...\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezyTBYVwb8Sw"
   },
   "source": [
    "## Perform PCA\n",
    "\n",
    "We'll use the principal components for visualization as well as for finding a permutation of the neurons that puts similar neurons, as measured by their loading on the first principal component, near to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CccRWyoNUrLr"
   },
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = PCA(20)\n",
    "data[\"pcs\"] = pca.fit_transform(data[\"y\"])\n",
    "neuron_perm = np.argsort(pca.components_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Btm5kHV8cB5n"
   },
   "source": [
    "## Plot the data\n",
    "We plot the time series of neural activity on top of the color-coded discrete states given by Kato et al. Each neuron has a name, like \"AVAL\", and a corresponding calcium fluorescence trace. You should see that the different discrete states correspond to different levels of neural activity across the population of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBFRdTodSIQl"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(data[\"z_kato\"][None, :], extent=(0, times[-1], -1, num_neurons + 2), \n",
    "           alpha=0.5, cmap=cmap, aspect=\"auto\")\n",
    "plt.plot(times, data[\"y\"][:, neuron_perm] + np.arange(num_neurons), '-k', lw=1)\n",
    "plt.xlabel(\"time[s]\")\n",
    "plt.ylabel(\"neurons\")\n",
    "plt.yticks(np.arange(num_neurons), \n",
    "           [data[\"neuron_names\"][i] for i in neuron_perm], \n",
    "           fontsize=10)\n",
    "plt.ylim(-1, num_neurons+2)\n",
    "\n",
    "for state_name, color in zip(data[\"state_names\"], palette):\n",
    "    plt.plot([np.nan], [np.nan], '-', color=color, lw=4, label=state_name)\n",
    "\n",
    "plt.legend(loc=\"lower right\", ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwY8A0mFclCR"
   },
   "source": [
    "## Plot the PCA trajectories\n",
    "\n",
    "We can also visualize the population activity as a trajectory through PCA space. Here we plot the trajectory in planes spanned by pairs of principal components. We color code the trajectory based on the given discrete states.\n",
    "\n",
    "**Note**: We smoothed the trajectories a bit to make the visualization nicer. \n",
    "\n",
    "**Note**: These differ from the figures in Kato et al (2015) in that they used PCA on the first order differences in neural activity (akin to the \"spikes\" in the calcium trace, even though _C elegans_ doesn't fire action potentials). We found that the first order differences didn't cluster as nicely with the MFA model, so we are working with the calcium traces directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlLZnT-LUnL2"
   },
   "outputs": [],
   "source": [
    "pcs_smooth = gaussian_filter1d(data[\"pcs\"], 1, axis=0)\n",
    "\n",
    "fig, axs = plt.subplots(4, 4, figsize=(10, 10), sharex=True, sharey=True)\n",
    "for i in range(4):\n",
    "    for j in range(i+1, 5):\n",
    "        plot_2d_continuous_states(pcs_smooth, data[\"z_kato\"], \n",
    "                                  ax=axs[j-1, i], inds=(i, j), lw=1)\n",
    "        axs[j-1, i].set_xlabel(\"PC{}\".format(i))\n",
    "        axs[j-1, i].set_ylabel(\"PC{}\".format(j))\n",
    "\n",
    "    for j in range(i):\n",
    "        axs[j, i].set_axis_off()\n",
    "\n",
    "for state_name, color in zip(data[\"state_names\"], palette):\n",
    "    axs[0, -1].plot([np.nan], [np.nan], '-', \n",
    "                    color=color, lw=4, label=state_name)\n",
    "axs[0, -1].legend(loc=\"upper right\", ncol=2, )\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7dntRF-zfaZ"
   },
   "source": [
    "## Problem 3a [Short Answer]: Interpret the PCA trajectories\n",
    "\n",
    "What can you say about the cycle of neural activity in this worm given the PCA trajectories and the state labels provided by Kato et al (2015)?\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2s_fAx73s8OK"
   },
   "source": [
    "## Fit the mixture of factor analyzers\n",
    "\n",
    "Now fit the model. We'll give it twice as many states as Kato et al (2015) did. This often helps avoid some local optima where states are unused. We'll use ten dimensional continuous latents, as they do in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95Az0AOgs-mQ"
   },
   "outputs": [],
   "source": [
    "# Fit the worm data\n",
    "torch.manual_seed(0)\n",
    "num_states = 16\n",
    "latent_dim = 10\n",
    "worm_model = MixtureOfFactorAnalyzers(num_states, latent_dim, num_neurons)\n",
    "\n",
    "# Fit the model!\n",
    "avg_elbos, posterior = variational_em(data, worm_model, \n",
    "                                      num_iters=100, \n",
    "                                      num_cavi_steps=1)\n",
    "\n",
    "plot_elbos(avg_elbos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4fIvl2nfRv2"
   },
   "source": [
    "## Compute the overlap between the given and inferred discrete states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1J4My95x27_"
   },
   "outputs": [],
   "source": [
    "# Find the most likely state segmentation\n",
    "q_z, q_x = posterior\n",
    "z_inf = q_z.probs.argmax(axis=1)\n",
    "\n",
    "# compute overlap with the manually labeled states\n",
    "overlap = torch.zeros(8, num_states)\n",
    "for i in range(8):\n",
    "    for j in range(num_states):\n",
    "        overlap[i, j] = torch.sum((to_t(data[\"z_kato\"]) == i) * (z_inf == j))\n",
    "\n",
    "# normalize since sum given states are used less frequently than others\n",
    "overlap /= overlap.sum(axis=0)\n",
    "\n",
    "# permute the inferred labels for easier visualization\n",
    "z_perm = np.argsort(np.argmax(overlap, axis=0))\n",
    "\n",
    "# show the permuted overlap matrix\n",
    "plt.imshow(overlap[:, z_perm])\n",
    "plt.ylabel(\"Kato et al labels\")\n",
    "plt.yticks(np.arange(8), data[\"state_names\"])\n",
    "plt.xlabel(\"inferred discrete states\")\n",
    "plt.title(\"overlap (column normalized)\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Permute the inferred discrete states per the new ordering\n",
    "z_inf_perm = np.argsort(z_perm)[z_inf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yk8uhvRBfMt_"
   },
   "source": [
    "## Plot the inferred segmentation and the given state labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmFBlgnLx9sg"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(20, 11), \n",
    "                        gridspec_kw=dict(height_ratios=[1, 10]), \n",
    "                        sharex=True)\n",
    "\n",
    "axs[0].imshow(data[\"z_kato\"][None, :], \n",
    "              extent=(0, times[-1], 0, 1), \n",
    "              alpha=0.8, cmap=cmap, aspect=\"auto\")\n",
    "axs[0].set_xticks([])\n",
    "axs[0].set_yticks([])\n",
    "axs[0].set_ylabel(\"$z_{\\mathsf{Kato}}$\")\n",
    "\n",
    "axs[1].imshow(z_inf_perm[None, :], extent=(0, times[-1], -1, num_neurons + 2), \n",
    "              cmap=cmap, alpha=0.8, aspect=\"auto\")\n",
    "axs[1].plot(times, data[\"y\"][:, neuron_perm] + np.arange(num_neurons), \n",
    "            '-k', lw=1)\n",
    "axs[1].set_xlabel(\"time[s]\")\n",
    "axs[1].set_yticks(np.arange(num_neurons))\n",
    "axs[1].set_yticklabels([data[\"neuron_names\"][i] for i in neuron_perm], \n",
    "                       fontsize=10)\n",
    "axs[1].set_ylabel(\"neurons\")\n",
    "axs[1].set_ylim(-1, num_neurons+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FL-YpASgOtX"
   },
   "source": [
    "## Problem 3b: Compare and contrast\n",
    "\n",
    "We fit the model with twice as many discrete states as Kato et al (2015) reported. Split this time series into training and test sets and then sweep over $K$ to choose the number of discrete states by cross validation.\n",
    "\n",
    "_Write your analysis code in the cell below and describe your results in words below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aQU0L2Jh-4N"
   },
   "outputs": [],
   "source": [
    "###\n",
    "# \n",
    "# YOUR ANALYSIS CODE BELOW\n",
    "\n",
    "#\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxjrbBk6iR7D"
   },
   "source": [
    "## Problem 3c: Assessing variability across initializations\n",
    "\n",
    "The estimated parameters and the inferred posterior depend on the random intialization of the model. How much does that affect our results? To compare the segmentations across model fits, make a $T \\times T$ matrix that shows how often the most likely discrete state at time $t$ is the same as that at time $t'$.  I.e. let $\\hat{z}_t^{(i)} = \\mathrm{argmax}_k q(z_{t}^{(i)}=k)$, where the superscript $(i)$ denotes inferred posterior from the $i$-th model fit. Make a matrix whose entries are the average of the indicator $\\mathbb{I}[\\hat{z}_t^{(i)} = \\hat{z}_{t'}^{(i)}]$ taken over multiple fits of the MFA model with different random initializations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbwWh3GHiRFv"
   },
   "outputs": [],
   "source": [
    "###\n",
    "# \n",
    "# YOUR ANALYSIS CODE BELOW\n",
    "\n",
    "#\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tO3viJgAj-wB"
   },
   "source": [
    "## Switching linear dynamical systems\n",
    "\n",
    "If you're interested in digging deeper and fitting models that explicitly incorporate temporal dependencies, check out our [SSM](https://github.com/lindermanlab/ssm) package! To get you started, here's some code to start fitting SLDS to this dataset. \n",
    "\n",
    "Note that by default SSM uses a Laplace approximation for the continuous state posterior, which it fits with Newton's method. Likewise, it uses a Monte Carlo approximation for the M-step, which works with non-Gaussian observation models. For the special case of Gaussian observations, we could find the exact posterior update for $q(x_{1:T})$ and exact M-steps, as described in class. That would be a bit faster than the code below, but this is still fast enough for our purposes.\n",
    "\n",
    "For more information, check out the [demo notebooks](https://github.com/lindermanlab/ssm/tree/master/notebooks)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdIrJmZ9kRBj"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import ssm\n",
    "except:\n",
    "    !pip install git+https://github.com/lindermanlab/ssm.git@master#egg=ssm\n",
    "\n",
    "from ssm import SLDS\n",
    "slds = SLDS(num_neurons, num_states, latent_dim)\n",
    "elbos, posterior = slds.fit(from_t(data[\"y\"]), num_iters=50)\n",
    "\n",
    "# Plot the normalized elbos\n",
    "# Hopefully they're a bit higher than what we got with the MFA model!\n",
    "plot_elbos(elbos / num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBzHH9cVYuCk"
   },
   "source": [
    "# Submission Instructions\n",
    "\n",
    "\n",
    "**Formatting:** check that your code does not exceed 80 characters in line width. You can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
    "\n",
    "Download your notebook in .ipynb format and use the following commands to convert it to PDF. \n",
    "\n",
    "**Option 1 (best case): ipynb &rarr; pdf** Run the following command to convert to a PDF:\n",
    "```\n",
    "jupyter nbconvert --to pdf lab8_teamname.ipynb\n",
    "```\n",
    "\n",
    "Unfortunately, `nbconvert` sometimes crashes with long notebooks. If that happens, here are a few options:\n",
    "\n",
    "\n",
    "**Option 2 (next best): ipynb &rarr; tex &rarr; pdf**:\n",
    "```\n",
    "jupyter nbconvert --to latex lab8_teamname.ipynb\n",
    "pdflatex lab8_teamname.tex\n",
    "```\n",
    "\n",
    "**Option 3: ipynb &rarr; html &rarr; pdf**:\n",
    "```\n",
    "jupyter nbconvert --to html lab8_teamname.ipynb\n",
    "# open lab8_teamname.html in browser and print to pdf\n",
    "```\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "- `nbconvert`: If you're using Anaconda for package management, \n",
    "```\n",
    "conda install -c anaconda nbconvert\n",
    "```\n",
    "- `pdflatex`: It comes with standard TeX distributions like TeXLive, MacTex, etc. Alternatively, you can upload the .tex and supporting files to Overleaf (free with Stanford address) and use it to compile to pdf.\n",
    "\n",
    "**Upload** your .ipynb and .pdf files to Gradescope. \n",
    "\n",
    "**Only one submission per team!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "STATS220/320 Lab 8: Latent Variable Models, Variational EM, and Worm Brains.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
