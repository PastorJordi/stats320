{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STATS220/320 Lab 3: Calcium demixing and deconvolution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofgRAXiPlLLV"
      },
      "source": [
        "# Lab 3: Calcium demixing and deconvolution\n",
        "\n",
        "**STATS320: Machine Learning Methods for Neural Data Analysis**\n",
        "\n",
        "_Stanford University. Winter, 2021._\n",
        "\n",
        "---\n",
        "\n",
        "**Team Name:** _Your team name here_\n",
        "\n",
        "**Team Members:** _Names of everyone on your team here_\n",
        "\n",
        "*Due: 11:59pm Thursday, Feb 4, 2021 via GradeScope (see below)*\n",
        "\n",
        "---\n",
        "\n",
        "In this lab you'll write your own code for demixing and deconvolving calcium imaging videos. Demixing refers to the problem of identifying potentially overlapping neurons in the video and separating their fluorescence traces. Deconvolving refers to taking those traces and finding the times of spiking activity, which produce exponentially decaying transients in fluorescence. We'll frame it as a constrained and (partially) non-negative matrix factorization problem, inspired by the CNMF model of Pnevmatikakis et al, 2016, which is implemented in [CaImAn](https://github.com/flatironinstitute/CaImAn) (Giovannucci et al, 2019). More details and further references are in the course notes. We'll use [CVXpy](https://www.cvxpy.org/) to solve the convex optimization problems at the hard of this approach.\n",
        "\n",
        "**References**\n",
        "- Pnevmatikakis, Eftychios A., Daniel Soudry, Yuanjun Gao, Timothy A. Machado, Josh Merel, David Pfau, Thomas Reardon, et al. 2016. “Simultaneous Denoising, Deconvolution, and Demixing of Calcium Imaging Data.” Neuron 89 (2): 285–99.\n",
        "[link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4881387/)\n",
        "- Giovannucci, Andrea, Johannes Friedrich, Pat Gunn, Jérémie Kalfon, Brandon L. Brown, Sue Ann Koay, Jiannis Taxidis, et al. 2019. “CaImAn an Open Source Tool for Scalable Calcium Imaging Data Analysis.” eLife. [link](http://dx.doi.org/10.7554/eLife.38173)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UyLyy32uvcu"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI-GgS6CWRjY"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse\n",
        "from scipy.signal import butter, sosfilt\n",
        "from scipy.stats import norm\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from skimage.feature import peak_local_max\n",
        "\n",
        "# we'll use CVXpy to solve convex optimization problems\n",
        "import cvxpy as cvx\n",
        "\n",
        "# plotting stuff\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
        "from matplotlib.patches import Circle\n",
        "import seaborn as sns\n",
        "\n",
        "# helpers\n",
        "from tqdm.auto import trange\n",
        "from copy import deepcopy\n",
        "import warnings\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuQdctloR_kS"
      },
      "source": [
        "## Download example data\n",
        "\n",
        "This demo data was contributed by Sue Ann Koay and David Tank (Princeton University). \n",
        "It is also used in the CaImAn demo notebook. \n",
        "We used CaImAn and NoRMCorr to correct for motion artifacts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYoQE7qUmjP2"
      },
      "source": [
        "%%capture\n",
        "! wget -nc https://www.dropbox.com/s/3gl299gbw1mavcl/data.npy\n",
        "\n",
        "# Load the data and permute it so that time is the last axis,\n",
        "# as in the notes. \n",
        "data = np.load(\"data.npy\").transpose(1, 2, 0)\n",
        "height, width, num_frames = data.shape\n",
        "\n",
        "# Set some constants \n",
        "FPS = 30                        # frames per second in the movie\n",
        "NEURON_WIDTH = 10               # approximate width (in pixels) of a neuron\n",
        "GCAMP_TIME_CONST_SEC = 0.300    # reasonable guess for calcium decay time const."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-s9ZoJomK4Q"
      },
      "source": [
        "#@title Helper functions for movies and plotting { display-mode: \"form\" }\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "from tempfile import NamedTemporaryFile\n",
        "import base64\n",
        "\n",
        "# Set some plotting defaults\n",
        "sns.set_context(\"talk\")\n",
        "\n",
        "# initialize a color palette for plotting\n",
        "palette = sns.xkcd_palette([\"windows blue\",\n",
        "                            \"red\",\n",
        "                            \"medium green\",\n",
        "                            \"dusty purple\",\n",
        "                            \"orange\",\n",
        "                            \"amber\",\n",
        "                            \"clay\",\n",
        "                            \"pink\",\n",
        "                            \"greyish\"])\n",
        "\n",
        "_VIDEO_TAG = \"\"\"<video controls>\n",
        " <source src=\"data:video/x-m4v;base64,{0}\" type=\"video/mp4\">\n",
        " Your browser does not support the video tag.\n",
        "</video>\"\"\"\n",
        "\n",
        "def _anim_to_html(anim, fps=20):\n",
        "    # todo: todocument\n",
        "    if not hasattr(anim, '_encoded_video'):\n",
        "        with NamedTemporaryFile(suffix='.mp4') as f:\n",
        "            anim.save(f.name, fps=fps, extra_args=['-vcodec', 'libx264'])\n",
        "            video = open(f.name, \"rb\").read()\n",
        "        anim._encoded_video = base64.b64encode(video)\n",
        "\n",
        "    return _VIDEO_TAG.format(anim._encoded_video.decode('ascii'))\n",
        "\n",
        "def _display_animation(anim, fps=30, start=0, stop=None):\n",
        "    plt.close(anim._fig)\n",
        "    return HTML(_anim_to_html(anim, fps=fps))\n",
        "\n",
        "def play(movie, fps=FPS, speedup=1, fig_height=6):\n",
        "    # First set up the figure, the axis, and the plot element we want to animate\n",
        "    Py, Px, T = movie.shape\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(fig_height * Px/Py, fig_height))\n",
        "    im = plt.imshow(movie[..., 0], interpolation='None', cmap=plt.cm.gray)\n",
        "    tx = plt.text(0.75, 0.05, 't={:.3f}s'.format(0), \n",
        "                  color='white',\n",
        "                  fontdict=dict(size=12),\n",
        "                  horizontalalignment='left',\n",
        "                  verticalalignment='center', \n",
        "                  transform=ax.transAxes)\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        im.set_data(movie[..., i * speedup])\n",
        "        tx.set_text(\"t={:.3f}s\".format(i * speedup / fps))\n",
        "        return im, \n",
        "\n",
        "    # call the animator.  blit=True means only re-draw the parts that have changed.\n",
        "    anim = animation.FuncAnimation(fig, animate, \n",
        "                                   frames=T // speedup, \n",
        "                                   interval=1, \n",
        "                                   blit=True)\n",
        "    plt.close(anim._fig)\n",
        "\n",
        "    # return an HTML video snippet\n",
        "    print(\"Preparing animation. This may take a minute...\")\n",
        "    return HTML(_anim_to_html(anim, fps=30))\n",
        "\n",
        "def plot_problem_1d(local_correlations, filtered_correlations, peaks):\n",
        "    def _plot_panel(ax, im, title):\n",
        "        h = ax.imshow(im, cmap=\"Greys_r\")\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlim(0, width)\n",
        "        ax.set_ylim(height, 0)\n",
        "        ax.set_axis_off()\n",
        "\n",
        "        # add a colorbar of the same height\n",
        "        divider = make_axes_locatable(ax)\n",
        "        cax = divider.append_axes(\"right\", size=\"5%\", pad=\"2%\")\n",
        "        plt.colorbar(h, cax=cax)\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 6))\n",
        "    _plot_panel(axs[0], local_correlations, \"local correlations\")\n",
        "    _plot_panel(axs[1], filtered_correlations, \"filtered correlations\")\n",
        "    _plot_panel(axs[2], local_correlations, \"candidate neurons\")\n",
        "\n",
        "    # Draw circles around the peaks\n",
        "    for n, yx in enumerate(peaks):\n",
        "        y, x = yx\n",
        "        axs[2].add_patch(Circle((x, y), \n",
        "                                radius=NEURON_WIDTH/2, \n",
        "                                facecolor='none', \n",
        "                                edgecolor='red', \n",
        "                                linewidth=1))\n",
        "        \n",
        "        axs[2].text(x, y, \"{}\".format(n),\n",
        "                    horizontalalignment=\"center\",\n",
        "                    verticalalignment=\"center\",\n",
        "                    fontdict=dict(size=10, weight=\"bold\"),\n",
        "                    color='r')\n",
        "\n",
        "def plot_problem_2(traces, denoised_traces, amplitudes):\n",
        "    num_neurons, num_frames = traces.shape\n",
        "\n",
        "    # Plot the traces and our denoised estimates\n",
        "    scale = np.percentile(traces, 99.5, axis=1, keepdims=True)\n",
        "    offset = -np.arange(num_neurons)\n",
        "\n",
        "    # Plot points at the time frames where the (normalized) amplitudes are > 0.05\n",
        "    sparse_amplitudes = amplitudes.copy() / scale\n",
        "    sparse_amplitudes = np.isclose(sparse_amplitudes, 0, atol=0.05).astype(float)\n",
        "    sparse_amplitudes[sparse_amplitudes == 1] = np.nan\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot((traces / scale).T + offset , color=palette[0], lw=1, alpha=0.5)\n",
        "    plt.plot((denoised_traces / scale).T + offset, color=palette[0], lw=2)\n",
        "    plt.plot((sparse_amplitudes).T + offset, color=palette[1], marker='o', markersize=2)\n",
        "    plt.xlabel(\"time (frames)\")\n",
        "    plt.xlim(0, num_frames)\n",
        "    plt.ylabel(\"neuron\")\n",
        "    plt.yticks(-np.arange(num_neurons, step=5), labels=np.arange(num_neurons, step=5))\n",
        "    plt.ylim(-num_neurons, 2)\n",
        "    plt.title(\"raw and denoised fluorescence traces\")\n",
        "\n",
        "\n",
        "def plot_problem_3(flat_data, params, latents, hypers, plot_bkgd=True, indices=None):\n",
        "    U = params[\"footprints\"].reshape(-1, height, width)\n",
        "    u0 = params[\"bkgd_footprint\"].reshape(height, width)\n",
        "    C = latents[\"traces\"]\n",
        "    c0 = latents[\"bkgd_trace\"]\n",
        "    N, T = C.shape\n",
        "\n",
        "    if indices is None: indices = np.arange(N)\n",
        "    \n",
        "\n",
        "    def _plot_factor(footprint, trace, title):\n",
        "        fig, ax1 = plt.subplots(1, 1, figsize=(12, 6))\n",
        "        vlim = abs(footprint).max()\n",
        "        h = ax1.imshow(footprint, vmin=-vlim, vmax=vlim, cmap=\"RdBu\")\n",
        "        ax1.set_title(title)\n",
        "        ax1.set_axis_off()\n",
        "\n",
        "        # add a colorbar of the same height\n",
        "        divider = make_axes_locatable(ax1)\n",
        "        cax = divider.append_axes(\"right\", size=\"5%\", pad=\"2%\")\n",
        "        plt.colorbar(h, cax=cax)\n",
        "\n",
        "        ax2 = divider.append_axes(\"right\", size=\"150%\", pad=\"75%\")\n",
        "        ts = np.arange(T) / FPS\n",
        "        ax2.plot(ts, trace, color=palette[0], lw=2)\n",
        "        ax2.set_xlabel(\"time (sec)\")\n",
        "        ax2.set_ylabel(\"fluorescence trace\")\n",
        "        \n",
        "    if plot_bkgd:\n",
        "        _plot_factor(u0, c0, \"background\")\n",
        "\n",
        "    for n in indices:            \n",
        "        _plot_factor(U[n], C[n], \"neuron {}\".format(n))\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYIIcByKs1n3"
      },
      "source": [
        "## Movie of the data\n",
        "\n",
        "It takes a minute to render the animation..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D2xrCAsr8fv"
      },
      "source": [
        "# Play the motion corrected movie.\n",
        "play(data, speedup=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe29mYhmIMX_"
      },
      "source": [
        "# Part 1: Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDt8V-AMwwG-"
      },
      "source": [
        "## Problem 1a: Estimate the noise at each pixel and standardize\n",
        "\n",
        "We'll use a simple heuristic to estimate the noise. With slow calcium responses, most of the high frequency content (e.g. above 8Hz) should be noise.  Since Gaussian noise has a flat spectrum (we didn't prove this but it's a useful fact to know!), the standard deviation of the high frequency signal should tell us the noise at lower frequencies as well. \n",
        "\n",
        "In this problem, use `butter` and `sosfilt` to high-pass filter the data at 8Hz with a 10-th order Butterworth filter. (Recall Lab 1.) Then compute the standard deviation for each pixel using `np.std` and the `axis` keyword argument to get the standard deviation over time for each pixel.\n",
        "\n",
        "Finally, standardize the data by dividing each pixel by its standard deviation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCeekO586_y_"
      },
      "source": [
        "###\n",
        "# High-pass filter the data at 8Hz using a Butterworth filter.\n",
        "# That should filter out the calcium transients and give a \n",
        "# reasonable estimate of the noise. \n",
        "# \n",
        "# YOUR CODE BELOW\n",
        "sos = butter(..., output='sos')\n",
        "noise = sosfilt(...)\n",
        "sigmas = ...\n",
        "assert sigmas.shape == (height, width)\n",
        "#\n",
        "###\n",
        "\n",
        "# Plot the noise standard deviation for each pixel\n",
        "plt.imshow(sigmas, vmin=0)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Estimated noise per pixel\")\n",
        "plt.colorbar(label=\"noise std. deviation\")\n",
        "\n",
        "# Standardize the data by dividing each frame by the standard deviation\n",
        "std_data = data / sigmas[:, :, None]\n",
        "\n",
        "# Check that we got the same answer\n",
        "assert np.allclose(sigmas.mean(), 23.4768, atol=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpldwqRXd7Ug"
      },
      "source": [
        "## Problem 1b: Find peaks in the local correlation matrix \n",
        "\n",
        "\n",
        "**Step 1**\n",
        "To find candidate neurons, look for places in the image where nearby pixels are highly correlated with one another. \n",
        "\n",
        "The correlation between pixels $(i,j)$ and $(k,\\ell)$ is\n",
        "\\begin{align}\n",
        "\\rho_{ijk\\ell} = \\frac{1}{T} \\sum_{t=1}^T z_{ijt} z_{k\\ell t},\n",
        "\\end{align}\n",
        "where \n",
        "\\begin{align}\n",
        "z_{ijt} = \\frac{y_{ijt} - \\bar{y}_{ij}}{\\sigma_{ij}}\n",
        "\\end{align}\n",
        "denotes the z-scored data. $y_{ijt}$ is the fluorescence at pixel $(ij)$ and frame $t$, $\\bar{y}_{ij}$ is the average fluorescence at that pixel over time, and $\\sigma_{ij}$ is the standard deviation of fluorescence in that pixel. You've already compute the noise level $\\sigma_{ij}$ for each pixel and you computed $y_{ijt} / \\sigma_{ij}$ in Problem 1b. To compute $z$, simply subtract the mean of the standardized data. \n",
        "\n",
        "Now define the local correlation at pixel $(i,j)$ to be the average correlation with its neighbors to the north, south, east, and west:\n",
        "\\begin{align}\n",
        "\\bar{\\rho}_{ij} = \\tfrac{1}{4} \\left(\\rho_{ij,i-1,j} +  \\rho_{ij,i+1,j} + \\rho_{ij,i,j+1} + \\rho_{ij,i,j-1}\\right).\n",
        "\\end{align}\n",
        "If $(i,j)$ is a border cell, assume the correlation with out-of-bounds neighbors is zero.\n",
        "\n",
        "**Step 2**\n",
        "Use the `gaussian_filter` function with a standard deviation `sigma=NEURON_WIDTH/4` to smooth the local correlations.\n",
        "\n",
        "**Step 3**\n",
        "Find peaks in the smoothed local correlations using `peak_local_max`,\n",
        "which we imported from the `skimage.feature` package. Set a `min_distance` \n",
        "of 2 and play with the `threshold_abs` to get 30 neurons, which we think is a reasonable estimate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2enApj0YILuR"
      },
      "source": [
        "###\n",
        "# Compute the zscored data by subtracting the mean of \n",
        "# the standardized data.\n",
        "#\n",
        "# YOUR CODE BELOW\n",
        "\n",
        "# First z-score the data\n",
        "zscored_data = ...\n",
        "\n",
        "# Then compute the local correlation by summing correlations with \n",
        "# neighboring pixels\n",
        "local_correlations = np.zeros((height, width))\n",
        "local_correlations[1:,  :] += ... # N\n",
        "local_correlations[:-1, :] += ... # S\n",
        "local_correlations[:, :-1] += ... # E\n",
        "local_correlations[:,  1:] += ... # W\n",
        "local_correlations /= 4\n",
        "\n",
        "# Smooth the local correlations with a Gaussian filter of width 1/4\n",
        "# the width of a typical neuron. \n",
        "filtered_correlations = gaussian_filter(...)\n",
        "        \n",
        "# Finally, find peaks in the smoothed local correlations using \n",
        "# `peak_local_max`. Set a `min_distance` of 2 and play with the \n",
        "# `threshold_abs` to get 30 neurons, which we think is a reasonable estimate.\n",
        "min_distance = 2\n",
        "threshold_abs = ...\n",
        "peaks = peak_local_max(...)\n",
        "\n",
        "num_neurons = len(peaks)\n",
        "print(\"Found\", num_neurons, \"candidate neurons\")\n",
        "#\n",
        "###\n",
        "\n",
        "plot_problem_1d(local_correlations, filtered_correlations, peaks)\n",
        "assert num_neurons == 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSL2cR3msZW0"
      },
      "source": [
        "## Problem 1c [Short Answer]: Explain this heuristic\n",
        "\n",
        "Why are peaks in the local correlations indicative of neurons? Why did you filter the correlations? What would happen if you didn't use the Gaussian filter, or you used a Gaussian filter of a larger width? \n",
        "\n",
        "_Answer below this line_\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmmKumdT3puz"
      },
      "source": [
        "## Problem 1d: Initialize the footprints\n",
        "\n",
        "Initialize the footprints to,\n",
        "\\begin{align}\n",
        "u_{nij} \\propto \\mathcal{N}\\left(\\begin{bmatrix}i \\\\ j \\end{bmatrix} \\,\\bigg|\\, \\begin{bmatrix} \\mu_{n,i} \\\\ \\mu_{n,j} \\end{bmatrix}, \\frac{w^2}{4^2} I \\right)\n",
        "\\end{align}\n",
        "where $\\mu_{n} \\in \\mathbb{R}^2$ is the location of the peak for neuron $n$ and $w$ is the width of a typical neuron. \n",
        "\n",
        "There's a simple trick to compute the footprints: convolve a Gaussian filter with a matrix that is zeros everywhere except for a one at the location of the peak. The `gaussian_filter` function with `sigma` set to 1/4 the neuron width will do this for you.\n",
        "\n",
        "Finally, normalize the footprints so that $\\|u_n\\|=1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwZVoSQg3rtX"
      },
      "source": [
        "###\n",
        "# Initialize the spatial footprints for each neuron.\n",
        "# For each neuron, apply a Gaussian filter to a one-hot \n",
        "# matrix with a one in the peak position for that neuron.\n",
        "# \n",
        "# YOUR CODE BELOW\n",
        "footprints = np.zeros((num_neurons, height, width))\n",
        "for neuron in range(num_neurons):\n",
        "    ...\n",
        "    footprints[neuron] = gaussian_filter(...)\n",
        "    \n",
        "# Scale the footprints to be unit norm\n",
        "footprints /= ...\n",
        "\n",
        "#\n",
        "###\n",
        "\n",
        "# Check that they're unit norm\n",
        "assert np.allclose(np.linalg.norm(footprints, axis=(1,2)), 1.0)\n",
        "\n",
        "# Plot the superimposed footprints\n",
        "plt.imshow(footprints.sum(axis=0), cmap=\"Greys_r\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"superimposed footprints\")\n",
        "_ = plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2UK8Qav3MlZ"
      },
      "source": [
        "## Problem 1e: Initialize the background\n",
        "Set the spatial background factor $u_0$ equal to the **median of the standardized data** and set the temporal background factor to $c_{0} = 1_T$. The median should be more robust to the large spikes than the mean is. Then normalize by dividing $u_0$ by its norm $\\|u_0\\|_2$ and multiplying $c_0$ by $\\|u_0\\|_2$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw9ozwR72-3b"
      },
      "source": [
        "###\n",
        "# Initialize the background footprint and trace\n",
        "bkgd_trace = np.ones(num_frames)\n",
        "bkgd_footprint = ...\n",
        "\n",
        "# rescale so that the spatial background has norm 1\n",
        "scale = ...\n",
        "bkgd_footprint /= scale\n",
        "bkgd_trace *= scale\n",
        "#\n",
        "###\n",
        "\n",
        "# Plot the background factor\n",
        "plt.imshow(bkgd_footprint)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"background footprint $u_0$\")\n",
        "plt.colorbar()\n",
        "\n",
        "assert np.isclose(bkgd_footprint.mean(), 0.0056, atol=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX4z3cZmmUhI"
      },
      "source": [
        "## Initialize the traces\n",
        "\n",
        "We'll initialize the traces for Part 2 by computing the residual, projecting it onto each footprint in order, and updating the residual by subtracting off each neuron's contribution.\n",
        "\n",
        "If we've done a good job initializing, the traces should show clear spikes and the noise should be roughly in the range $[-3, +3]$ since the data is standardized to have standard deviation 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlkWcoBnmUKC"
      },
      "source": [
        "residual = std_data - np.einsum('ij,t->ijt', bkgd_footprint, bkgd_trace)\n",
        "traces = np.zeros((num_neurons, num_frames))\n",
        "for n in trange(num_neurons):\n",
        "    traces[n] = np.einsum('ij,ijt->t', footprints[n], residual)\n",
        "    residual -= np.einsum('ij,t->ijt', footprints[n], traces[n])\n",
        "\n",
        "# Plot trace for a single neuron\n",
        "n = 16\n",
        "plt.plot(traces[n], label=\"trace\")\n",
        "plt.hlines([-3, 3], 0, num_frames, \n",
        "        colors='r', linestyles=':', zorder=10, \n",
        "        label=\"noise level\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlim(0, num_frames)\n",
        "plt.xlabel(\"time (frames)\")\n",
        "plt.ylabel(\"fluorescence\")\n",
        "plt.title(\"neuron {}\".format(n))\n",
        "\n",
        "# check that we got the same answer using the parameters from parts 1a-1e.\n",
        "assert np.isclose(traces[16].mean(), 2.3430)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WOpyizUs6Ah"
      },
      "source": [
        "# Part 2: Deconvolving spikes from calcium traces\n",
        "\n",
        "In this part you'll use [CVXpy](https://www.cvxpy.org/) to deconvolve the calcium traces by solving a convex optimization problem. CVX is a \"Python-embedded modeling language for convex optimization problems,\" as the website says.  It provides an easy-to-use interface for translating convex optimization problems into code and easy access to a variety of underlying solvers. The key objects are:\n",
        "- `cvx.Variable` objects, which specify the variables you wish to optimize with respect to,\n",
        "- `cvx.Minimize` objects, which let you specify the objective you wish to minimize,\n",
        "- `cvx.Problem` objects, which combine an objective and a set of constraints. \n",
        "\n",
        "CVX also has lots of helper functions like\n",
        "- `cvx.sum_squares`, which computes the sum of squares of an array, and\n",
        "- `cx.norm`, which computes norms of the specified order.\n",
        "\n",
        "The following example is modified from the CVXpy homepage, linked above. It solves a least-squares problem with box constraints and compares the constrained and unconstrained solutions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wtp80R3qKdzz"
      },
      "source": [
        "# A simple CVX example...\n",
        "\n",
        "# Problem data.\n",
        "np.random.seed(1)\n",
        "A = np.random.randn(30, 20)\n",
        "b = np.random.randn(30)\n",
        "\n",
        "# Construct the problem.\n",
        "x = cvx.Variable(20)\n",
        "objective = cvx.Minimize(cvx.sum_squares(A @ x - b))\n",
        "constraints = [0 <= x, x <= 1]\n",
        "prob = cvx.Problem(objective, constraints)\n",
        "\n",
        "# The optimal objective value is returned by `prob.solve()`.\n",
        "# The optimal value for x is stored in `x.value`.\n",
        "result = prob.solve(verbose=False)\n",
        "\n",
        "# Plot the constrained optimum vs the unconstrained.\n",
        "plt.fill_between([0, 19], 0, 1, color='k', alpha=0.1, hatch='x', label=\"constraint set\")\n",
        "plt.plot(x.value, '-o', label=\"$0 \\leq x \\leq 1$\")\n",
        "plt.plot(np.linalg.lstsq(A, b, rcond=None)[0], '-', marker='.', label=\"unconstrained\")\n",
        "plt.xlim(0, 19)\n",
        "plt.ylim(-1, 1.0)\n",
        "plt.xlabel(\"$n$\")\n",
        "plt.ylabel(\"$x_n^\\star$\")\n",
        "plt.legend(loc=\"lower right\", fontsize=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T43ScVIr1N_j"
      },
      "source": [
        "## Problem 2a: Solve the convex optimization problem in dual form with CVX\n",
        "\n",
        "In this part of the lab you'll use CVXpy to maximize the log joint probability in its **dual form:**\n",
        "\n",
        "\\begin{align}\n",
        "    \\hat{c}_n, \\hat{b}_n = \\text{arg min}_{c_n, b_n} \\; \\|G c_n\\|_1 \n",
        "    \\quad \\text{subject to } \\quad \n",
        "    \\|\\mu_n - c_n - b_n\\|_2^2 &\\leq \\theta^2, \\; G c_n \\geq 0,\n",
        "\\end{align}\n",
        "\n",
        "where $\\mu_n = u_n^\\top R_n \\in \\mathbb{R}^T$ is the target for neuron $n$ and \n",
        "\n",
        "\\begin{align}\n",
        "    G &= \n",
        "    \\begin{bmatrix}\n",
        "    1             &               &        &        \\\\\n",
        "    -e^{-1/\\tau} & 1             &        &        \\\\\n",
        "    0             & -e^{-1/\\tau} & 1      &        \\\\\n",
        "                  & 0             & \\ddots & \\ddots \\\\\n",
        "    \\end{bmatrix}\n",
        "\\end{align}\n",
        "is the first order difference matrix. The spike amplitudes (i.e. jumps in the fluorescence) are given by $a_n = G c_n$, so you can think about the optimization problem as minimizing the $L^1$ norm of the jumps subject to a non-negativity constraint and an upper bound on the $L^2$ norm of the difference between the target $\\mu_n$ and the trace $c_n$. \n",
        "\n",
        "**Note** that this is a slight modification of the problem presented in class:\n",
        "1. Here we've added a bias term $b_n$, which will be helpful in cases where the target has a nonzero baseline.  Accounting for this possibility will lead to more robust estimates of the calcium traces.\n",
        "2. In class we presented the constraint $\\|\\mu_n - c_n - b_n\\|_2 \\leq \\theta$. CVXpy does a much better job at solving these \"second order cone programs,\" so in practice that's what you should do! For this problem, however, you'll square both sides, as written in the objective above. Squaring doesn't change the constraint set, but it will make it easier to compare to the \"primal\" form you'll solve in Problem 2c and 2d. \n",
        "3. There was a slight typo in my notes and slides that had the constraint $c_n \\geq 0$ rather than $G c_n \\geq 0$.  The former allows for positive and negative jumps (it just penalizes their absolute value), whereas the latter only allows positive jumps.\n",
        "\n",
        "We argued that a reasonable guess for the norm threshold is $\\theta = (1+\\epsilon) \\sigma \\sqrt{T}$.  For large $T$ and good estimates of the target, we should be able to set $\\epsilon$ pretty small.  Here, we'll use a fairly liberal upper bound and set $\\epsilon = 1$ since we're working with a short dataset and a poor initial guess.\n",
        "\n",
        "One of the great things about CVXpy is that it **works with SciPy's sparse matrices.** For example, you can use `scipy.sparse.diags` to construct the $G$ matrix. Under the hood, the solver will leverage the sparsity to run in linear time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itd6oizT04im"
      },
      "source": [
        "def deconvolve(trace, \n",
        "               noise_std=1.0, \n",
        "               epsilon=1.0,\n",
        "               tau=GCAMP_TIME_CONST_SEC * FPS,\n",
        "               full_output=False,\n",
        "               verbose=False):\n",
        "    \"\"\"Deconvolve a noisy calcium trace (aka \"target\") by solving a \n",
        "    the convex optimization problem described above.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    trace: a T numpy array containing the noisy trace (aka target).\n",
        "    noise_std: scalar noise standard deviation $\\sigma$\n",
        "    epsilon: extra slack for the norm constraint. \n",
        "        (Typically > 0 and certainly > -1)\n",
        "    tau: the time constant of the calcium indicator decay.\n",
        "    full_output: if True, return a dictionary with the deconvolved \n",
        "        trace and a bunch of extra info, otherwise just return the trace.\n",
        "    verbose: flag to pass to the CVX solver to print more info.\n",
        "    \"\"\"\n",
        "    assert trace.ndim == 1\n",
        "    T = len(trace)\n",
        "\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "\n",
        "    # Initialize the variable we're optimizing over\n",
        "    c = cvx.Variable(...)\n",
        "    b = cvx.Variable(...)\n",
        "\n",
        "    # Create the sparse matrix G with 1 on the diagonal and -e^{-1/\\tau} on the first lower diagonal\n",
        "    G = ...\n",
        "\n",
        "    # set the threshold to (1+\\epsilon) \\sigma \\sqrt{T}\n",
        "    theta = ...\n",
        "\n",
        "    # Define the objective function\n",
        "    objective = cvx.Minimize(...)\n",
        "    \n",
        "    # Set the constraints. \n",
        "    # PUT THE NORM CONSTRAINT FIRST, THEN THE NON-NEGATIVITY CONSTRAINT!\n",
        "    constraints = [..., ...]\n",
        "\n",
        "    # Construct the problem\n",
        "    prob = cvx.Problem(..., ...)\n",
        "    #\n",
        "    ###\n",
        "\n",
        "    # Solve the optimization problem. \n",
        "    try:\n",
        "        # First try the default solver then revert to SCS if it fails.\n",
        "        result = prob.solve(verbose=verbose)\n",
        "    except Exception as e:\n",
        "        print(\"Default solver failed with exception:\")\n",
        "        print(e)\n",
        "        print(\"Trying 'solver=SCS' instead.\")\n",
        "        # if this still fails we give up!\n",
        "        result = prob.solve(verbose=verbose, solver=\"SCS\")\n",
        "\n",
        "    # Make sure the result is finite (i.e. it found a feasible solution)\n",
        "    if np.isinf(result): \n",
        "        raise Exception(\"solver failed to find a feasible solution!\")\n",
        "\n",
        "    # Package complete results into a dict\n",
        "    all_results = dict(\n",
        "        trace=c.value,\n",
        "        baseline=b.value,\n",
        "        result=result,\n",
        "        amplitudes=G @ c.value,\n",
        "        lagrange_multiplier=constraints[0].dual_value[0]\n",
        "    )\n",
        "    assert np.size(constraints[0].dual_value) == 1, \\\n",
        "        \"Make sure your first constraint is on the norm of the residual.\"\n",
        "\n",
        "    return all_results if full_output else c.value\n",
        "\n",
        "# Solve the deconvolution problem for one neuron\n",
        "n = 16              # this neuron has particularly high SNR\n",
        "noise_std = 1.0     # \\sigma is 1 since we standardized the data\n",
        "epsilon = 1.0       # start with a generous tolerance of 2 \\sigma \\sqrt{T} (i.e. \\epsilon = 1)\n",
        "dual_results = deconvolve(traces[n], \n",
        "                          noise_std=noise_std, \n",
        "                          epsilon=epsilon,\n",
        "                          full_output=True, \n",
        "                          verbose=True)\n",
        "\n",
        "# Plot \n",
        "plt.plot(traces[n], color=palette[0], lw=1, alpha=0.5, label=\"raw\")\n",
        "plt.plot(dual_results[\"trace\"] + dual_results[\"baseline\"], \n",
        "         color=palette[0], lw=2, label=\"deconvolved\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlim(0, num_frames)\n",
        "plt.xlabel(\"time (frames)\")\n",
        "plt.ylabel(\"fluorescence\")\n",
        "_ = plt.title(\"neuron {}\".format(n))\n",
        "\n",
        "# Check your answer\n",
        "assert np.isclose(dual_results[\"result\"], 563.4, 1e-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwRBOndN1H9R"
      },
      "source": [
        "## Plot solutions as a function of $\\epsilon$ (and hence of $\\theta$)\n",
        "\n",
        "Compute and plot the solutions (in separate figures) for a range of $\\epsilon$ values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHQDls6w15Nu"
      },
      "source": [
        "epsilons = [0, 0.25, 0.5, 0.75, 1, 2, 5, 10]\n",
        "for epsilon in epsilons:\n",
        "    # deconvolve with this epsilon\n",
        "    dual_results = deconvolve(traces[n], \n",
        "                              noise_std=noise_std, \n",
        "                              epsilon=epsilon,\n",
        "                              full_output=True, \n",
        "                              verbose=False)\n",
        "    \n",
        "    # Plot \n",
        "    plt.figure()\n",
        "    plt.plot(traces[n], color=palette[0], lw=1, alpha=0.5, label=\"raw\")\n",
        "    plt.plot(dual_results[\"trace\"] + dual_results[\"baseline\"], \n",
        "            color=palette[0], lw=2, label=\"\".format(epsilon))\n",
        "    plt.legend(loc=\"upper left\", fontsize=10)\n",
        "    plt.xlim(0, num_frames)\n",
        "    plt.xlabel(\"time (frames)\")\n",
        "    plt.ylabel(\"fluorescence\")\n",
        "    _ = plt.title(\"neuron {} $\\epsilon$ = {:.2f}\".format(n, epsilon))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67HdszCk3j8A"
      },
      "source": [
        "## Problem 2b [Short Answer]: Explain these results\n",
        "\n",
        "How does the solution change as you increase $\\epsilon$ and thereby increase $\\theta$? Why?\n",
        "\n",
        "_Answer below this line_\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht1ZnUYjTxNO"
      },
      "source": [
        "## Problem 2c [Math]: Relate the dual form to the primal\n",
        "\n",
        "Replacing the upper bound on the squared norm in Problem 2a with its Lagrangian, we obtain the following \"primal\" form of the problem:\n",
        "\n",
        "\\begin{align}\n",
        "    \\hat{c}_n, \\hat{b}_n = \\text{arg min}_{c_n, b_n} \\; \\eta (\\|\\mu_n - c_n - b_n\\|_2^2 - \\theta^2) + \\|G c_n\\|_1 \n",
        "    \\quad \\text{subject to } \\quad  G c_n \\geq 0,\n",
        "\\end{align}\n",
        "\n",
        "where $\\eta$ is the Lagrange multiplier.\n",
        "\n",
        "**Show** that this is equivalent to maximizing the log joint (with a baseline $b_n$) \n",
        "\n",
        "\\begin{align}\n",
        "\\hat{c}_n, \\hat{b}_n = \\text{arg max}_{c_n, b_n} \\mathcal{L}(c_n, b_n) &= -\\frac{1}{2\\sigma^2} \\|\\mu_n - c_n - b_n\\|_2^2 - \\lambda_n\\|G c_n\\|_1 \\quad \\text{subject to } \\quad  G c_n \\geq 0\n",
        "\\end{align}\n",
        "\n",
        "by **solving for the value of $\\lambda_n$** (in terms of $\\eta$ and $\\sigma$) that makes these problems equivalent. \n",
        "\n",
        "_Answer below this line_\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7WCkiEyk8Yw"
      },
      "source": [
        "## Problem 2d: Solve the problem in primal form with $\\lambda_n$ set to match the dual\n",
        "\n",
        "Solve the primal problem with CVX using the amplitude rate hyperparameter $\\lambda_n$ that you solved for in Problem 2d and the optimal Lagrange multiplier $\\eta$ output in Problem 2a.\n",
        "```\n",
        "dual_results[\"lagrange_multiplier\"]   # this is \\eta\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YPMnNTxThwi"
      },
      "source": [
        "def deconvolve_primal(trace, \n",
        "                      amplitude_rate,\n",
        "                      noise_std=1.0, \n",
        "                      tau=GCAMP_TIME_CONST_SEC * FPS,\n",
        "                      verbose=True,\n",
        "                      full_output=False):\n",
        "    \"\"\"Deconvolve a noisy calcium trace (aka \"target\") by solving a \n",
        "    the convex optimization problem in the primal form.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    trace: a T numpy array containing the noisy trace.\n",
        "    amplitude_rate: non-negative rate (inverse scale) parameter $\\lambda$\n",
        "    noise_std: scalar noise standard deviation $\\sigma$\n",
        "    tau: the time constant of the calcium indicator decay.\n",
        "    full_output: if True, return a dictionary with the deconvolved \n",
        "        trace and a bunch of extra info, otherwise just return the trace.\n",
        "    verbose: flag to pass to the CVX solver to print more info.\n",
        "    \"\"\"\n",
        "    assert trace.ndim == 1\n",
        "    T = len(trace)\n",
        "\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "\n",
        "    # Initialize the variable we're optimizing over\n",
        "    c = cvx.Variable(...)\n",
        "    b = cvx.Variable(...)\n",
        "\n",
        "    # Create the sparse matrix G with 1 on the diagonal and -e^{-1/\\tau} on the first lower diagonal\n",
        "    G = ...\n",
        "\n",
        "    # Define the objective function\n",
        "    objective = cvx.Minimize(...)\n",
        "    constraints = [...]\n",
        "    prob = cvx.Problem(...)\n",
        "    #\n",
        "    ###\n",
        "    \n",
        "    # Solve the optimization problem\n",
        "    result = prob.solve(verbose=verbose)\n",
        "    if np.isinf(result): \n",
        "        raise Exception(\"solver failed to find a feasible solution!\")\n",
        "\n",
        "    all_results = dict(\n",
        "        trace=c.value,\n",
        "        baseline=b.value,\n",
        "        result=result,\n",
        "        amplitudes=G @ c.value\n",
        "    )\n",
        "    return all_results if full_output else c.value\n",
        "\n",
        "\n",
        "# Solve the deconvolution problem in the dual form\n",
        "n = 16              # this neuron has particularly high SNR\n",
        "noise_std = 1.0     # \\sigma is 1 since we standardized the data\n",
        "epsilon = 1.0       # start with a generous tolerance of 2 \\sigma \\sqrt{T} (i.e. \\epsilon = 1)\n",
        "dual_results = deconvolve(traces[n], \n",
        "                          noise_std=noise_std, \n",
        "                          epsilon=epsilon,\n",
        "                          full_output=True, \n",
        "                          verbose=True)\n",
        "\n",
        "\n",
        "###\n",
        "# Convert the optimal Lagrange multiplier returned in Problem 2a\n",
        "# to a hyperparameter $\\lambda_n$ that sets the rate (inverse scale)\n",
        "# of the exponential prior on spike amplitudes. The multiplier `eta` is in \n",
        "# `dual_results['lagrange_multiplier']` and \\sigma is set by `noise_std`.\n",
        "#\n",
        "# YOUR CODE BELOW\n",
        "amplitude_rate = ...\n",
        "###\n",
        "\n",
        "# Solve the problem in primal form\n",
        "primal_results = deconvolve_primal(traces[n], \n",
        "                                   amplitude_rate=amplitude_rate, \n",
        "                                   verbose=True, \n",
        "                                   full_output=True)\n",
        "\n",
        "# Plot raw, primal, and dual optimal trace for neuron n\n",
        "plt.plot(traces[n], color=palette[0], lw=1, alpha=0.5, label=\"raw\")\n",
        "plt.plot(dual_results[\"trace\"] + dual_results[\"baseline\"],\n",
        "         color=palette[0], ls='-', lw=2, label=\"dual\")\n",
        "plt.plot(primal_results[\"trace\"] + primal_results[\"baseline\"], \n",
        "         color=palette[1], ls='-', lw=1, label=\"primal\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlim(0, num_frames)\n",
        "plt.xlabel(\"time (frames)\")\n",
        "plt.ylabel(\"fluorescence\")\n",
        "plt.title(\"neuron {}\".format(n))\n",
        "\n",
        "# Make sure the traces are the same!\n",
        "primal_diff = abs(dual_results[\"trace\"] - primal_results[\"trace\"]).max()\n",
        "print(\"primal and dual solutions match to absolute value: {:.4f}\".format(primal_diff))\n",
        "assert np.allclose(dual_results[\"trace\"], primal_results[\"trace\"], atol=1e-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdfARactBOPx"
      },
      "source": [
        "## Compute all deconvolved traces and plot them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZjkStbq1CJk"
      },
      "source": [
        "# Deconvolve each trace and concatenate the results\n",
        "deconvolved_traces = np.zeros_like(traces)\n",
        "amplitudes = np.zeros_like(traces)\n",
        "for neuron in trange(num_neurons):\n",
        "    try:\n",
        "        all_results = deconvolve(traces[neuron], epsilon=1.0, full_output=True)\n",
        "    except Exception as e:\n",
        "        print(\"Failed to extract trace for neuron {}\".format(neuron))\n",
        "        all_results = deconvolve(traces[neuron], epsilon=1.0, verbose=True, full_output=True)\n",
        "        raise(e)\n",
        "    deconvolved_traces[neuron] = all_results[\"trace\"]\n",
        "    amplitudes[neuron] = all_results[\"amplitudes\"]\n",
        "\n",
        "plot_problem_2(traces, deconvolved_traces, amplitudes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It5amaIbDtyp"
      },
      "source": [
        "# Part 3: Demix and deconvolve the calcium imaging video\n",
        "\n",
        "In this part you'll write the updates for MAP estimation in the constrained non-negative matrix factorization model. \n",
        "\n",
        "As in the notes and slides, we will operate on the **flattened** data and residuals by raveling the frames into 1d vectors.\n",
        "\n",
        "**Note** that unlike CNMF (Pnevmatikakis et al, 2016), we're not going to constrain the footprints to be non-negative. Instead, we'll just assume they are normalized, since that's a bit easier to and it makes a clearer connection to the spike sorting algorithms from Lab 2. It would be a simple extension to enforce non-negativity, and the course notes describe how."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdWHVDWvYsnt"
      },
      "source": [
        "## Flatten the pixel dimensions and package the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh1ClO8B8EeH"
      },
      "source": [
        "flat_data = std_data.reshape(-1, num_frames)\n",
        "flat_footprints = footprints.reshape(num_neurons, -1)\n",
        "flat_bkgd_footprint = bkgd_footprint.reshape(-1)\n",
        "\n",
        "# The latent variables are the traces (they grow with time).\n",
        "init_latents = dict(\n",
        "    traces=np.zeros((num_neurons, num_frames)),\n",
        "    bkgd_trace=bkgd_trace,\n",
        ")\n",
        "\n",
        "# The parameters are the spatial components \n",
        "# (they don't grow with the length of the data).\n",
        "init_params = dict(\n",
        "    footprints=flat_footprints,\n",
        "    bkgd_footprint=flat_bkgd_footprint\n",
        ")\n",
        "\n",
        "# The hyperparameters specify the number of neurons,\n",
        "# the noise standard deviation ($\\sigma = 1$ since we standardized the data),\n",
        "# the prior variance of the background trace (something really large),\n",
        "# and the tolerance for our norm constrain ($\\epsilon$).\n",
        "hypers = dict(\n",
        "    num_neurons=num_neurons,\n",
        "    noise_std=1.0,\n",
        "    bkgd_trace_var=1e6,\n",
        "    epsilon=1.0,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILoHcgb7yRTr"
      },
      "source": [
        "## Problem 3a: Write a function to compute the log likelihood given the residual\n",
        "\n",
        "The log likelihood is \n",
        "\\begin{align}\n",
        "\\log p(Y \\mid U, C, u_0, c_0, \\sigma^2) &= \n",
        "\\sum_{p=1}^P \\sum_{t=1}^T \\log \\mathcal{N}(y_{pt} \\mid \\sum_{n=1}^N u_{np} c_{nt} + u_{0p} c_{0t}, \\sigma^2) \\\\\n",
        "&= -\\frac{PT}{2} \\log (2\\pi \\sigma^2) -\\frac{1}{2\\sigma^2} \\left\\|Y - U^\\top C - u_0 c_0^\\top \\right\\|_F^2\n",
        "\\end{align}\n",
        "Write a function to compute the log likelihood given the precomputed residual $R = Y - U^\\top C - u_0 c_0^\\top $.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SafHuShnofsh"
      },
      "source": [
        "def log_likelihood_residual(residual, noise_std):\n",
        "    \"\"\" Evaluate the log joint probability of the data \n",
        "    given the precomputed residual $Y - U^T C - u_0 c_0^T$\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    residual: a PxT numpy array containing the residual noise\n",
        "        after subtracting the neuron and background contributions.\n",
        "\n",
        "    noise_std: scalar per-pixel standard deviation $\\sigma$\n",
        "    \"\"\"\n",
        "    ### \n",
        "    # YOUR CODE BELOW\n",
        "    ll = ...\n",
        "    ###\n",
        "    return ll / residual.size\n",
        "    \n",
        "# check it on the flat data (as if C and c_0 were zero)\n",
        "assert np.isclose(log_likelihood_residual(flat_data, hypers[\"noise_std\"]), -4.6867, atol=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FpZC0-0zW1B"
      },
      "source": [
        "## Problem 3b: Optimize a trace\n",
        "\n",
        "Optimize a single neuron's trace using the `deconvolve` function you wrote in Problem 2a. The target is $\\mu_n = u_n^\\top R_n$ where $R_n$ is the residual for this neuron. The residual is given as input to this function.\n",
        "\n",
        "**Note:** In your final version, make sure you have `verbose=False` so that the final code doesn't print a bunch of unnecessary stuff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa3C6cfh_Ukj"
      },
      "source": [
        "def _update_trace(neuron, residual, params, latents, hypers):\n",
        "    \"\"\"Update a single neuron's trace by calling your `deconvolve` function.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    neuron: integer index of which neuron to update\n",
        "    residual: a PxT numpy array containing the residual for this neuron.\n",
        "    params: dictionary with keys ['footprints', 'bkgd_footprint']\n",
        "    latents: dictionary with keys ['traces', 'bkgd_trace']\n",
        "    hypers: dictionary with keys ['num_neurons', 'epsilon', 'noise_std', 'bkgd_trace_var']\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "    ...\n",
        "    trace = ...\n",
        "    #\n",
        "    ###\n",
        "    assert np.all(np.isfinite(trace))\n",
        "    return trace\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4HW8kqD1s3v"
      },
      "source": [
        "## Problem 3c: Optimize a footprint\n",
        "\n",
        "Optimize a single neuron's footprint as follows,\n",
        "\\begin{align}\n",
        "u_n = \\frac{R_n c_n}{\\|R_n c_n\\|}\n",
        "\\end{align}\n",
        "where $R_n$ is the given residual and $c_n$ is the neuron's trace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ_xw9YY1sHW"
      },
      "source": [
        "def _update_footprint(neuron, residual, params, latents, hypers):\n",
        "    \"\"\"Update a single neuron's footprint.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    neuron: integer index of which neuron to update\n",
        "    residual: a PxT numpy array containing the residual for this neuron.\n",
        "    params: dictionary with keys ['footprints', 'bkgd_footprint']\n",
        "    latents: dictionary with keys ['traces', 'bkgd_trace']\n",
        "    hypers: dictionary with keys ['num_neurons', 'epsilon', 'noise_std', 'bkgd_trace_var']\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "    ...\n",
        "    footprint = ...\n",
        "    #\n",
        "    ###\n",
        "    assert np.all(np.isfinite(footprint))\n",
        "    return footprint\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYtR8VGAsg3V"
      },
      "source": [
        "## Problem 3d: Optimize the background\n",
        "\n",
        "Optimize the background trace by projecting the residual onto the background footprint and shrinking the result slightly,\n",
        "\\begin{align}\n",
        "c_0 = \\left(\\frac{\\varsigma_0^2}{\\sigma^2 + \\varsigma_0^2}\\right) u_0^\\top R_0 \n",
        "\\end{align}\n",
        "where $R_0 = Y - U^\\top C$ is the background residual and $\\varsigma_0^2$ is the prior variance on the background trace. (See the course notes for a derivation.)\n",
        "\n",
        "Update the background footprint by setting it to,\n",
        "\\begin{align}\n",
        "u_0 = \\frac{R_0 c_0}{\\|R_0 c_0\\|}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3lvVemWsWuO"
      },
      "source": [
        "def _update_bkgd_trace(residual, params, latents, hypers):\n",
        "    \"\"\"Update the background trace $c_0$.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    residual: a PxT numpy array containing the residual for the background.\n",
        "    params: dictionary with keys ['footprints', 'bkgd_footprint']\n",
        "    latents: dictionary with keys ['traces', 'bkgd_trace']\n",
        "    hypers: dictionary with keys ['num_neurons', 'epsilon', 'noise_std', 'bkgd_trace_var']\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "    ...\n",
        "    shrink_factor = ...\n",
        "    target = ...\n",
        "    #\n",
        "    ###\n",
        "    return shrink_factor * target\n",
        "\n",
        "def _update_bkgd_footprint(residual, params, latents, hypers):\n",
        "    \"\"\"Update the background footprint $u_0$.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    residual: a PxT numpy array containing the residual for the background.\n",
        "    params: dictionary with keys ['footprints', 'bkgd_footprint']\n",
        "    latents: dictionary with keys ['traces', 'bkgd_trace']\n",
        "    hypers: dictionary with keys ['num_neurons', 'epsilon', 'noise_std', 'bkgd_trace_var']\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "    ...\n",
        "    bkgd_footprint = ...\n",
        "    #\n",
        "    ###\n",
        "    return bkgd_footprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9FL_P01plBx"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "Now we'll put these steps together into the MAP estimation algorithm. It's very similar to what you implemented in Lab 2. It amounts to:\n",
        "- Initialize the residual $R = Y - U^\\top C - u_0 c_0^\\top$\n",
        "- Repeat until convergence:\n",
        "    - For each neuron $n=1,\\ldots,N$:\n",
        "        - Update the residual to $R = R + u_n c_n^\\top$\n",
        "        - Update the trace $c_n$ by applying your `deconvolve` function from Part 2a to the target $\\mu_n = u_n^\\top R$\n",
        "        - Update the footprint to $u_n = \\frac{R c_n}{\\|R c_n\\|}$\n",
        "        - Downdate the residual to $R = R - u_n c_n^\\top$ using the new footprint and trace\n",
        "    - Update the background:\n",
        "        - Update the residual to $R = R + u_0 c_0^\\top$\n",
        "        - Set the background trace to $c_0 = \\frac{\\varsigma_0^2}{\\sigma^2 + \\varsigma_0^2} u_0^\\top R$ where $\\varsigma_0^2$ is the prior variance of the background trace. (We will set it to be very large so that we barely shrink the background trace.)\n",
        "        - Set the background footprint to $u_0 = \\frac{R c_0}{\\|R c_0\\|}$\n",
        "        - Downdate the residual to $R = R - u_0 c_0^\\top$ using the new background footprint and trace.\n",
        "    - Compute the log likelihood using the residual $R$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oIpGwwk2dKJ"
      },
      "source": [
        "def map_estimate(flat_data,\n",
        "                 init_latents,\n",
        "                 init_params,\n",
        "                 hypers,\n",
        "                 num_iters=10,\n",
        "                 tol=2e-4):\n",
        "    \"\"\"Fit the CNMF model via coordinate ascent.\n",
        "    \"\"\"\n",
        "    \n",
        "    # make a fancy reusable progress bar for the inner loops over neurons.\n",
        "    outer_pbar = trange(num_iters)\n",
        "    inner_pbar = trange(hypers[\"num_neurons\"])\n",
        "    inner_pbar.set_description(\"updating neurons\")\n",
        "\n",
        "    # make a copy of the model rather than overwriting the inputs\n",
        "    latents = deepcopy(init_latents)\n",
        "    params = deepcopy(init_params)\n",
        "\n",
        "    # initialize the residual\n",
        "    residual = flat_data.copy()\n",
        "    residual -= params[\"footprints\"].T @ latents[\"traces\"]\n",
        "    residual -= np.outer(params[\"bkgd_footprint\"], latents[\"bkgd_trace\"])\n",
        "\n",
        "    # track log likelihoods over iterations\n",
        "    lls = [log_likelihood_residual(residual, hypers[\"noise_std\"])]\n",
        "    outer_pbar.set_description(\"LL: {:.4f}\".format(lls[-1]))\n",
        "\n",
        "    # coordinate ascent\n",
        "    for itr in outer_pbar:\n",
        "        \n",
        "        # update neurons one at a time\n",
        "        inner_pbar.reset()\n",
        "        for n in range(hypers[\"num_neurons\"]):\n",
        "            # update the residual (add $u_n c_n^\\top$)\n",
        "            residual += np.outer(params[\"footprints\"][n], latents[\"traces\"][n])\n",
        "    \n",
        "            # update the trace and footprint with the residual\n",
        "            latents[\"traces\"][n] = _update_trace(n, residual, params, latents, hypers)\n",
        "            params[\"footprints\"][n] = _update_footprint(n, residual, params, latents, hypers)\n",
        "            \n",
        "            # downdate the residual (subtract $u_n c_n^\\top$)\n",
        "            residual -= np.outer(params[\"footprints\"][n], latents[\"traces\"][n])\n",
        "\n",
        "            # step the progress bar\n",
        "            inner_pbar.update()\n",
        "\n",
        "        # update the background\n",
        "        residual += np.outer(params[\"bkgd_footprint\"], latents[\"bkgd_trace\"])\n",
        "        latents[\"bkgd_trace\"] = _update_bkgd_trace(residual, params, latents, hypers)\n",
        "        params[\"bkgd_footprint\"] = _update_bkgd_footprint(residual, params, latents, hypers)\n",
        "        residual -= np.outer(params[\"bkgd_footprint\"], latents[\"bkgd_trace\"])\n",
        "        \n",
        "        # compute the log likelihood \n",
        "        lls.append(log_likelihood_residual(residual, hypers[\"noise_std\"]))\n",
        "        outer_pbar.set_description(\"LL: {:.4f}\".format(lls[-1]))\n",
        "        \n",
        "        # check for convergence\n",
        "        if abs(lls[-1] - lls[-2]) < tol:\n",
        "            print(\"Convergence detected!\")\n",
        "            break\n",
        "    \n",
        "    return np.array(lls), latents, params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh1v8NhFFNGI"
      },
      "source": [
        "## Fit it!\n",
        "\n",
        "This should take about 5 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDmCcwkh39ZL"
      },
      "source": [
        "# Fit it!\n",
        "lls, latents, params = map_estimate(flat_data, init_latents, init_params, hypers)\n",
        "\n",
        "# Plot the log likelihoods\n",
        "plt.plot(lls, '-o',)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.xlim(-.1, len(lls) - .9)\n",
        "plt.ylabel(\"Log Likelihood\")\n",
        "plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szs4eIDYOxXn"
      },
      "source": [
        "## Plot the inferred footprints and traces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brOpyqsT1yq-"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\") # Yes, we know we're creating a lot of figures...\n",
        "plot_problem_3(flat_data, params, latents, hypers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnIbegX2O4-j"
      },
      "source": [
        "## Problem 3e: Make a movie of the data, reconstruction, and residual\n",
        "\n",
        "Show a movie with the data, reconstruction, and residual side by side. If all goes well, the data should show a nice, clean movie of spiking neurons and the residual should mostly look like white noise. In practice, you'll probably still see some evidence of neurons in the residual, suggesting that the model still isn't perfect. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHou0U_HGg0q"
      },
      "source": [
        "###\n",
        "# Reconstruct the data and compute the residual.\n",
        "# Then make a movie of the data, reconstruction, and residual \n",
        "# side-by-side.\n",
        "#\n",
        "# YOUR CODE BELOW\n",
        "flat_recon = ...\n",
        "flat_residual = ...\n",
        "\n",
        "# Reshape into (height x width x frames) image stacks and concatenate along axis=1.\n",
        "movie = np.concatenate([...], axis=1)\n",
        "#\n",
        "###\n",
        "\n",
        "# Play the movie\n",
        "play(movie, speedup=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWM2YDdr8uDU"
      },
      "source": [
        "# Part 4: Discussion\n",
        "\n",
        "Hopefully you were successful in separating the neurons from the background and noise! Let's take a minute to reflect on the model and results. Write a couple paragraphs in response to the following prompts.\n",
        "\n",
        "- We mentioned a few times that actual CNMF implementations also constrain the footprints to be non-negative. Without this constraint, you probably found in the plots above (before Problem 3e) that some of these footprints contain negative values. Why is this unrealistic and what are the consequences of omitting this constraint?\n",
        "- What were the bottlenecks in the MAP estimation algorithm? Do you think implementing this model on a GPU, like we did in Lab 2, could help? Why or why not?\n",
        "- You probably noticed that the background has lots of rings in it, like little Cheerios. Why do you think that is?\n",
        "- We assumed that all neurons share the same time constant $\\tau$. Is that reasonable? How could you learn per-neuron time constants?\n",
        "- Do you think we can infer the number of underlying action potentials from the amplitude of the jumps in the calcium traces? \n",
        "\n",
        "_Answer below this line_\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvqShfKkBtNf"
      },
      "source": [
        "# Submission instructions\n",
        "\n",
        "Download your notebook in .ipynb format and use the following command to convert it to PDF\n",
        "```\n",
        "jupyter nbconvert --to pdf lab3_teamname.ipynb\n",
        "```\n",
        "If you're using Anaconda for package management, you can install `nbconvert` with\n",
        "```\n",
        "conda install -c anaconda nbconvert\n",
        "```\n",
        "Upload your .ipynb and .pdf files to Gradescope. \n",
        "\n",
        "**Only one submission per team!**"
      ]
    }
  ]
}