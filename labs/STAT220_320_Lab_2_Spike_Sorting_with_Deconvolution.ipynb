{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhZTZAxw1_2_"
   },
   "source": [
    "# Lab 2: Spike Sorting with Deconvolution\n",
    "\n",
    "\n",
    "**STATS320: Machine Learning Methods for Neural Data Analysis**\n",
    "\n",
    "_Stanford University. Winter, 2021._\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/slinderman/stats320/blob/main/labs/STAT220_320_Lab_2_Spike_Sorting_with_Deconvolution.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Team Name:** _Your team name here_\n",
    "\n",
    "**Team Members:** _Names of everyone on your team here_\n",
    "\n",
    "*Due: 11:59pm Thursday, Jan 28, 2021 via GradeScope (see below)*\n",
    "\n",
    "---\n",
    "\n",
    "This lab extends the basic spike sorting algorithm you developed in Lab 1. Rather than assuming putative spikes have been identified and windows around them have been extracted, here we'll model the multi-channel voltage time series directly using a convolutional matrix factorization model. We'll use PyTorch to implement the key operations&mdash;convolutions and cross-correlations&mdash;on a GPU. We'll test it on a small synthetic dataset first and then try it on the data from Lab 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dyv2VRJq2kGU"
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PWc6KZ91vya"
   },
   "outputs": [],
   "source": [
    "# First, import necessary libraries.\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# We'll use PyTorch for this lab\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions \n",
    "\n",
    "# Some helper utilities\n",
    "from tqdm.auto import trange\n",
    "import pickle\n",
    "\n",
    "# Plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZ61SzIpOE68"
   },
   "outputs": [],
   "source": [
    "# Specify that we want our tensors on the GPU and in float32\n",
    "device = torch.device('cuda')\n",
    "dtype = torch.float32\n",
    "\n",
    "# Helper function to convert between numpy arrays and tensors\n",
    "to_t = lambda array: torch.tensor(array, device=device, dtype=dtype)\n",
    "from_t = lambda tensor: tensor.to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1CBbW2uAT71U"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions for plotting (run this cell)\n",
    "\n",
    "# initialize a color palette for plotting\n",
    "palette = sns.xkcd_palette([\"windows blue\",\n",
    "                            \"red\",\n",
    "                            \"medium green\",\n",
    "                            \"dusty purple\",\n",
    "                            \"orange\",\n",
    "                            \"amber\",\n",
    "                            \"clay\",\n",
    "                            \"pink\",\n",
    "                            \"greyish\"])\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "\n",
    "def plot_data(timestamps,\n",
    "              data, \n",
    "              plot_slice=slice(0, 6000), \n",
    "              labels=None, \n",
    "              spike_times=None,\n",
    "              neuron_channels=None,\n",
    "              spike_width=81,\n",
    "              scale=10,\n",
    "              figsize=(12, 9),\n",
    "              cmap=\"jet\"):\n",
    "    n_channels, n_samples = data.shape\n",
    "    cmap = get_cmap(cmap) if isinstance(cmap, str) else cmap\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(timestamps[plot_slice], \n",
    "             data.T[plot_slice] - scale * np.arange(n_channels), \n",
    "             '-k', lw=1)\n",
    "    \n",
    "    if not any(x is None for x in [labels, spike_times, neuron_channels]):\n",
    "        # Plot the ground truth spikes and assignments\n",
    "        n_units = labels.max()\n",
    "        in_slice = (spike_times >= plot_slice.start) & (spike_times < plot_slice.stop)\n",
    "        labels = labels[in_slice]\n",
    "        times = spike_times[in_slice]\n",
    "        for i in range(n_units):\n",
    "            i_channels = neuron_channels[i]\n",
    "            for t in times[labels == i]:\n",
    "                window = slice(t, t + spike_width)\n",
    "                plt.plot(timestamps[window], \n",
    "                         data.T[window, i_channels] - scale * np.arange(n_channels)[i_channels],\n",
    "                         color=cmap(i / (n_units-1)),\n",
    "                         alpha=0.5,\n",
    "                         lw=2)\n",
    "                \n",
    "    plt.yticks(-scale * np.arange(1, n_channels+1, step=2), \n",
    "            np.arange(1, n_channels+1, step=2) + 1)\n",
    "    plt.xlabel(\"time [s]\")\n",
    "    plt.ylabel(\"channel\")\n",
    "    plt.xlim(timestamps[plot_slice.start], timestamps[plot_slice.stop])\n",
    "    plt.ylim(-scale * n_channels, scale)\n",
    "\n",
    "\n",
    "def plot_templates(templates, \n",
    "                   indices,\n",
    "                   scale=0.1,\n",
    "                   n_cols=8,\n",
    "                   panel_height=6,\n",
    "                   panel_width=1.25,\n",
    "                   colors=('k',),\n",
    "                   label=\"neuron\",\n",
    "                   sample_freq=30000,\n",
    "                   fig=None,\n",
    "                   axs=None):\n",
    "    n_subplots = len(indices)\n",
    "    n_cols = min(n_cols, n_subplots)\n",
    "    n_rows = int(np.ceil(n_subplots / n_cols))\n",
    "\n",
    "    if fig is None and axs is None:\n",
    "        fig, axs = plt.subplots(n_rows, n_cols, \n",
    "                                figsize=(panel_width * n_cols, panel_height * n_rows),\n",
    "                                sharex=True, sharey=True)\n",
    "    \n",
    "    n_units, n_channels, spike_width = templates.shape\n",
    "    timestamps = np.arange(-spike_width // 2, spike_width//2) / sample_freq\n",
    "    for i, (ind, ax) in enumerate(zip(indices, np.ravel(axs))):\n",
    "        color = colors[i % len(colors)]\n",
    "        ax.plot(timestamps * 1000, \n",
    "                templates[ind].T - scale * np.arange(n_channels), \n",
    "                '-', color=color, lw=1)\n",
    "        \n",
    "        ax.set_title(\"{} {:d}\".format(label, ind + 1))\n",
    "        ax.set_xlim(timestamps[0] * 1000, timestamps[-1] * 1000)\n",
    "        ax.set_yticks(-scale * np.arange(n_channels+1, step=4))\n",
    "        ax.set_yticklabels(np.arange(n_channels+1, step=4) + 1)\n",
    "        ax.set_ylim(-scale * n_channels, scale)\n",
    "\n",
    "        if i // n_cols == n_rows - 1:\n",
    "            ax.set_xlabel(\"time [ms]\")\n",
    "        if i % n_cols == 0:\n",
    "            ax.set_ylabel(\"channel\")\n",
    "\n",
    "        # plt.tight_layout(pad=0.1)\n",
    "\n",
    "    # hide the remaining axes\n",
    "    for ax in np.ravel(axs)[n_subplots:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    return fig, axs\n",
    "\n",
    "\n",
    "def plot_model(templates, amplitude, data, scores=None, lw=2, figsize=(12, 6)):\n",
    "    \"\"\"Plot the raw data as well as the underlying signal amplitudes and templates.\n",
    "    \n",
    "    amplitude: (N,T) array of underlying signal amplitude\n",
    "    template: (N,C,D) array of template that is convolved with signal\n",
    "    data: (C, T) array (channels x time)\n",
    "    scores: optional (N,T) array of correlations between data and template\n",
    "    \"\"\"    \n",
    "    # prepend dimension if data and template are 1d\n",
    "    data = np.atleast_2d(data)\n",
    "    C, T = data.shape\n",
    "    amplitude = np.atleast_2d(amplitude)\n",
    "    N, _ = amplitude.shape\n",
    "    templates = templates.reshape(N, C, -1)\n",
    "    D = templates.shape[-1]\n",
    "    dt = np.arange(D)\n",
    "    if scores is not None:\n",
    "        scores = np.atleast_2d(scores)\n",
    "\n",
    "    # Set up figure with 2x2 grid of panels\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = GridSpec(2, N + 1, height_ratios=[1, 2], width_ratios=[1] * N + [2 * N])\n",
    "\n",
    "    # plot the templates\n",
    "    t_spc = 1.05 * abs(templates).max()\n",
    "    for n in range(N):\n",
    "        ax = fig.add_subplot(gs[1, n])\n",
    "        ax.plot(dt, templates[n].T - t_spc * np.arange(C), \n",
    "                '-', color=palette[n % len(palette)], lw=lw)\n",
    "        ax.set_xlabel(\"delay $d$\")\n",
    "        ax.set_xlim([0, D])\n",
    "        ax.set_yticks(-t_spc * np.arange(C))\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_ylim(-C * t_spc, t_spc)\n",
    "        if n == 0:\n",
    "            ax.set_ylabel(\"channels $c$\")\n",
    "        ax.set_title(\"$W_{{ {} }}$\".format(n+1))\n",
    "\n",
    "    # plot the amplitudes for each neuron\n",
    "    ax = fig.add_subplot(gs[0, -1])\n",
    "    a_spc = 1.05 * abs(amplitude).max()\n",
    "    if scores is not None:\n",
    "        a_spc = max(a_spc, 1.05 * abs(scores).max())\n",
    "\n",
    "    for n in range(N):\n",
    "        ax.plot(amplitude[n] - a_spc * n, '-', color=palette[n % len(palette)], lw=lw)\n",
    "        \n",
    "        if scores is not None:\n",
    "            ax.plot(scores[n] - a_spc * n, ':', color=palette[n % len(palette)], lw=lw,\n",
    "                label=\"$y \\star W$\")\n",
    "        \n",
    "    ax.set_xlim([0, T])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticks(-a_spc * np.arange(N))\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_ylabel(\"neurons $n$\")\n",
    "    ax.set_title(\"amplitude $a$\")\n",
    "    if scores is not None:\n",
    "        ax.legend()\n",
    "\n",
    "    # plot the data\n",
    "    ax = fig.add_subplot(gs[1, -1])\n",
    "    d_spc = 1.05 * abs(data).max()\n",
    "    ax.plot(data.T - d_spc * np.arange(C), '-', color='gray', lw=lw)\n",
    "    ax.set_xlabel(\"time $t$\")\n",
    "    ax.set_xlim([0, T])\n",
    "    ax.set_yticks(-d_spc * np.arange(C))\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_ylim(-C * d_spc, d_spc)\n",
    "    # ax.set_ylabel(\"channels $c$\")\n",
    "    ax.set_title(\"data $y$\")\n",
    "\n",
    "    # plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "uyKMNSvSNR_D"
   },
   "outputs": [],
   "source": [
    "#@title Helper function to generate synthetic data (run this cell)\n",
    "\n",
    "def generate_templates(rng, C, D, N):\n",
    "    # Make (semi) random templates\n",
    "    templates = []\n",
    "    for n in range(N):\n",
    "        # center = n * C / (N - 1) if N > 1 else C / 2\n",
    "        center = C * rng.random()\n",
    "        width = 1 + (C / 10) * rng.random()\n",
    "        spatial_factor = np.exp(-0.5 * (np.arange(C) - center)**2 / width**2)\n",
    "        \n",
    "        dt = np.arange(D)\n",
    "        period = D / (1 + rng.random())\n",
    "        z = (dt - 0.75 * period) / (.25 * period)\n",
    "        warp = lambda x: -np.exp(-x) + 1\n",
    "        window = np.exp(-0.5 * z**2)\n",
    "        shape = np.sin(2 * np.pi * dt / period)\n",
    "        temporal_factor = warp(window * shape)\n",
    "\n",
    "        template = np.outer(spatial_factor, temporal_factor)\n",
    "        template /= np.linalg.norm(template)\n",
    "        templates.append(template)\n",
    "    \n",
    "    return np.array(templates)\n",
    "    \n",
    "\n",
    "def generate(rng, T, C, D, N, \n",
    "             mean_amplitude=15,\n",
    "             shape_amplitude=3.0,\n",
    "             noise_std=1, \n",
    "             sample_freq=1000):\n",
    "    \"\"\"Create a random set of model parameters and sample data.\n",
    "\n",
    "    Parameters:\n",
    "    T: integer number of time samples in the data\n",
    "    C: integer number of channels\n",
    "    D: integer duration (number of samples) of each template\n",
    "    N: integer number of neurons\n",
    "    \"\"\"\n",
    "    # Make semi-random templates\n",
    "    templates = generate_templates(rng, C, D, N)\n",
    "\n",
    "    # Make random amplitudes\n",
    "    amplitudes = np.zeros((N, T))\n",
    "    for n in range(N):\n",
    "        num_spikes = rng.poisson(T / sample_freq * 10)\n",
    "        times = rng.integers(0, T, size=num_spikes)\n",
    "        amps = rng.gamma(shape_amplitude, \n",
    "                         scale=mean_amplitude / shape_amplitude, \n",
    "                         size=num_spikes)\n",
    "        amplitudes[n, times] = amps\n",
    "\n",
    "        # Only keep spikes separated by at least D\n",
    "        times, props = find_peaks(amplitudes[n], distance=D, height=1e-3)\n",
    "        amplitudes[n] = 0\n",
    "        amplitudes[n, times] = props['peak_heights']\n",
    "\n",
    "    # Convolve the signal with each row of the multi-channel template\n",
    "    data = 0\n",
    "    for temp, amp in zip(templates, amplitudes):\n",
    "        data += np.row_stack([\n",
    "            np.convolve(amp, row, mode='full')[:-(D-1)]\n",
    "            for row in temp])\n",
    "        \n",
    "    data += rng.normal(scale=noise_std, size=data.shape)\n",
    "\n",
    "    return templates, amplitudes, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Q2OqRCSiH8E"
   },
   "source": [
    "# Part 1: Convolution and cross-correlation with PyTorch\n",
    "\n",
    "PyTorch is a machine learning framework for implementing fast numerical computations on CPU and GPU hardware. It has an interface very similar to NumPy's. Instead of numpy `ndarray` objects, we operate on PyTorch `Tensor` objects. Tensors are stored on the specified device, and in the setup above you'll see that we specified our device to be `'cuda'`, i.e. a GPU. That's also why you need to run this Colab notebook with a GPU Runtime. (If you click the RAM/disk icon in the upper right, you should see that this session is a GPU session. If it's not, go to \"Runtime -> Change Runtime Type\" to select a GPU.)\n",
    "\n",
    "PyTorch `Tensor` objects have a similar interface to Numpy arrays. You can read all about them in the [docs](https://pytorch.org/docs/stable/tensors.html). In this notebook, we'll name our variables with `_t` suffixes to indicate that when they are a `Tensor`. We can convert back and forth between an `array` and a `Tensor` using the convenience functions `to_t` and `from_t`, which we defined above. (There are a few minor concerns in making the translation; for example, Numpy defaults to 64-bit floats whereas PyTorch defaults to 32-bit. Similarly, we have to make sure that we copy the tensor back to the CPU and \"detach\" it from the computation graph before converting it to a Numpy array.)\n",
    "\n",
    "`Tensor` objects have a few nice functions that will make your life easier in this lab. Suppose `data_t` is a `Tensor`. Then,\n",
    "- `data_t.flip(dims=(-1,))` flips the tensor along its last axis.\n",
    "- `data_t.unsqueeze(0)` creates a new leading axis.\n",
    "- `data_t.permute(1, 0, 2)` permutes the order of the axes.\n",
    "- `data_t.reshape(1, 1, -1)` vectorizes the data and reshapes it to have two leading dimensions, each of length 1.\n",
    "- `data_t.sum()` sums the entries in the data.\n",
    "\n",
    "If this is your first time using PyTorch and you get stuck, you might want to check out some of their [tutorials](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEZMH3hCMP24"
   },
   "source": [
    "## Problem 1a: Perform a 1d convolution\n",
    "\n",
    "As a first step, we generate synthetic data with a single neuron and a single channel. Based on a template and a 1d array representing amplitudes for the single neuron, we simulate data as the convolution of these two arrays. \n",
    "\n",
    "You'll use the `conv1d` function in the `torch.nn.functional` package. We've already imported that package with the shorthand name `F` so that you can call the function with `F.conv1d(...)`. Take a look at its documentation [here](https://pytorch.org/docs/stable/nn.functional.html?highlight=conv1d#torch.nn.functional.conv1d), as well as the corresponding documentation for the `torch.nn.Conv1d` object, which implements a convolutional layer for a neural network. \n",
    "\n",
    "Here's an example,\n",
    "```\n",
    "# make the input (i.e. the signal)\n",
    "B = 1   # batch size\n",
    "N = 2   # number of input channels\n",
    "T = 100 # length of input signal\n",
    "input_t = torch.rand(B, N, T)\n",
    "\n",
    "# make the weights (i.e. the filter)\n",
    "C = 3   # number of output channels\n",
    "D = 10  # length of the filter\n",
    "weight_t = torch.rand(C, N, D)\n",
    "\n",
    "# perform the convolution\n",
    "output_t = F.conv1d(input_t, weight_t)\n",
    "\n",
    "# output.shape is (B, C, T - D + 1)\n",
    "```\n",
    "**Remember that `conv1d` actually performs a cross-correlation!** \n",
    "\n",
    "Let $X \\in \\mathbb{R}^{B \\times N \\times T}$ denote the signal/input and $W \\in \\mathbb{R}^{C \\times N \\times D}$ denote the filter/weights (note that the axes are permuted relative to our templates), and let $Y \\in \\mathbb{R}^{B \\times C \\times T - D + 1}$ denote the output. Then the `conv1d` function implements the cross-correlation, \n",
    "\\begin{align}\n",
    "y_{b,c,t} = \\sum_{n = 1}^{N} \\sum_{d=1}^D x_{b,n,t+d-1} w_{c,n,d}.\n",
    "\\end{align}\n",
    "for $b=1,\\ldots,B$, $c=1,\\ldots,C$, and $t=1,\\ldots,T-D+1$. \n",
    "\n",
    "By default the output only contains the \"valid\" portion of the convolution; i.e. the $T-D+1$ samples where the inputs and weights completely overlap. If you want the \"full\" output, you have to call `F.conv1d(input_t, weights_t, padding=D-1)`. This pads the input with $D-1$ zeros at the beginning and end so that the resulting output is length $T + D - 1$. Depending your application, you may want the first $T$ or the last $T$ entries in this array. When in doubt, try both and see!\n",
    "\n",
    "Use `conv1d` to implement a 1d **convolution**. Remember that you can do it by cross-correlation as long as you flip your weights along the last axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MDZg1AVrXBx"
   },
   "outputs": [],
   "source": [
    "# Create a dataset with a single channel and one neuron.\n",
    "T = 1000    # number of time samples\n",
    "N = 1       # one neuron\n",
    "C = 1       # number of channels\n",
    "D = 51      # duration of a spike (in samples)\n",
    "\n",
    "# Generate random templates, amplitudes, and noisy data.\n",
    "# `templates` are NxCxD and `amplitudes` are NxT\n",
    "rng = np.random.default_rng(seed=2)\n",
    "templates, amplitudes, _ = generate(rng, T, C, D, N)\n",
    "\n",
    "# Convolve the amplitude with the template using numpy.\n",
    "data = np.convolve(amplitudes[0], templates[0, 0], mode='full')[:T]\n",
    "\n",
    "# Plot the templates, amplitude, and data\n",
    "plot_model(templates, amplitudes, data)\n",
    "\n",
    "###\n",
    "# Now perform the same convolution using PyTorch's `conv1d` function.\n",
    "#\n",
    "# YOUR CODE BELOW\n",
    "amplitudes_t = to_t(amplitudes)\n",
    "templates_t = to_t(templates)\n",
    "data_t = F.conv1d(...)\n",
    "\n",
    "#\n",
    "### \n",
    "\n",
    "assert np.allclose(data, from_t(data_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGsHSSO6MeR2"
   },
   "source": [
    "## Problem 1b: Perform a 1d cross-correlation in PyTorch\n",
    "\n",
    "Recall from class that the cross-correlation measures the similarity between the template and the actual data at every time window. In those time points where the data and the template coincide, we should obtain a high correlation indicating the presence of a spike. The peaks of the amplitude array and the cross-correlation array should match, as you see in the plot below.  The dotted line shows the cross-correlation of the data and the template, and we see that it peaks where there are spikes in the true underlying amplitude that generated the data. \n",
    "\n",
    "Use `conv1d` to implement this **cross-correlation** and get the dotted line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsJKzLTUxXUx"
   },
   "outputs": [],
   "source": [
    "# Now correlate the data with the template to estimate spike times\n",
    "score = np.correlate(data, templates[0, 0], mode='full')[D-1:]\n",
    "assert score.shape[0] == T\n",
    "\n",
    "plot_model(templates, amplitudes, data, scores=score)\n",
    "\n",
    "###\n",
    "# Now perform the same cross-correlation using PyTorch's `conv1d` function.\n",
    "#\n",
    "# YOUR CODE BELOW\n",
    "score_t = F.conv1d(...)\n",
    "\n",
    "#\n",
    "### \n",
    "\n",
    "assert np.allclose(score, from_t(score_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPW3qbjpMlqu"
   },
   "source": [
    "## Problem 1c: Perform a 1d convolution across multiple channels at once\n",
    "\n",
    "Similar to problem 1a, except that the template (and therefore the final synthetic data) has multiple _output_ channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z56Mvq42T4Zo"
   },
   "outputs": [],
   "source": [
    "# Create a dataset with a single channel and one neuron.\n",
    "T = 1000    # number of time samples\n",
    "C = 10      # number of channels\n",
    "D = 100     # duration of a spike (in samples)\n",
    "N = 1       # one neuron\n",
    "\n",
    "# Generate random templates, amplitudes, and noisy data.\n",
    "# `templates` are NxCxD and `amplitudes` are NxT\n",
    "rng = np.random.default_rng(seed=0)\n",
    "templates, amplitudes, _ = generate(rng, T, C, D, N)\n",
    "\n",
    "# Convolve the signal with each row of the multi-channel tempalte\n",
    "data = np.row_stack([\n",
    "    np.convolve(amplitudes[0], row, mode='full')[:T]\n",
    "    for row in templates[0]])\n",
    "\n",
    "plot_model(templates, amplitudes, data)\n",
    "\n",
    "###\n",
    "# Now perform the same convolution using PyTorch's `conv1d` function.\n",
    "#\n",
    "# YOUR CODE BELOW\n",
    "#\n",
    "amplitudes_t = to_t(amplitudes)\n",
    "templates_t = to_t(templates)\n",
    "data_t = F.conv1d(...)\n",
    "\n",
    "#\n",
    "### \n",
    "\n",
    "assert np.allclose(data, from_t(data_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4axdluyAMvma"
   },
   "source": [
    "## Problem 1d: Perform a 1d cross-correlation across multiple channels at once\n",
    "\n",
    "Same as Problem 1b except that the data and templates have multiple _input_ channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wWa7SpZz2Ds"
   },
   "outputs": [],
   "source": [
    "# We'll first perform the cross-correlation in numpy by correlating\n",
    "# each row of the data with the corresponding row of the template and summing.\n",
    "# Then you'll do the same thing in PyTorch using a single call to `F.conv1d`.\n",
    "score = np.sum([np.correlate(data[c], templates[0, c], mode='full')[D-1:]\n",
    "                for c in range(C)], axis=0)\n",
    "\n",
    "plot_model(templates, amplitudes, data, scores=score)\n",
    "\n",
    "###\n",
    "# Now perform the same cross-correlation using PyTorch's \n",
    "# ``nn.functional.conv1d` function. You should only need \n",
    "# one call to this function!\n",
    "#\n",
    "# YOUR CODE BELOW\n",
    "score_t = F.conv1d(...)\n",
    "\n",
    "### \n",
    "\n",
    "assert np.allclose(score, from_t(score_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dlmZoM8G1Q5"
   },
   "source": [
    "## Problem 1e: Convolving multiple neurons' spikes and templates\n",
    "\n",
    "Similar to problem 1c, but here we have multiple neurons (input channels), each associated to a template with multiple (output) channels. The final simulated data is aggregated across neurons to simulate actual measurements where signals from multiple neurons are superimposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5GWnW2WG9cD"
   },
   "outputs": [],
   "source": [
    "# Create a dataset with a multiple channels and neurons.\n",
    "T = 1000    # number of time samples\n",
    "C = 10      # number of channels\n",
    "D = 100     # duration of a spike (in samples)\n",
    "N = 3       # multiple neuron\n",
    "\n",
    "# Generate random templates, amplitudes, and noisy data.\n",
    "# `templates` are NxCxD and `amplitudes` are NxT\n",
    "rng = np.random.default_rng(seed=0)\n",
    "templates, amplitudes, _ = generate(rng, T, C, D, N)\n",
    "\n",
    "# Convolve the signal with each row of the multi-channel template\n",
    "data = 0\n",
    "for temp, amp in zip(templates, amplitudes):\n",
    "    data += np.row_stack([\n",
    "        np.convolve(amp, row, mode='full')[:T]\n",
    "        for row in temp])\n",
    "\n",
    "plot_model(templates, amplitudes, data)\n",
    "\n",
    "###\n",
    "# Now perform the convolution using PyTorch's `conv1d` function. \n",
    "# One call to `F.conv1d` should perform the sum over neurons for you.\n",
    "#\n",
    "# YOUR CODE BELOW\n",
    "#\n",
    "templates_t = to_t(templates)\n",
    "amplitudes_t = to_t(amplitudes)\n",
    "data_t = F.conv1d(...)\n",
    "\n",
    "#\n",
    "###\n",
    "assert np.allclose(data, from_t(data_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4X2UQxJRK2R"
   },
   "source": [
    "## Problem 1f: Perform a 1d cross-correlation across multiple channels and neurons at once\n",
    "\n",
    "Same as Problem 1c but now we're performing the cross-correlation with multiple neurons' templates at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBFZBiG7Qe-m"
   },
   "outputs": [],
   "source": [
    "# We'll first perform the cross-correlation in numpy by correlating\n",
    "# each row of the data with the corresponding row of each template and summing.\n",
    "# Then you'll do the same thing in PyTorch using a single call to `F.conv1d`.\n",
    "score = np.array([\n",
    "    np.sum([np.correlate(data[c], templates[n, c], mode='full')[D-1:]\n",
    "            for c in range(C)], axis=0)\n",
    "    for n in range(N)])\n",
    "\n",
    "plot_model(templates, amplitudes, data, scores=score)\n",
    "\n",
    "###\n",
    "# Now perform the convolution using PyTorch's `conv1d` function. \n",
    "# One call to `F.conv1d` should perform all cross-correlations for you.\n",
    "#\n",
    "# YOUR CODE BELOW\n",
    "score_t = F.conv1d(...)\n",
    "\n",
    "#\n",
    "###\n",
    "\n",
    "assert np.allclose(score, from_t(score_t), atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXtWh5AhjJiE"
   },
   "source": [
    "# Part 2: Spike sorting by deconvolution\n",
    "\n",
    "In this part of the lab you'll use those cross-correlation and convolution operations to implement the spike sorting algorithm we discussed in class. We'll work with a synthetic dataset, as above, but this time we'll make it slightly larger.  (It's still a lot smaller than the dataset we used in Lab 1 though!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAdBOmbh3yf-"
   },
   "source": [
    "## Simulate some data\n",
    "\n",
    "No coding necessary here. We're just simulating the data for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3foCrNrhTQHU"
   },
   "outputs": [],
   "source": [
    "# Create a larger dataset with a multiple channels and neurons.\n",
    "T = 1000000 # number of time samples\n",
    "C = 32      # number of channels\n",
    "D = 81      # duration of a spike (in samples)\n",
    "N = 10      # multiple neurons\n",
    "\n",
    "# Generate random templates, amplitudes, and noisy data.\n",
    "# `templates` are NxCxD and `amplitudes` are NxT\n",
    "rng = np.random.default_rng(seed=1)\n",
    "print(\"Simulating data. This could take a minute!\")\n",
    "true_templates, true_amplitudes, data = generate(rng, T, C, D, N)\n",
    "plot_model(true_templates, true_amplitudes[:, :2000], data[:,:2000], lw=1, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZvZK96twMyc"
   },
   "outputs": [],
   "source": [
    "# Generate another set of random templates and amplitudes to seed the model\n",
    "rng = np.random.default_rng(seed=1)\n",
    "templates = generate_templates(rng, C, D, N)\n",
    "amplitudes = np.zeros((N, T))\n",
    "noise_std = 1.0\n",
    "\n",
    "# Copy to the device\n",
    "templates_t = to_t(templates)\n",
    "amplitudes_t = to_t(amplitudes)\n",
    "data_t = to_t(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCwPHmZOrLOS"
   },
   "source": [
    "## Problem 2a: Compute the log likelihood\n",
    "\n",
    "One of the most awesome features of PyTorch is its `torch.distributions` package. See the docs [here](https://pytorch.org/docs/stable/distributions.html). It contains objects for many of our favorite distributions, and has convenient functions for computing log probabilities (with `d.log_prob()` where `d` is a `Distribution` object), sampling (`d.sample()`), computing the entropy (`d.entropy()`), etc. These functions broadcast as you'd expect (unlike `scipy.stats`!), and they're designed to work with automatic differentiation.  More on that another day...\n",
    "\n",
    "For now, you'll use `torch.distributions.Normal` to compute the log likelihood of the data given the template and amplitudes, $\\log p(Y \\mid A, W)$.  To do that, you'll convolve the amplitudes and templates (recall Problem 1e) to get the mean value of $Y$, then you'll use the `log_prob` function to evaluate the likelihood of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m14LDrF1bWua"
   },
   "outputs": [],
   "source": [
    "def log_likelihood(templates_t, amplitudes_t, data_t, noise_std):\n",
    "    \"\"\"Evaluate the log likelihood\"\"\"\n",
    "    N, C, D = templates_t.shape\n",
    "    _, T = data_t.shape\n",
    "    \n",
    "    ### \n",
    "    # Compute the log probability \n",
    "    #\n",
    "    # YOUR CODE BELOW\n",
    "    \n",
    "    # Compute the model predictions by convolving the amplitude and templates\n",
    "    pred_t = F.conv1d(...)\n",
    "\n",
    "    # Evaluate the log probability using torch.distributions.Normal\n",
    "    lp = ...\n",
    "    #\n",
    "    ###\n",
    "\n",
    "    # Return the log probability normalized by the data size\n",
    "    return lp / (C * T)\n",
    "\n",
    "ll = log_likelihood(templates_t, amplitudes_t, data_t, noise_std)\n",
    "assert ll.isclose(to_t(-1.7955), atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULZKN48jsT8y"
   },
   "source": [
    "## Problem 2b: Compute the residual\n",
    "\n",
    "Next, compute the residual for a specified neuron by subtracting the convolved amplitudes and templates for all the other neurons. Again, recall Problem 1e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcPl7gUsNEiN"
   },
   "outputs": [],
   "source": [
    "def compute_residual(neuron, templates_t, amplitudes_t, data_t):\n",
    "    N, C, D = templates_t.shape\n",
    "\n",
    "    ###\n",
    "    # Compute the predicted value of the data by \n",
    "    # convolving the amplitudes and the templates for all\n",
    "    # neurons except the specified one.\n",
    "    #\n",
    "    # YOUR CODE BELOW\n",
    "    not_n = np.concatenate([np.arange(neuron), np.arange(neuron+1, N)])\n",
    "    pred_t = F.conv1d(...)\n",
    "    ###\n",
    "\n",
    "    # Return the data minus the predicted value given other neurons\n",
    "    return data_t - pred_t\n",
    "\n",
    "residual_t = compute_residual(0, templates_t, amplitudes_t, data_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HWVckFesZAU"
   },
   "source": [
    "## Problem 2c: Compute the score\n",
    "\n",
    "We defined the \"score\" for neuron $n$ to be the cross-correlation of the residual and its template. Compute it using `conv1d`. Recall Problem 1d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2Ww9dNLTscP"
   },
   "outputs": [],
   "source": [
    "def compute_score(neuron, templates_t, amplitudes_t, data_t):\n",
    "    N, C, D = templates_t.shape\n",
    "    T = data_t.shape[1]\n",
    "\n",
    "    # First get the residual\n",
    "    residual_t = compute_residual(neuron, templates_t, amplitudes_t, data_t)\n",
    "\n",
    "    ###\n",
    "    # Compute the 'score' by cross-correlating the residual\n",
    "    # and the template for this neuron.\n",
    "    #\n",
    "    # YOUR CODE BELOW\n",
    "    score_t = F.conv1d(...)\n",
    "\n",
    "    #\n",
    "    ###\n",
    "    return score_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM3nt9QysfEA"
   },
   "source": [
    "## Problem 2d: Update the amplitudes using `find_peaks`\n",
    "\n",
    "Our next step is to update the amplitudes given the scores. We'll use the simple heuristic described in lecture to find peaks in the score that are separated by a distance of at least $D$ samples and at least a height of $\\sigma^2 \\lambda$, where $\\sigma$ is the standard deviation of the noise and $\\lambda$ is the amplitude rate hyperparameter. Use the `find_peaks` function from Lab 1 to do this, and then update the amplitude tensor with your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWscC7mXPJEM"
   },
   "outputs": [],
   "source": [
    "def _update_amplitude(neuron, templates_t, amplitudes_t, data_t, noise_std=1.0, amp_rate=5.0):\n",
    "    N, C, D = templates_t.shape\n",
    "    T = data_t.shape[1]\n",
    "\n",
    "    # Compute the score and convert it to a numpy array.\n",
    "    score_t = compute_score(neuron, templates_t, amplitudes_t, data_t)\n",
    "    score = from_t(score_t)\n",
    "\n",
    "    ###\n",
    "    # Find the peaks in the cross-correlation and update the amplitude tensor.\n",
    "    #\n",
    "    # YOUR CODE BELOW\n",
    "    peaks, props = find_peaks(...)\n",
    "    heights = props['peak_heights']\n",
    "\n",
    "    # Update the amplitude tensor for this neuron.\n",
    "    # ...\n",
    "    \n",
    "    #\n",
    "    ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EDCljEus1L7"
   },
   "source": [
    "## Problem 2e: Update the templates \n",
    "Our last step is to update the template for a given neuron by projecting the _target_ $\\overline{R}_n \\in \\mathbb{R}^{C \\times D}$. The target is the sum of scaled residuals at the times of spikes in the amplitudes:\n",
    "\\begin{align}\n",
    "    \\overline{R}_n = \\sum_{t:a_{nt} > 0} a_{nt} R_{n,:,t:t+D}.\n",
    "\\end{align}\n",
    "where $R_n \\in \\mathbb{R}^{C \\times T}$ denotes the residual for neuron $n$. \n",
    "\n",
    "In lecture we suggested a simple trick to implement this summation. First compute a matrix of regressors $X_n \\in \\mathbb{R}^{D \\times T}$ where the $d$-th row is equal to the lagged amplitudes. That is,\n",
    "\\begin{align}\n",
    "x_{ndt} = a_{n,t-d+1}.\n",
    "\\end{align}\n",
    "We can equivalently compute the regressor matrix by convolving the amplitudes with a \"delay\" matrix, which is just the $D \\times D$ identity matrix.\n",
    "\n",
    "Once we have the regressors, we can compute the target as $\\overline{R}_n = R_n X_n^\\top$.\n",
    "\n",
    "Finally, to get the template, project $\\overline{R}_n$ onto $\\mathcal{S}_K$, the set of rank-$K$, unit-norm matrices, using the SVD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zbvv-XzndvcE"
   },
   "outputs": [],
   "source": [
    "def _update_template(rng, neuron, templates_t, amplitudes_t, data_t, template_rank=1):\n",
    "    N, C, D = templates.shape\n",
    "    T = data_t.shape[1]\n",
    "\n",
    "\n",
    "    # Check if the factor is used. If not, generate a random new one.\n",
    "    if amplitudes_t[neuron].sum() < 1:\n",
    "        target_t = to_t(generate_templates(rng, C, D, 1)[0])\n",
    "\n",
    "    else:\n",
    "        ###\n",
    "        # Make a TxD array of regressors for this neuron\n",
    "        # by convolving its amplitude with a \"delay\" matrix;\n",
    "        # i.e. a DxD identity matrix. \n",
    "        #\n",
    "        # YOUR CODE BELOW\n",
    "        delay_t = to_t(np.eye(D))\n",
    "        regressors_t = F.conv1d(...)\n",
    "        \n",
    "        # Get the residual using the function you wrote above\n",
    "        residual_t = compute_residual(neuron, templates_t, amplitudes_t, data_t)\n",
    "\n",
    "        # Compute the target (inner product of residual and regressors)\n",
    "        target_t = ...\n",
    "        #\n",
    "        ###\n",
    "\n",
    "\n",
    "    ###\n",
    "    # Project the target onto the set of normalized rank-K templates using \n",
    "    # `torch.svd` and `torch.norm`. Note that `torch.svd` returns V rather \n",
    "    # than V^T, as `np.linalg.svd` does.\n",
    "    #\n",
    "    # YOUR CODE BELOW\n",
    "    Un_t, Sn_t, Vn_t = torch.svd(target_t)\n",
    "    ...\n",
    "    templates_t[neuron] = ...\n",
    "    #\n",
    "    ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve_ubfWPJnaK"
   },
   "source": [
    "## Put it all together\n",
    "\n",
    "That's it! We've written a little function to perform coordinate ascent using your `_update_*` functions. It tracks the log likelihood at each iteration. (We're ignoring the priors for now, but it would be easy to compute those in Problem 2a if you wanted to). It also uses some nice progress bars so you can see how fast (or slow?) your code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPMbKOV7uUY6"
   },
   "outputs": [],
   "source": [
    "def map_estimate(rng,\n",
    "                 templates_t, \n",
    "                 amplitudes_t, \n",
    "                 data_t,\n",
    "                 num_iters=20, \n",
    "                 template_rank=1,\n",
    "                 noise_std=1.0, \n",
    "                 amp_rate=5.0,\n",
    "                 tol=1e-4):\n",
    "    \"\"\"Fit the templates and amplitudes by maximum a posteriori (MAP) estimation.\n",
    "    \"\"\"\n",
    "    N, C, D = templates_t.shape\n",
    "\n",
    "    # Make fancy reusable progress bars\n",
    "    outer_pbar = trange(num_iters)\n",
    "    inner_pbar = trange(N)\n",
    "    inner_pbar.set_description(\"updating neurons\")\n",
    "\n",
    "    # Track log likelihoods over iterations\n",
    "    lls = [from_t(log_likelihood(templates_t, amplitudes_t, data_t, noise_std=noise_std))]\n",
    "    for itr in outer_pbar:\n",
    "        inner_pbar.reset()\n",
    "        for n in range(N):\n",
    "            # Update the amplitude\n",
    "            _update_amplitude(n, templates_t, amplitudes_t, data_t, noise_std=noise_std, amp_rate=amp_rate)    \n",
    "            # Update the template\n",
    "            _update_template(rng, n, templates_t, amplitudes_t, data_t, template_rank=template_rank)\n",
    "            inner_pbar.update()\n",
    "\n",
    "        # Compute the log likelihood \n",
    "        lls.append(from_t(log_likelihood(templates_t, amplitudes_t, data_t, noise_std=noise_std)))\n",
    "\n",
    "        # Check for convergence\n",
    "        if abs(lls[-1] - lls[-2]) < tol:\n",
    "            print(\"Convergence detected!\")\n",
    "            break\n",
    "    \n",
    "    return np.array(lls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rvtQmiv-ZXQ"
   },
   "source": [
    "## Fit the synthetic data and plot the log likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z817fJGULIkd"
   },
   "outputs": [],
   "source": [
    "# Make random templates and set amplitude to zero\n",
    "rng = np.random.default_rng(seed=1)\n",
    "templates = generate_templates(rng, C, D, N)\n",
    "amplitudes = np.zeros((N, T))\n",
    "noise_std = 1.0     # \\sigma\n",
    "amp_rate = 5.0      # \\lambda\n",
    "\n",
    "# Copy to the device\n",
    "templates_t = to_t(templates)\n",
    "amplitudes_t = to_t(amplitudes)\n",
    "data_t = to_t(data)\n",
    "\n",
    "# Fit the model.\n",
    "lls = map_estimate(rng, templates_t, amplitudes_t, data_t, noise_std=noise_std, amp_rate=amp_rate)\n",
    "\n",
    "# For comparison, compute the log likelihood with the true templates and amplitudes.\n",
    "true_ll = from_t(log_likelihood(to_t(true_templates),\n",
    "                                to_t(true_amplitudes),\n",
    "                                data_t,\n",
    "                                noise_std))\n",
    "\n",
    "# Plot the log likelihoods\n",
    "plt.plot(lls, '-o')\n",
    "plt.hlines(true_ll, 0, len(lls) - 1, colors='r', linestyles=':', label=\"true LL\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.xlim(-.1, len(lls) - .9)\n",
    "plt.ylabel(\"Log Likelihood\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ethJ1Ya6_dLk"
   },
   "source": [
    "## Find a permutation of the inferred neurons that best matches the true neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7Ydy6W7-Mgt"
   },
   "outputs": [],
   "source": [
    "# Compute the similarity (inner product) of the true and inferred templates\n",
    "templates = from_t(templates_t)\n",
    "similarity = np.zeros((N, N))\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        similarity[i, j] = np.sum(true_templates[i] * templates[j])\n",
    "        \n",
    "# Show the similarity matrix\n",
    "_, perm = linear_sum_assignment(similarity, maximize=True)\n",
    "plt.imshow(similarity[:, perm], vmin=0, vmax=1)\n",
    "plt.xlabel(\"true neuron\")\n",
    "plt.ylabel(\"inferred neuron\")\n",
    "plt.title(\"similarity of amplitudes\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIJqkZBh-dPF"
   },
   "source": [
    "## Plot the true and inferred templates\n",
    "\n",
    "They should line up pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwLzxeJt4sPE"
   },
   "outputs": [],
   "source": [
    "# Plot the true and inferred templates, permuted to best match\n",
    "fig, axs = plot_templates(true_templates, np.arange(N), n_cols=N)\n",
    "_ = plot_templates(templates[perm], np.arange(N), \n",
    "                   n_cols=N, colors=('r',), fig=fig, axs=axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xI6mDrHOGqSt"
   },
   "source": [
    "# Part 3: Now do it more efficiently\n",
    "\n",
    "Our code above was not optimized for efficiency. For example, we kept recomputing the residual, which incurred costly convolutions. Likewise, we naively cross-correlated the residual with the templates, without leveraging the fact that the templates are low rank. In this section you'll go back and address these shortcomings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_c9SOQSdoNE"
   },
   "source": [
    "## Problem 3a: Write helper functions to up/down-date the residual\n",
    "\n",
    "Each iteration of `map_estimate` updates one neuron at a time. We can save a lot of computation by just adding and subtracting that neuron's convolved template from the residual at the start and end of the iteration. Moreover, we can perform that convolution more efficiently by working directly with the factors of the template. \n",
    "\n",
    "Let $W_n = U_n \\tilde{V}_n^\\top$ where $\\tilde{V}_n^\\top = \\mathrm{diag}(\\varsigma_n) V_n^\\top \\in K \\times D$ denotes the weighted temporal factors of neuron $n$'s template. We can use this factorization to implement the convolution as,\n",
    "\\begin{align}\n",
    "a_n \\circledast W_n = U_n (a_n \\circledast \\tilde{V}_n^\\top)\n",
    "\\end{align}\n",
    "In code, we'll let `U_t` denote the tensor of spatial factors. It is of shape $N \\times C \\times K$ so that `U_t[n]` corresponds to $U_n$ in our equations above. Likewise, let `weighted_VT_t` denote the tensor of weighted temporal factors.  It is of shape $N \\times K \\times D$ so that `weighted_VT_t[n]` corresponds to $\\tilde{V}_n^\\top$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNfCbMxodqfZ"
   },
   "outputs": [],
   "source": [
    "def update_residual(neuron, U_t, weighted_VT_t, amplitudes_t, residual_t):\n",
    "    \"\"\"\n",
    "    Add the specified neurons' template convolved with its amplitude to the residual.\n",
    "    \"\"\"\n",
    "    N, C, K = U_t.shape\n",
    "    _, _, D = weighted_VT_t.shape\n",
    "    _, T = data_t.shape\n",
    "\n",
    "    ###\n",
    "    # Convolve this neuron's amplitude with its weighted temporal factor.\n",
    "    # Project the result using the neuron's channel factor.\n",
    "    # Add that result to the residual.\n",
    "    #\n",
    "    # YOUR CODE BELOW\n",
    "    residual_t += ...\n",
    "    #\n",
    "    ###\n",
    "\n",
    "def downdate_residual(neuron, U_t, weighted_VT_t, amplitudes_t, residual_t):\n",
    "    \"\"\"\n",
    "    Subtract the specified neurons' template convolved with its amplitude to the residual.\n",
    "    \"\"\"\n",
    "    N, C, K = U_t.shape\n",
    "    _, _, D = weighted_VT_t.shape\n",
    "    _, T = data_t.shape\n",
    "\n",
    "    ###\n",
    "    # Convolve this neuron's amplitude with its weighted temporal factor.\n",
    "    # Project the result using the neuron's channel factor.\n",
    "    # Subtract that result to the residual.\n",
    "    #\n",
    "    # YOUR CODE BELOW\n",
    "    residual_t -= ...\n",
    "    #\n",
    "    ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cL6CzIYz2N8"
   },
   "source": [
    "## Problem 3b: Update the amplitudes using a fast score calculation\n",
    "\n",
    "In lecture we showed that the score&mdash;the cross correlation of the residual and a template&mdash; can be computed more efficiently by projecting the residual on the spatial factors of the template and then convolving with the weighted temporal factors. In math,\n",
    "\\begin{align}\n",
    "R_n \\star W_n &= (U_n^\\top R_n) \\star \\tilde{V}_n^\\top.\n",
    "\\end{align}\n",
    "Re-implement your `update_amplitude` function from above, but this time compute the score by projecting and convolving the residual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWPJsX7xezzv"
   },
   "outputs": [],
   "source": [
    "def _update_amplitude_fast(neuron, U_t, weighted_VT_t, amplitudes_t, residual_t, \n",
    "                           noise_std=1.0, amp_rate=5.0):\n",
    "    D = weighted_VT_t.shape[2]\n",
    "\n",
    "    ###\n",
    "    # compute the score by projecting the residual (U_n^T R_n)\n",
    "    # and cross-correlating with the weighted temporal factor (V_n^T)\n",
    "    # \n",
    "    # YOUR CODE BELOW\n",
    "    \n",
    "    # project the residual onto the channel factor for this neuron\n",
    "    proj_residual_t = ...     \n",
    "    \n",
    "    # correlate the projected residual with the weighted temporal factor\n",
    "    score_t = F.conv1d(...)\n",
    "    \n",
    "    # Find the peaks in the cross-correlation\n",
    "    score = from_t(score_t)\n",
    "    peaks, props = find_peaks(...)\n",
    "    heights = props['peak_heights']\n",
    "\n",
    "    # Update the amplitudes for this neuron in place\n",
    "    ...\n",
    "    \n",
    "    #\n",
    "    ###\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0P-m5Hedz-0P"
   },
   "source": [
    "## Problem 3c: Update the low rank factors of the template\n",
    "\n",
    "Finally, copy your answer from Problem 2e to update the templates, but this time just keep the template factors $U_n$ and $\\tilde{V}_n^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NCVElZwfEHR"
   },
   "outputs": [],
   "source": [
    "def _update_template_factors(rng, neuron, U_t, weighted_VT_t, amplitudes_t, residual_t, template_rank=1):\n",
    "    N, C, D = templates.shape\n",
    "    T = residual_t.shape[1]\n",
    "\n",
    "    # Check if the factor is used. if not, generate a random new one.\n",
    "    if amplitudes_t[neuron].sum() < 1:\n",
    "        target_t = to_t(generate_templates(rng, C, D, 1)[0])\n",
    "    else:\n",
    "        # Make the regressors (i.e. a TxD matrix of delayed amplitudes) for this neuron \n",
    "        delay_t = torch.eye(D, device=device, dtype=dtype)\n",
    "\n",
    "        ###\n",
    "        # Compute the matrix of regressors by convolving the amplitude and the delay\n",
    "        # as in Problem 2e.\n",
    "        #\n",
    "        # YOUR CODE BELOW\n",
    "        regressors_t = ...\n",
    "        \n",
    "        # Compute the target (inner product of residual and regressors)\n",
    "        target_t = ...\n",
    "        \n",
    "        #\n",
    "        ###\n",
    "\n",
    "    ###\n",
    "    # Project the target onto the set of normalized rank-K templates\n",
    "    # and keep only the spatial factors (U_n) and the weighted temporal\n",
    "    # factors (V_n^T)\n",
    "    # \n",
    "    # YOUR CODE BELOW\n",
    "    Un_t, Sn_t, Vn_t = torch.svd(target_t)\n",
    "    \n",
    "    # Truncate and normalize the singular values\n",
    "    Sn_t = ...\n",
    "    \n",
    "    # Truncate, weight, and transpose the factors as appropriate\n",
    "    U_t[neuron] = ...\n",
    "    weighted_VT_t[neuron] = ...\n",
    "    #\n",
    "    ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAtOGqHIcuWv"
   },
   "source": [
    "## Put it all together\n",
    "\n",
    "Now we'll write another function to perform MAP estimation using your new-and-improved code. We've got to do a little work to initialize the residuals and the templates, but it's all pretty straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEaKv3UgfNN-"
   },
   "outputs": [],
   "source": [
    "def map_estimate_fast(rng,\n",
    "                      templates_t, \n",
    "                      amplitudes_t, \n",
    "                      data_t,\n",
    "                      num_iters=20, \n",
    "                      template_rank=1,\n",
    "                      noise_std=1.0, \n",
    "                      amp_rate=5.0,\n",
    "                      tol=1e-4):\n",
    "    \"\"\"Fit the templates and amplitudes by maximum a posteriori (MAP) estimation.\n",
    "    \"\"\"\n",
    "    N, C, D = templates_t.shape\n",
    "\n",
    "    # Make a fancy reusable progress bar for the inner loops over neurons.\n",
    "    outer_pbar = trange(num_iters)\n",
    "    inner_pbar = trange(N)\n",
    "    inner_pbar.set_description(\"updating neurons\")\n",
    "\n",
    "    # Initialize the residual\n",
    "    residual_t = data_t - F.conv1d(amplitudes_t.unsqueeze(0),\n",
    "                                   templates_t.permute(1, 0, 2).flip(dims=(2,)),\n",
    "                                   padding=D-1)[0, :, :-(D-1)]\n",
    "\n",
    "    # Initialize the template factors\n",
    "    U_t, S_t, V_t = torch.svd(templates_t)\n",
    "    U_t, S_t, V_t = U_t[..., :template_rank], S_t[..., :template_rank], V_t[..., :template_rank]\n",
    "    weighted_VT_t = V_t * S_t.unsqueeze(1)\n",
    "    weighted_VT_t = weighted_VT_t.permute(0, 2, 1)\n",
    "\n",
    "    # Track log likelihoods over iterations\n",
    "    lls = [from_t(log_likelihood(templates_t, amplitudes_t, data_t, noise_std=noise_std))]\n",
    "\n",
    "    # Coordinate ascent\n",
    "    for itr in outer_pbar:\n",
    "\n",
    "        # Update neurons one at a time\n",
    "        inner_pbar.reset()\n",
    "        for n in range(N):\n",
    "            # Update the residual (add a_n \\circledast W_n)\n",
    "            update_residual(n, U_t, weighted_VT_t, amplitudes_t, residual_t)\n",
    "    \n",
    "            # Update the template and amplitude with the residual\n",
    "            _update_amplitude_fast(n, U_t, weighted_VT_t, amplitudes_t, residual_t, \n",
    "                                   noise_std=noise_std, amp_rate=amp_rate)\n",
    "            \n",
    "            _update_template_factors(rng, n, U_t, weighted_VT_t, amplitudes_t, residual_t, \n",
    "                                     template_rank=template_rank)\n",
    "    \n",
    "            # Downdate the residual (subtract a_n \\circledast W_n)\n",
    "            downdate_residual(n, U_t, weighted_VT_t, amplitudes_t, residual_t)\n",
    "\n",
    "            # Step the progress bar\n",
    "            inner_pbar.update()\n",
    "\n",
    "        # Reconstruct the templates in place\n",
    "        templates_t[:] = U_t @ weighted_VT_t\n",
    "\n",
    "        # Compute the log likelihood \n",
    "        lls.append(from_t(log_likelihood(templates_t, amplitudes_t, data_t, noise_std=noise_std)))\n",
    "\n",
    "        # Check for convergence\n",
    "        if abs(lls[-1] - lls[-2]) < tol:\n",
    "            print(\"Convergence detected!\")\n",
    "            break\n",
    "    \n",
    "    return np.array(lls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOgc3k1x0PpN"
   },
   "source": [
    "## Run it on the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2r6JNLqFhegz"
   },
   "outputs": [],
   "source": [
    "# Make random templates and set amplitude to zero\n",
    "rng = np.random.default_rng(seed=1)\n",
    "templates = generate_templates(rng, C, D, N)\n",
    "amplitudes = np.zeros((N, T))\n",
    "noise_std = 1.0\n",
    "amp_rate = 5.0\n",
    "\n",
    "# Copy to the device\n",
    "templates_t = to_t(templates)\n",
    "amplitudes_t = to_t(amplitudes)\n",
    "data_t = to_t(data)\n",
    "\n",
    "# Fit the model.\n",
    "fast_lls = map_estimate_fast(rng, templates_t, amplitudes_t, data_t, \n",
    "                             noise_std=noise_std, amp_rate=amp_rate)\n",
    "\n",
    "# Plot the log likelihoods from the fast code on top of those from the original code\n",
    "# They should like up exactly since we used the same initial condition.\n",
    "plt.plot(lls, '-o', label=\"orig. MAP\")\n",
    "plt.plot(fast_lls, '--.', label=\"fast MAP\")\n",
    "plt.hlines(true_ll, 0, len(lls) - 1, colors='r', linestyles=':', label=\"true LL\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.xlim(-.1, len(lls) - .9)\n",
    "plt.ylabel(\"Log Likelihood\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "assert np.allclose(lls, fast_lls, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yJzCeueH-oh"
   },
   "source": [
    "# Part 4: Try it on the data from Lab 1\n",
    "\n",
    "Last but not least, let's try it on the data from last week's lab! This is a bit larger: 3.6x more samples and 2x more channels. We'll also be fitting 10x more neurons. The fast code you wrote in Problem 3 will make a big difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrWkNIrX2Cx8"
   },
   "source": [
    "## Load the preprocessed data and results from Lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9wJuTuQ2slC"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget -nc https://www.dropbox.com/s/sl6i7zn0e44ykk8/lab1_preprocessed_data.npy\n",
    "!wget -nc https://www.dropbox.com/s/4kpacfqj39tg7sn/lab1_results.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAWjwoNx23Cd"
   },
   "outputs": [],
   "source": [
    "# Set some constants\n",
    "sample_freq = 30000           # sampling frequency\n",
    "spike_width = 81              # width of a spike, in samples\n",
    "plot_slice = slice(0, 6000)   # 200ms window of frames to plot\n",
    "\n",
    "# Load the clustering results\n",
    "with open(\"lab1_results.pkl\", \"rb\") as f:\n",
    "    latents, params, hypers = pickle.load(f)\n",
    "num_neurons = hypers[\"num_neurons\"]\n",
    "\n",
    "# Load the data and plot it\n",
    "data = np.load(\"lab1_preprocessed_data.npy\")\n",
    "num_channels, num_samples = data.shape\n",
    "timestamps = np.arange(num_samples) / sample_freq\n",
    "plot_data(timestamps, data, scale=10, plot_slice=plot_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5i71inwFYW_D"
   },
   "source": [
    "## Fit the model using the fast MAP estimation code from Part 3\n",
    "This should still take about 4.5 minutes when initialized with random templates. You could also initialize with the results from Lab 1; in fact, this is what Kilosort v1 did. (The latest version has a slightly different approach.) If you dare, try running the \"slow\" code from Part 2. It should be about 20x slower, so you saved an hour by implementing Part 3! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCOkVOMrks2B"
   },
   "outputs": [],
   "source": [
    "# Initialize the templates randomly\n",
    "rng = np.random.default_rng(0)\n",
    "templates = generate_templates(rng, num_channels, spike_width, num_neurons)\n",
    "\n",
    "# Instead, you could use the templates from the mixture model fit in Lab 1\n",
    "# templates_t = to_t(params[\"templates\"])\n",
    "\n",
    "# Copy to GPU\n",
    "templates_t = to_t(templates)\n",
    "amplitudes_t = torch.zeros(num_neurons, num_samples, device=device, dtype=dtype)\n",
    "data_t = to_t(data)\n",
    "\n",
    "# Fit the model.\n",
    "lls = map_estimate_fast(rng, templates_t, amplitudes_t, data_t, \n",
    "                        noise_std=1.0, amp_rate=10.0)\n",
    "\n",
    "# If you dare, run the \"slow\" code for comparison...\n",
    "# lls = map_estimate(rng, templates_t, amplitudes_t, data_t, \n",
    "#                    noise_std=1.0, amp_rate=10.0)\n",
    "\n",
    "# Get the results off the GPU\n",
    "templates = from_t(templates_t)\n",
    "amplitudes = from_t(amplitudes_t)\n",
    "\n",
    "# Plot the log likelihoods\n",
    "plt.plot(lls, '-o')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.xlim(-.1, len(lls) - .9)\n",
    "plt.ylabel(\"Log Likelihood\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFoBUP45YcCk"
   },
   "source": [
    "## Plot the inferred templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0Poi2oJmkHI"
   },
   "outputs": [],
   "source": [
    "# Plot some templates (note, they're not ordered in any way)\n",
    "_ = plot_templates(templates, np.arange(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TmB2rO6Yfoq"
   },
   "source": [
    "## Plot the data with inferred spikes overlaid\n",
    "This doesn't show the ground truth, but you should see that the algorithm has picked up the major spikes in the voltage, and it has assigned similar spikes to the same neuron (colors). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5BfoNaCyosC"
   },
   "outputs": [],
   "source": [
    "# Find the spike times and neuron labels\n",
    "spike_times = []\n",
    "labels = []\n",
    "for neuron in trange(num_neurons):\n",
    "    times = np.where(amplitudes[neuron])[0]\n",
    "    spike_times.append(times)\n",
    "    labels.append(neuron * np.ones(len(times), dtype=int))\n",
    "\n",
    "spike_times = np.concatenate(spike_times)\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "# Find the channels that are significantly modulated in each template\n",
    "neuron_channels = []\n",
    "for template in templates:\n",
    "    thresh = np.percentile(abs(template), 97.5)\n",
    "    neuron_channels.append(np.any(abs(template) > thresh, axis=1))\n",
    "\n",
    "print(\"Found {} putative spikes\".format(len(spike_times)))\n",
    "\n",
    "# Plot the data and overlay the inferred spikes\n",
    "plot_data(timestamps, data, \n",
    "          labels=labels, \n",
    "          spike_times=spike_times, \n",
    "          neuron_channels=neuron_channels, \n",
    "          spike_width=spike_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0EWrQzcTfeI"
   },
   "source": [
    "## Problem 4a: Discussion\n",
    "This is an open-ended question so there's not necessarily a right answer.  We just want you to think critically about the algorithm and the results. Please respond to the following prompts:\n",
    "\n",
    "- In practice, you would post-process the extracted spikes to reject unrealistic neurons and merge overlapping ones. How would you approach this problem. \n",
    "- Over the course of a long recording session, the probe could drift up and down so that the channels activated by a neuron shift. How could you compensate for this slow drift in this model, or possibly try to correct for it during preprocessing?\n",
    "- How do you expect the computational complexity of your algorithm to scale with the key parameters $(T, D, C, N, K)$? Based on the runtime for this relatively small dataset in Part 4, what runtime would you expect for an hour long recording with 384 channels (as in a real Neuropixels probe)? Are there other computational concerns you might worry about as you scale to much longer recordings?\n",
    "\n",
    "*Answer below this line*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urFXE6RTUyAb"
   },
   "source": [
    "# Submission instructions\n",
    "- Print to PDF and download an .ipynb file. \n",
    "- Submit both on GradeScope"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "STAT220/320 Lab 2: Spike Sorting with Deconvolution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
