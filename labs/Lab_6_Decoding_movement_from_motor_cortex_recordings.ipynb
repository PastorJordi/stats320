{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV7RUxCC_Xw9"
   },
   "source": [
    "# Lab 6: Decoding movement from motor cortex recordings\n",
    "\n",
    "**STATS320: Machine Learning Methods for Neural Data Analysis**\n",
    "\n",
    "_Stanford University. Winter, 2021._\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/slinderman/stats320/blob/main/labs/Lab_6_Decoding_movement_from_motor_cortex_recordings.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Team Name:** _Your team name here_\n",
    "\n",
    "**Team Members:** _Names of everyone on your team here_\n",
    "\n",
    "*Due: 11:59pm Thursday, Feb 25, 2021 via GradeScope*\n",
    "\n",
    "---\n",
    "\n",
    "Last week we developed nonlinear _encoder_ models for predicting neural responses to sensory stimuli.  This week we'll develop _decoder_ models for inferring movements from neural spike trains.  One way to decode is via Bayesian inference: if we have a strong encoding model and a good prior distribution on movements, we can combine them to infer the posterior distribution of movements given spikes. This allows us to build constraints into the prior; e.g. that movements tend to be smooth. Ultimately, decoding is just another regression problem, and the Bayesian approach offers just one way of constructing a conditional distribution of movements given spikes. In this lab, we'll start with a Bayesian approach and then take a step back and construct a \"direct\" decoder that captures the dynamical structure of the Bayesian models while also capturing nonlinear features of the spike train.\n",
    "\n",
    "We'll use a dataset from the [Shenoy Lab](https://shenoy.people.stanford.edu/) (Stanford University) consisting of a 96-channel Utah array recording from motor cortex of a non-human primate performing a center-out reaching task. The data and a related model are described in (Gilja\\*, Nuyujukian\\*, et al, 2012).\n",
    "\n",
    "\n",
    "### References\n",
    "Gilja, Vikash, Paul Nuyujukian, Cindy A. Chestek, John P. Cunningham, Byron M. Yu, Joline M. Fan, Mark M. Churchland, et al. 2012. “A High-Performance Neural Prosthesis Enabled by Control Algorithm Design.” _Nature Neuroscience_ 15 (12): 1752–57.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBZ8NkZXAj9P"
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LATqLgaQ3Td1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "# Set some plotting defaults\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "# initialize a color palette for plotting\n",
    "palette = sns.xkcd_palette([\"windows blue\",\n",
    "                            \"red\",\n",
    "                            \"medium green\",\n",
    "                            \"dusty purple\",\n",
    "                            \"greyish\",\n",
    "                            \"orange\",\n",
    "                            \"amber\",\n",
    "                            \"clay\",\n",
    "                            \"pink\"])\n",
    "\n",
    "\n",
    "# Specify that we want our tensors on the CPU and in float32\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.float32\n",
    "\n",
    "# Helper function to convert between numpy arrays and tensors\n",
    "to_t = lambda array: torch.tensor(array, device=device, dtype=dtype)\n",
    "from_t = lambda tensor: tensor.to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lACnGctVl0Ec"
   },
   "outputs": [],
   "source": [
    "#@title Implement `train_model` function (run this cell!)\n",
    "def train_model(model, \n",
    "                train_dataset, \n",
    "                val_dataset,\n",
    "                objective,\n",
    "                regularizer=None,\n",
    "                num_epochs=100, \n",
    "                lr=0.1,\n",
    "                momentum=0.9,\n",
    "                lr_step_size=25,\n",
    "                lr_gamma=0.9):\n",
    "    # progress bars\n",
    "    pbar = trange(num_epochs)\n",
    "    pbar.set_description(\"---\")\n",
    "    inner_pbar = trange(len(train_dataset))\n",
    "    inner_pbar.set_description(\"Batch\")\n",
    "\n",
    "    # data loaders for train and validation\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=1)\n",
    "    dataloaders = dict(train=train_dataloader, val=val_dataloader)\n",
    "\n",
    "    # use standard SGD with a decaying learning rate\n",
    "    optimizer = optim.SGD(model.parameters(), \n",
    "                          lr=lr, \n",
    "                          momentum=momentum)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, \n",
    "                                    step_size=lr_step_size, \n",
    "                                    gamma=lr_gamma)\n",
    "    \n",
    "    # Keep track of the best model\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_loss = 1e8\n",
    "\n",
    "    # Track the train and validation loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in pbar:\n",
    "        for phase in ['train', 'val']:\n",
    "            # set model to train/validation as appropriate\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                inner_pbar.reset()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            # track the running loss over batches\n",
    "            running_loss = 0\n",
    "            running_size = 0\n",
    "            for datapoint in dataloaders[phase]:\n",
    "                velocity_t = to_t(datapoint['velocity']).squeeze(0)\n",
    "                spikes_t = to_t(datapoint['spikes']).squeeze(0)\n",
    "                if phase == \"train\":\n",
    "                    with torch.set_grad_enabled(True):\n",
    "                        optimizer.zero_grad()\n",
    "                        # compute the model output and loss\n",
    "                        output_t = model(spikes_t)\n",
    "                        loss_t = objective(output_t, velocity_t)\n",
    "                        # only add the regularizer in the training phase\n",
    "                        if regularizer is not None:\n",
    "                            loss_t += regularizer(model)\n",
    "\n",
    "                        # take the gradient and perform an sgd step\n",
    "                        loss_t.backward()\n",
    "                        optimizer.step()\n",
    "                    inner_pbar.update(1)\n",
    "                else:\n",
    "                    # just compute the loss in validation\n",
    "                    output_t = model(spikes_t)\n",
    "                    loss_t = objective(output_t, velocity_t)\n",
    "\n",
    "                assert torch.isfinite(loss_t)\n",
    "                running_loss += loss_t.item()\n",
    "                running_size += 1\n",
    "            \n",
    "            # compute the train/validation loss and update the best\n",
    "            # model parameters if this is the lowest validation loss yet\n",
    "            running_loss /= running_size\n",
    "            if phase == \"train\":\n",
    "                train_losses.append(running_loss)\n",
    "            else:\n",
    "                val_losses.append(running_loss)\n",
    "                if running_loss < best_loss:\n",
    "                    best_loss = running_loss\n",
    "                    best_model_wts = deepcopy(model.state_dict())\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.set_description(\"Epoch {:03} Train {:.4f} Val {:.4f}\"\\\n",
    "                             .format(epoch, train_losses[-1], val_losses[-1]))\n",
    "        pbar.update(1)\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return np.array(train_losses), np.array(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vui7h_TZAmc4"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "This data, kindly shared by Prof. Krishna Shenoy (Stanford), is used in his EE124 class, so it may be familiar to some of you. It's given as a Matlab file with a single struct called `R`. That struct contains data from a number of trials, each consisting of a spike train (`spikeRaster`), a `target`, and a time series of cursor positions (`cursorPos`). There are a bunch of other fields that we won't use for this lab.\n",
    "\n",
    "Note that the data is not spike sorted. We are simply given the times of putative spikes picked up on each of the 96 channels of the Utah array. \n",
    "<!-- Since the electrodes of the Utah are farther apart than those of a Neuropixels probe, it's less common to see spikes from the same neuron on more than one channel.  -->\n",
    "We might be able to extract some extra information by spike sorting, but to be consistent with previous analyses from the Shenoy lab, we'll just stick with spike counts per channel for this \n",
    "\n",
    "The spikes and positions are given in 1ms bins. We'll downsample to 25ms bins, which sacrifices some temporal resolution but will improve our signal-to-noise in the simple Bayesian models and our runtime in all cases.\n",
    "\n",
    "We pack the results into a list of dictionaries with the following keys:\n",
    "- `spikes` a (Tx96) array of spike counts for each of the 96 channels where T is the number of time bins in that trial\n",
    "- `position` a (Tx2) array of cursor positions\n",
    "- `velocity` a (Tx2) array of estimated cursor velocities\n",
    "- `condition` a number 0,..,8 specifying the reaching condition (i.e. starting point and target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCgRe-Zp1mV2"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget -nc https://www.dropbox.com/s/b99vsgiz679y5bw/JR_2015-12-04_truncated2.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Opt12nVW3X9I"
   },
   "outputs": [],
   "source": [
    "def load_data(bin_size):\n",
    "    data = loadmat('JR_2015-12-04_truncated2.mat')\n",
    "    R = data['R'][0]            # the infamous Shenoy lab \"R\" struct\n",
    "\n",
    "    # Extract the targets\n",
    "    targets = np.row_stack([Ri['target'].ravel() for Ri in R])\n",
    "    unique_targets, conditions = np.unique(targets, return_inverse=True, axis=0)\n",
    "\n",
    "    # Extract the spikes and positions in 10ms bins\n",
    "    def bin_spikes(trial):\n",
    "        spikes =  R[trial]['spikeRaster'].toarray()\n",
    "        num_neurons, num_frames = spikes.shape\n",
    "        num_bins = int(np.ceil(num_frames / bin_size))\n",
    "        pad = np.zeros((num_neurons, num_bins * bin_size - num_frames))\n",
    "        padded_spikes = np.column_stack([spikes, pad])\n",
    "        return padded_spikes.reshape(num_neurons, num_bins, bin_size).sum(-1)\n",
    "\n",
    "    def bin_positions(trial, smooth=20):\n",
    "        pos = R[trial]['cursorPos'][:2]\n",
    "        dim, num_frames = pos.shape\n",
    "        num_bins = int(np.ceil(num_frames / bin_size))\n",
    "        smooth_pos = gaussian_filter1d(pos, smooth, axis=1)\n",
    "        f = interp1d(np.arange(num_frames), smooth_pos, \n",
    "                     axis=-1, fill_value=\"extrapolate\")\n",
    "        return f(np.arange(num_bins) * bin_size)\n",
    "\n",
    "    # Get the spikes, cursor positions, and cursor velocities for each trial\n",
    "    all_data = []\n",
    "    for trial in trange(len(R)):\n",
    "        spk = bin_spikes(trial).T\n",
    "        pos = bin_positions(trial).T\n",
    "        vel = np.gradient(pos, axis=0)\n",
    "        con = conditions[trial]\n",
    "        all_data.append(dict(spikes=spk, \n",
    "                            position=pos, \n",
    "                            velocity=vel, \n",
    "                            condition=con))\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_data = all_data[:400]\n",
    "    test_data = all_data[400:]\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "bin_size = 25\n",
    "train_data, test_data = load_data(bin_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzNFVh9bDyhg"
   },
   "source": [
    "## Plot the spike counts and position/velocity times for a single trial\n",
    "\n",
    "Let's look at one trial. It consists of a time series of spike counts from each channel of the Utah array, as well as a position time series. We also plot the velocity, which is just a first-order estimate of the derivative of the position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5kIogG43m_k"
   },
   "outputs": [],
   "source": [
    "def plot_trial(data, trial):\n",
    "    fig, axs = plt.subplots(3, 1, sharex=True, figsize=(8, 8))\n",
    "\n",
    "    axs[0].imshow(data[trial][\"spikes\"].T, \n",
    "                interpolation=\"none\", \n",
    "                aspect=\"auto\", \n",
    "                cmap=\"Greys\")\n",
    "    axs[0].set_ylabel(\"channel\")\n",
    "    axs[0].set_title(\"trial {}\".format(trial))\n",
    "\n",
    "    axs[1].plot(data[trial][\"position\"][:, 0], label='x')\n",
    "    axs[1].plot(data[trial][\"position\"][:, 1], label='y')\n",
    "    axs[1].set_ylabel(\"position [mm]\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    axs[2].plot(data[trial][\"velocity\"][:, 0], label='x')\n",
    "    axs[2].plot(data[trial][\"velocity\"][:, 1], label='y')\n",
    "    axs[2].set_xlabel(\"time bin [{}ms]\".format(bin_size))\n",
    "    axs[2].set_ylabel(\"velocity [mm / bin]\")\n",
    "    axs[2].legend()\n",
    "\n",
    "plot_trial(train_data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQtMaSW5DttT"
   },
   "source": [
    "## Plot the cursor positions across all trials\n",
    "\n",
    "Plot the reach directions color-coded by the condition type. Most conditions start at the center and move outward to a target. One condition type (gray) starts at the perimeter and moves to the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frP7vwq23eUV"
   },
   "outputs": [],
   "source": [
    "def plot_reach_trajectories(data):\n",
    "    for d in data:\n",
    "        plt.plot(d[\"position\"][:, 0], \n",
    "                 d[\"position\"][:, 1], \n",
    "                 color=palette[d[\"condition\"]])\n",
    "        \n",
    "    plt.xlabel(\"cursor x position [mm]\")\n",
    "    plt.ylabel(\"cursor y position [mm]\")\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "plot_reach_trajectories(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqD5QLypasoV"
   },
   "source": [
    "# Part 1: Implement a simple Bayesian decoder\n",
    "\n",
    "Our first model is an extremely simple Bayesian decoder.  Let $x_t^{(\\ell)} \\in \\mathbb{R}^2$ denote the **cursor velocity** at time $t$ on trial $\\ell$. Likewise, let $y_t^{(\\ell)} \\in \\mathbb{R}^{96}$ denote the spike count measured on the 96-channel array at time $t$ on trial $\\ell$. Let $T_\\ell$ denote the number of time bins of trial $\\ell$, and let $L$ denote the total number of trials. \n",
    "\n",
    "We will estimate the posterior distribution over velocity under the following model,\n",
    "\\begin{align}\n",
    "x_t^{(\\ell)} &\\sim \\mathcal{N}(0, Q) \\\\\n",
    "y_t^{(\\ell)} \\mid x_t^{(\\ell)} &\\sim \\mathcal{N}(C x_t^{(\\ell)} + d, R).\n",
    "\\end{align}\n",
    "The model has the following parameters,\n",
    "- $Q \\in \\mathbb{R}^{2 \\times 2}$: the prior covariance of the velocity\n",
    "- $C \\in \\mathbb{R}^{96 \\times 2}$: the _emission_ matrix\n",
    "- $d \\in \\mathbb{R}^{96}$: the _emission_ bias\n",
    "- $R \\in \\mathbb{R}^{96 \\times 96}$: the _emission_ covariance\n",
    "Note that $Q$ and $R$ are positive definite matrices.\n",
    "\n",
    "As we derived in class, the posterior distribution of the velocity given the spike counts is,\n",
    "\\begin{align}\n",
    "p(x_t^{(\\ell)} \\mid y_t^{(\\ell)}) &= \\mathcal{N}(\\mu_t, \\Sigma) \\\\\n",
    "\\Sigma &= J^{-1}  \n",
    "& \\mu_t &= J^{-1} h_t \\\\\n",
    "J &= Q^{-1} + C^\\top R^{-1} C &\n",
    "h_t &= C^\\top R^{-1}(y_t^{(\\ell)} - d),\n",
    "\\end{align}\n",
    "where $(\\mu_t, \\Sigma)$ are the posterior mean and covariance, respectively, and $(J, h_t)$ are the _natural_ (aka _information-form_) parameters. \n",
    "\n",
    "To be consistent with the subsequent parts of the lab, we will formulate the model as a conditional distribution over the entire time series of velocities, $\\mathbf{x}^{(\\ell)} = (x_1^{(\\ell)}, \\ldots, x_{T_\\ell}^{(\\ell)}) \\in \\mathbb{R}^{T \\times 2}$ given the entire time series of spike counts $\\mathbf{y}^{(\\ell)} \\in \\mathbb{R}^{T \\times 96}$. Since the simple model factors into a product of Gaussians for each time bin, and since each Gaussian has the same covariance, the full posterior distribution is,\n",
    "\\begin{align}\n",
    "p(\\mathbf{x}^{(\\ell)} \\mid \\mathbf{y}^{(\\ell)}) \n",
    "&= \\prod_{t=1}^{T_\\ell} p(x_t^{(\\ell)} \\mid y_t^{(\\ell)}) \\\\\n",
    "&= \\mathcal{N}\\left( \\mathrm{vec}(\\mathbf{x}^{(\\ell)}) \\, \\bigg| \\,\n",
    "    \\begin{bmatrix} \\mu_1 \\\\ \\vdots \\\\ \\mu_{T_\\ell} \\end{bmatrix},\n",
    "    \\begin{bmatrix} \\Sigma & & \\\\ & \\ddots & \\\\ & & \\Sigma \\end{bmatrix} \n",
    "    \\right).\n",
    "\\end{align}\n",
    "\n",
    "In Part 1, you'll estimate the parameters of this model, $\\{Q, C, d, R\\}$ and implement it as a PyTorch module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEKYpsZ2LFGn"
   },
   "source": [
    "## Problem 1a: Estimate the prior covariance $Q$\n",
    "\n",
    "Estimate the prior covariance, assuming the velocity is zero mean, as \n",
    "\\begin{align}\n",
    "\\hat{Q} &= \\frac{1}{\\sum_{\\ell = 1}^L T_\\ell} \\left(\\sum_{\\ell=1}^L \\sum_{t=1}^{T_\\ell} x_t^{(\\ell)} (x_t^{(\\ell)})^\\top \\right).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IorpLmxaFIvl"
   },
   "outputs": [],
   "source": [
    "def estimate_basic_prior_prms(train_data):\n",
    "    \"\"\"\n",
    "    Estimate \\hat{Q} from the training data.\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    ...\n",
    "    #\n",
    "    ###\n",
    "    return Q_hat\n",
    "\n",
    "Q_hat = estimate_basic_prior_prms(train_data)\n",
    "assert np.allclose(Q_hat, np.array([[15.6646, -0.9956],\n",
    "                                    [-0.9956, 16.4991]]),\n",
    "                   atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwGtJmYLLKvu"
   },
   "source": [
    "## Problem 1b: Estimate the encoding model parameters\n",
    "\n",
    "Estimate $C$, $d$, and $R$ by fitting a linear regression to the training velocity and spikes.\n",
    "\n",
    "You can use the `LinearRegression` class from `sklearn.linear_model` (imported above), `scipy.linalg.lstsq`, or you can do it manually. Your choice!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1gSElO0FcL8"
   },
   "outputs": [],
   "source": [
    "def estimate_emission_prms(train_data):\n",
    "    \"\"\"Estimate \\hat{C}, \\hat{d}, and \\hat{R} from the training data.\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    ...\n",
    "    #\n",
    "    ###\n",
    "    return C_hat, d_hat, R_hat\n",
    "\n",
    "C_hat, d_hat, R_hat = estimate_emission_prms(train_data)\n",
    "assert np.allclose(C_hat.sum(), -0.1172, atol=1e-4)\n",
    "assert np.allclose(C_hat.std(), 0.0121, atol=1e-4)\n",
    "assert np.allclose(d_hat.sum(), 39.7506, atol=1e-1)\n",
    "assert np.allclose(d_hat.std(), 0.3210, atol=1e-4)\n",
    "assert np.allclose(R_hat.sum(), 587.7989, atol=1e-1)\n",
    "assert np.allclose(R_hat.std(), 0.1022, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Fm4Yg5pLUDI"
   },
   "source": [
    "## Problem 1c: Implement the simple decoder model\n",
    "\n",
    "Implement the simple decoder as a PyTorch `Module`. We won't be fitting any parameters (you just estimated them above!), but implementing it this way will allow us to compare directly to the model developed in Part 3.\n",
    "\n",
    "Specifically, complete the `forward` function to compute `J`, `h`, `mu`, and `Sigma`. The functions `torch.solve` and `torch.inverse` will be useful. \n",
    "\n",
    "_Note: there are generally more numerically stable/efficient approaches than directly computing a matrix inverse&mdash;solve is almost always preferable!&mdash;but for this lab go ahead and call `torch.inverse` to compute `Sigma`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ne7VwLmpvvLN"
   },
   "outputs": [],
   "source": [
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, Q, C, d, R):\n",
    "        super(SimpleDecoder, self).__init__()\n",
    "        \n",
    "        # Store the parameters as tensors\n",
    "        self.Q = to_t(Q)\n",
    "        self.C = to_t(C)\n",
    "        self.d = to_t(d)\n",
    "        self.R = to_t(R)\n",
    "        \n",
    "    def forward(self, spikes):\n",
    "        \"\"\"\n",
    "        spikes: T x 96 array of spikes for a single trial\n",
    "        returns: (mu, Sigma) where \n",
    "            - mu is the (T x 2) posterior mean of the velocity\n",
    "            - Sigma is the (2 x 2) posterior covariance (the same for all bins)\n",
    "        \"\"\"\n",
    "        # Get the model parameters so we don't always have to reference self\n",
    "        Q, C, d, R = self.Q, self.C, self.d, self.R\n",
    "\n",
    "        # Convert spikes to tensor, if they're not already\n",
    "        Y = to_t(spikes)\n",
    "        T, _ = Y.shape\n",
    "\n",
    "        ###\n",
    "        # YOUR CODE BELOW\n",
    "        #\n",
    "        J = ...     # 2x2 tensor\n",
    "        Sigma = ... # 2x2 tensor\n",
    "        h = ...     # Tx2 tensor\n",
    "        mu = ...    # Tx2 tensor\n",
    "        #\n",
    "        ###\n",
    "\n",
    "        # Convert Sigma to a 2T x 2T block diagonal matrix for consistency\n",
    "        # with the models in Parts 2 and 3\n",
    "        assert Sigma.shape == (2, 2)\n",
    "        Sigma = torch.block_diag(*(Sigma * torch.ones(T, 2, 2)))\n",
    "\n",
    "        # Return the mean and covariance\n",
    "        return mu, Sigma\n",
    "        \n",
    "\n",
    "# Instantiate the model\n",
    "Q_hat = estimate_basic_prior_prms(train_data)\n",
    "C_hat, d_hat, R_hat = estimate_emission_prms(train_data)\n",
    "simple_decoder = SimpleDecoder(Q_hat, C_hat, d_hat, R_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrPcxn2QbGRh"
   },
   "source": [
    "## Problem 1d: Compute the multivariate normal log probability\n",
    "\n",
    "Write a function to compute the (average negative) log probability under the a multivariate normal distribution,\n",
    "\\begin{align}\n",
    "p(\\mathbf{x}^{(\\ell)} \\mid \\mathbf{y}^{(\\ell)}) \n",
    "&= \\prod_{t=1}^{T_\\ell} p(x_t^{(\\ell)} \\mid y_t^{(\\ell)}) \\\\\n",
    "&= \\mathcal{N}\\left( \\mathrm{vec}(\\mathbf{x}^{(\\ell)}) \\, \\bigg| \\,\n",
    "    \\boldsymbol{\\mu}, \\mathbf{\\Sigma}\n",
    "    \\right).\n",
    "\\end{align}\n",
    "where $\\mathrm{vec}(\\mathbf{x}^{(\\ell)}) \\in \\mathbb{R}^{2T}$ is the vectorized array of velocities, $\\boldsymbol{\\mu} \\in \\mathbb{R}^{2T}$ is the mean, and $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{2T \\times 2T}$ is the covariance matrix.\n",
    "\n",
    "The function will take in the velocity and mean as shape (T,2) tensors, so you'll have to do some flattening. We'll negate and normalize for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgmIMOIdIN-q"
   },
   "outputs": [],
   "source": [
    "def mvn_loss(prms, velocity):\n",
    "    \"\"\"Compute the average negative log-likelihood under a \n",
    "    multivariate normal model.\n",
    "\n",
    "    prms: tuple containing (mean, cov) where:\n",
    "        mean: T x 2 tensor of predicted velocity mean\n",
    "        cov:  2T x 2T tensor of predicted velocity covariance\n",
    "    velocity: T x 2 array of measured velocity\n",
    "    returns: average negative log likelihood (mean over all spikes)\n",
    "    \"\"\"\n",
    "    mean, cov = prms\n",
    "    velocity = to_t(velocity)\n",
    "    \n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    ll = ...\n",
    "    ###\n",
    "    return -ll / velocity.numel()\n",
    "    \n",
    "assert torch.isclose(\n",
    "    mvn_loss(simple_decoder(train_data[0]['spikes']), \n",
    "             train_data[0]['velocity']),\n",
    "    torch.tensor(2.6651), atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5UrxxHdcXBU"
   },
   "source": [
    "## Plot the decoded positions\n",
    "That's it! No more fitting required. You already estimated the parametes in Problems 1a and 1b. Now we'll decode the velocities from the test set and integrate them to get positions that we can compare to the true cursor trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJ1O9PazxGZx"
   },
   "outputs": [],
   "source": [
    "def plot_decoder(test_data, decoder):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "    for trial in trange(len(test_data)):\n",
    "        spikes = test_data[trial][\"spikes\"]\n",
    "        position = test_data[trial][\"position\"]\n",
    "        cond = test_data[trial][\"condition\"]\n",
    "        \n",
    "        # decode velocity and integrate to get position\n",
    "        dec_velocity = from_t(decoder(spikes)[0])\n",
    "        dec_position = position[0] + np.cumsum(dec_velocity, axis=0)\n",
    "\n",
    "        axs[0].plot(position[:, 0], position[:, 1], color=palette[cond])\n",
    "        axs[1].plot(dec_position[:, 0], dec_position[:, 1], color=palette[cond])\n",
    "    \n",
    "    axs[0].set_title(\"test movements\")\n",
    "    axs[0].set_xlabel(\"cursor x position\")\n",
    "    axs[0].set_ylabel(\"cursor y position\")\n",
    "    axs[1].set_xlabel(\"cursor x position\")\n",
    "    axs[1].set_title(\"decoded test movements\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_decoder(test_data, simple_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VZCeZmpcuYe"
   },
   "source": [
    "## Problem 1e: [Short answer] Discuss your results\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgRqYIqFXLhY"
   },
   "source": [
    "# Part 2: Bayesian decoding with a linear dynamical system prior\n",
    "\n",
    "The simple model failed to account for any temporal correlations in the velocities. This followed from the fact that both the prior and the likelihood were independent across time bins. In this section, we'll address this shortcoming by building temporal dependencies into the prior.\n",
    "\n",
    "Consider the following model instead,\n",
    "\\begin{align}\n",
    "x_1^{(\\ell)} &\\sim \\mathcal{N}(0, Q_0) \\\\\n",
    "x_t^{(\\ell)} &\\sim \\mathcal{N}(A x_{t-1}, Q) & \\text{for }  t&=2,\\ldots, T\\\\\n",
    "y_t^{(\\ell)} \\mid x_t^{(\\ell)} &\\sim \\mathcal{N}(C x_t^{(\\ell)} + d, R) \n",
    " & \\text{for }  t&=1,\\ldots, T.\n",
    "\\end{align}\n",
    "\n",
    "Under this model, the velocity $x_t^{(\\ell)}$ is a linear function of the preceding velocity with additive Gaussian noise. This is called a _linear dynamical system_ (LDS). These autoregressive dependencies of the LDS lead to correlations across time in the posterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJawIyJCdy7i"
   },
   "source": [
    "## Problem 2a [Math]: Derive the natural parameters of the LDS posterior\n",
    "\n",
    "As we discussed in class, the posterior distribution of the LDS is also a multivariate Gaussian, \n",
    "\\begin{align}\n",
    "p(\\mathbf{x}^{(\\ell)} \\mid \\mathbf{y}^{(\\ell)}) \n",
    "&= \\mathcal{N}\\left( \\mathrm{vec}(\\mathbf{x}^{(\\ell)}) \\, \\bigg| \\,\n",
    "    \\boldsymbol{\\mu}, \\mathbf{\\Sigma}\n",
    "    \\right).\n",
    "\\end{align}\n",
    "but here the covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{2T \\times 2T}$ is no longer block diagonal. Instead, we have,\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\Sigma} &= \\mathbf{J}^{-1} &\n",
    "\\boldsymbol{\\mu} &= \\mathbf{J}^{-1} \\mathbf{h} \n",
    "\\end{align}\n",
    "where $\\mathbf{J} \\in \\mathbb{R}^{2T \\times 2T}$ is a block tridiagonal matrix and $\\mathbf{h} \\in \\mathbb{R}^{2T}$ is a vector,\n",
    "\\begin{align}\n",
    "\\mathbf{J} &=\n",
    "\\begin{bmatrix}\n",
    "J_{11} & J_{21}^\\top &    &           &                \\\\\n",
    "J_{21} & J_{22} &  \\ddots &           &                \\\\\n",
    "       & \\ddots & \\ddots  & \\ddots    &                \\\\\n",
    "       &        & \\ddots  & \\ddots    & J_{T,T-1}^\\top \\\\\n",
    "       &        &         & J_{T,T-1} & J_{T,T}\n",
    "\\end{bmatrix}\n",
    "&\n",
    "\\mathbf{h} &=\n",
    "\\begin{bmatrix}\n",
    "h_1 \\\\\n",
    "h_2 \\\\\n",
    "\\vdots \\\\\n",
    "h_T\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "Derive the form of the natural parameters of the posterior distribution, including:\n",
    "- the diagonal blocks $J_{tt}$ (including the special cases for $t=1$ and $t=T$), \n",
    "- the lower diagonal blocks $J_{t,t-1}$, and\n",
    "- the linear terms $h_t$. \n",
    "\n",
    "To do so, apply Bayes rule, expanding the joint probability, and collecting terms to write the posterior as a multivariate normal distribution on $\\mathbf{x}^{(\\ell)}$.\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-ix2297XRRX"
   },
   "source": [
    "## Problem 2b: Estimate the velocity dynamics\n",
    "\n",
    "Write a function to estimate the prior parameters $\\hat{Q}_0$, $\\hat{A}$, and $\\hat{Q}$. Estimate $\\hat{Q}_0$ as the empirical variance of the initial velocity (similar to Problem 1a), and estimate $\\hat{A}$ and $\\hat{Q}$ via linear regression (similar to Problem 1b).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQeUwJ_b0Zoh"
   },
   "outputs": [],
   "source": [
    "def estimate_dynamics_prms(train_data):\n",
    "    \"\"\"\n",
    "    Estimate the prior parameters Q0, A, and Q, as described above.\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    #\n",
    "    ...\n",
    "    #\n",
    "    ###\n",
    "    return Q0_hat, A_hat, Q_hat\n",
    "\n",
    "Q0_hat, A_hat, Q_hat = estimate_dynamics_prms(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTy7OS5Rib3a"
   },
   "source": [
    "## A helper function to construct a symmetric block tridiagonal matrix from its parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_baE501Na8dr"
   },
   "outputs": [],
   "source": [
    "def symm_block_tridiag(J_diag, J_lower_diag):\n",
    "    \"\"\"\n",
    "    Helper function to construct a symmetric block tridiagonal matrix\n",
    "    from its diagonal blocks (J_diag) and lower diagonal blocks (J_lower_diag).\n",
    "    The shape of the output is inferred from the shapes of the inputs\n",
    "\n",
    "    J_diag: T x D x D tensor of diagonal blocks\n",
    "    J_lower_diag: T-1 x D x D tensor of lower diagonal blocks\n",
    "\n",
    "    returns: TD x TD symmetric block tridiagonal matrix\n",
    "    \"\"\"\n",
    "    T, D, _ = J_diag.shape\n",
    "    assert J_diag.shape[2] == D\n",
    "    assert J_lower_diag.shape == (T-1, D, D)\n",
    "\n",
    "    J = torch.block_diag(*J_diag)\n",
    "    J[D:, :-D] += torch.block_diag(*J_lower_diag)\n",
    "    J[:-D, D:] += torch.block_diag(*torch.transpose(J_lower_diag, 1, 2))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUCTuFyOis4j"
   },
   "source": [
    "## Problem 2c: Implement the LDS decoder\n",
    "\n",
    "Implement the LDS decoder as a PyTorch module, using your derivations from Problem 2a and the helper function above to construct the big $\\mathbf{J}$ matrix and $\\mathbf{h}$ vector. Then use `torch.invert` to convert the natural parameters to mean parameters $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$. \n",
    "\n",
    "As in Part 1, return $\\boldsymbol{\\mu}$ as a `T x 2` tensor and $\\boldsymbol{\\Sigma}$ as a `2T x 2T` tensor.\n",
    "\n",
    "**Note**: Inverting $\\mathbf{J}$ naively takse $O(T^3)$ operations since it's a $2T \\times 2T$ matrix.  We can get this down to $O(T)$ using the Kalman smoother&mdash;a dynamic programming algorithm. We'll talk more about this in class next week or the week after. For now, just pay the cubic price since these matrices aren't that big anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwfMZuv71Syj"
   },
   "outputs": [],
   "source": [
    "class LDSDecoder(nn.Module):\n",
    "    def __init__(self, Q0, A, Q, C, d, R):\n",
    "        self.Q0 = to_t(Q0)\n",
    "        self.A = to_t(A)\n",
    "        self.Q = to_t(Q)\n",
    "        self.C = to_t(C)\n",
    "        self.d = to_t(d)\n",
    "        self.R = to_t(R)\n",
    "        super(LDSDecoder, self).__init__()\n",
    "        \n",
    "    def forward(self, spikes):\n",
    "        \"\"\"\n",
    "        spikes: T x num_channels array of spikes for a single trial\n",
    "        returns: (mu, Sigma) where \n",
    "            - mu is the (T x 2) posterior mean of the velocity\n",
    "            - Sigma is the (2 x 2) posterior covariance (the same for all bins)\n",
    "        \"\"\"\n",
    "        # Get the model parameters so we don't always have to reference self\n",
    "        Q0, A, Q, C, d, R = self.Q0, self.A, self.Q, self.C, self.d, self.R\n",
    "\n",
    "        # Convert spikes to tensor, if they're not already\n",
    "        Y = to_t(spikes)\n",
    "        T, _ = Y.shape\n",
    "\n",
    "        # Initialize the diagonal and lower diagonal blocks of J\n",
    "        J_diag = torch.zeros(T, 2, 2)\n",
    "        J_lower_diag = torch.zeros(T-1, 2, 2)\n",
    "\n",
    "        ###\n",
    "        # YOUR CODE BELOW\n",
    "        #\n",
    "        \n",
    "        # Fill in the diagonal blocks of J\n",
    "        ...        \n",
    "        \n",
    "        # Fill in the lower diagonal blocks of J\n",
    "        ...\n",
    "\n",
    "        # Construct the full J matrix\n",
    "        J = symm_block_tridiag(J_diag, J_lower_diag)\n",
    "\n",
    "        # Construct the full h vector\n",
    "        h = ...\n",
    "        \n",
    "        # Convert to standard parameters\n",
    "        Sigma = ...\n",
    "        mu = ...\n",
    "        #\n",
    "        ###\n",
    "\n",
    "        return mu, Sigma\n",
    "        \n",
    "# Instantiate the decoder\n",
    "Q0_hat, A_hat, Q_hat = estimate_dynamics_prms(train_data)\n",
    "C_hat, d_hat, R_hat = estimate_emission_prms(train_data)\n",
    "lds_decoder = LDSDecoder(Q0_hat, A_hat, Q_hat, C_hat, d_hat, R_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMoEM8IzkGkx"
   },
   "source": [
    "## Plot the decoded positions\n",
    "Once again, that's it! No more fitting required. You already estimated the parametes in Problems 1a and 1b. Now we'll decode the velocities from the test set and integrate them to get positions that we can compare to the true cursor trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfYCrpZj2Tn9"
   },
   "outputs": [],
   "source": [
    "plot_decoder(test_data, lds_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PY9GUcYRkOZS"
   },
   "source": [
    "## Problem 2d: [Short answer] Discuss your results\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThUIrzd7iyWW"
   },
   "source": [
    "# Part 3: Direct decoding with a LDS-inspired model\n",
    "\n",
    "One of the limitations of the LDS in Part 2 is that it treated the spikes as a **linear function** of the instantaneous velocity with **additive Gaussian** noise. We could work on the encoder and instead substitute a Poisson GLM for $p(y_t^{(\\ell)} \\mid x_t^{(\\ell)})$, for example, but that would destroy the conjugacy of the model, and the resulting posterior distribution would no longer have a closed form. \n",
    "\n",
    "A common approach to Bayesian inference in \"non-conjugate\" models is to approximate the resulting posterior with a Gaussian distribution; e.g. via Laplace approximation. Note that the Gaussian approximation to the non-conjugate model's posterior is not the same as the posterior under a linear Gaussian model&mdash;it is influenced by the nonlinear likelihood model, for instance.\n",
    "\n",
    "That said, if we're ultimately going to approximate the posterior as a Gaussian, why not just _learn_ to output a Gaussian distribution whose mean and covariance are some function of the spikes $\\mathbf{y}^{(\\ell)}$? There are a few challenges:\n",
    "- The output of the model would include a $2T_\\ell \\times 2T_\\ell$ covariance matrix, which grows quadratically with the length of the input.\n",
    "- Moreover, we want to handle arbitrary length time series ($T_\\ell$ is not fixed). \n",
    "\n",
    "To handle these two concerns, we'll develop a _structured_ prediction model that constrains the Gaussian covariance matrix to mimic that of an LDS. In particular, we'll constrain the inverse covariance matrix (aka the precision matrix) to be block-tridiagonal, as in the LDS. However, we'll allow the diagonal blocks of $\\mathbf{J}$ and the entries in $\\mathbf{h}$ to be learned, nonlinear functions of the spike train.\n",
    "\n",
    "Concretely, we'll implement the following model:\n",
    "\\begin{align}\n",
    "p(\\mathbf{x}^{(\\ell)} \\mid \\mathbf{y}^{(\\ell)}) \n",
    "&= \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) \\\\\n",
    "\\boldsymbol{\\Sigma} &= \\mathbf{J}^{-1}\\\\\n",
    "\\boldsymbol{\\mu} &= \\mathbf{J}^{-1} \\mathbf{h}\n",
    "\\end{align}\n",
    "where $\\mathbf{J}$ and $\\mathbf{h}$ have blocks,\n",
    "\\begin{align}\n",
    "J_{t,t-1} &= -Q^{-1} A \\\\\n",
    "J_{tt} &= Q^{-1} + A^\\top Q^{-1} A + f(\\mathbf{y}_{t-\\Delta:t+\\Delta}^{(\\ell)}) \\\\\n",
    "h_{t} &= g(\\mathbf{y}_{t-\\Delta:t+\\Delta}^{(\\ell)})\n",
    "\\end{align}\n",
    "Here, $f$ and $g$ are learned functions of the data in a window of size $2\\Delta$ around the current bin. \n",
    "(Note that you'll have to handle edge cases in $J_{11}$ and $J_{TT}$ as in Part 2.)\n",
    "\n",
    "This model retains the LDS prior that couples the velocities via a linear Gaussian model, but it allows richer functions of the data in place of the likelihood. Let's see if we can get this model to improve on the decoders above!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvUN0pBMnPVn"
   },
   "source": [
    "## Problem 3a: Implement a structured decoder with features from a CNN\n",
    "\n",
    "Implement the functions $f$ and $g$ above by first passing the spikes through a `Conv1d` layer with 96 input channels, `interm_dim` output channels, a kernel of size `kernel_size`, and half that padding on either end to obtain a `T x interm_dim` output. Pass the output through a `relu` and then apply two separate linear layers. \n",
    "1. The first linear layer gives you a `Tx2` tensor containing the linear coefficients $h_t$.\n",
    "2. The second layer gives you a `Tx2` tensor which you'll then pass through a softplus to make non-negative. Add those non-negative entries to the diagonals of your precision blocks $J_{tt}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLOmJfaDerGl"
   },
   "outputs": [],
   "source": [
    "class StructuredDecoder(nn.Module):\n",
    "    def __init__(self, Q0, A, Q, \n",
    "                 num_channels=96,\n",
    "                 kernel_size=5,\n",
    "                 interm_dim=10):\n",
    "        super(StructuredDecoder, self).__init__()\n",
    "        self.Q0 = to_t(Q0)\n",
    "        self.A = to_t(A)\n",
    "        self.Q = to_t(Q)\n",
    "        \n",
    "        ###\n",
    "        # Initialize the learnable layers\n",
    "        # YOUR CODE BELOW\n",
    "        self.conv = nn.Conv1d(...)\n",
    "        self.linear_h = nn.Linear(...)\n",
    "        self.linear_J = nn.Linear(...)\n",
    "        #\n",
    "        ###\n",
    "        \n",
    "    def forward(self, spikes):\n",
    "        \"\"\"\n",
    "        spikes: T x 96 array of spikes for a single trial\n",
    "        returns: (mu, Sigma) where \n",
    "            - mu is the (T x 2) posterior mean of the velocity\n",
    "            - Sigma is the (2T x 2T) posterior covariance matrix\n",
    "        \"\"\"\n",
    "        # Get the model parameters so we don't always have to reference self\n",
    "        Q0, A, Q = self.Q0, self.A, self.Q\n",
    "\n",
    "        # Convert spikes to tensor, if they're not already\n",
    "        Y = to_t(spikes)\n",
    "        T, _ = Y.shape\n",
    "\n",
    "        # Initialize the diagonal and lower diagonal blocks of J\n",
    "        J_diag = torch.zeros(T, 2, 2)\n",
    "        J_lower_diag = torch.zeros(T-1, 2, 2)\n",
    "\n",
    "        ###\n",
    "        # YOUR CODE BELOW\n",
    "        #\n",
    "        ...\n",
    "\n",
    "        Sigma = ...\n",
    "        mu = ...\n",
    "        #\n",
    "        ###\n",
    "        assert torch.all(torch.isfinite(mu))\n",
    "        assert torch.all(torch.isfinite(Sigma))\n",
    "        return mu, Sigma\n",
    "        \n",
    "torch.manual_seed(0)\n",
    "struct_decoder = StructuredDecoder(Q0_hat, A_hat, Q_hat)\n",
    "mvn_loss(struct_decoder(train_data[0][\"spikes\"]),\n",
    "         train_data[0][\"velocity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCcBDeqisLqu"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4G_fjYDG7H7R"
   },
   "outputs": [],
   "source": [
    "print(\"Training structured decoder model. This should take about 4 minutes...\")\n",
    "train_losses, val_losses = train_model(struct_decoder, \n",
    "                                       train_data[:300], \n",
    "                                       train_data[300:], \n",
    "                                       mvn_loss, \n",
    "                                       lr=1e-4, \n",
    "                                       lr_step_size=100)\n",
    "\n",
    "# Plot the training and validation curves\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].plot(train_losses, color=palette[0], label=\"train\")\n",
    "axs[0].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
    "axs[0].set_xlabel(\"epoch\")\n",
    "axs[0].set_ylabel(\"loss\")\n",
    "axs[0].grid(True)\n",
    "axs[0].legend(loc=\"upper right\")\n",
    "\n",
    "axs[1].plot(train_losses, color=palette[0], label=\"train\")\n",
    "axs[1].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
    "axs[1].set_xlabel(\"epoch\")\n",
    "axs[1].set_ylabel(\"loss\")\n",
    "axs[1].set_ylim(top=val_losses[20])\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hf79fdOcsS0f"
   },
   "source": [
    "## Plot the decoded movements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsMCkwZkCqd0"
   },
   "outputs": [],
   "source": [
    "plot_decoder(test_data, struct_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZnxJSUxsZYT"
   },
   "source": [
    "## Problem 3b: Compute and report the test loss for all 3 decoders\n",
    "\n",
    "Compute the test loss for all three models, averaging the loss from each test trial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOBikPa-CVNK"
   },
   "outputs": [],
   "source": [
    "def compute_test_loss(decoder):\n",
    "    \"\"\"\n",
    "    Compute the average loss on the test set\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    ...\n",
    "    #\n",
    "    ###\n",
    "    return avg_nll\n",
    "    \n",
    "print(\"Average test loss:\")\n",
    "print(\"Simple:     {:.4f}\".format(from_t(compute_test_loss(simple_decoder))))\n",
    "print(\"LDS:        {:.4f}\".format(from_t(compute_test_loss(lds_decoder))))\n",
    "print(\"Structured: {:.4f}\".format(from_t(compute_test_loss(struct_decoder))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05DtI2DssrFL"
   },
   "source": [
    "# Part 4: Discussion\n",
    "\n",
    "Write a few paragraphs summarizing what you learned in this lab. \n",
    "\n",
    "Here are a few prompts to consider (no need to respond to all; these are just to spark some ideas):\n",
    "\n",
    "- Do the numerical results match your intuitive judgements of decoding quality?\n",
    "- How might you generalize the structured decoder?\n",
    "- What might you expect if you tried to directly decode cursor position instead of velocity?\n",
    "- Decoding performance could be limited by either the raw amount of information about movement contained in the neural activity, or by the particular form of decoder that we employed. What do you think the limiting factor is in this lab?\n",
    "- We only plotted the mean of the decoder, $\\boldsymbol{\\mu}$. What could you learn from the covariance $\\boldsymbol{\\Sigma}$?\n",
    "- Are the weights of the structured decoder in Part 3 interpretable?\n",
    "- Suppose we wanted to use the decoder to control a prosthetic limb and we needed it to work for months or years at a time. Would you expect the same decoder parameters to work throughout that time course, and if not, how might you adapt the decoder over time to maintain or potentially even improve performance?\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciDpg7PKuNX7"
   },
   "source": [
    "# Submission Instructions\n",
    "\n",
    "\n",
    "**Formatting:** check that your code does not exceed 80 characters in line width. You can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
    "\n",
    "Download your notebook in .ipynb format and use the following commands to convert it to PDF. \n",
    "\n",
    "**Option 1 (best case): ipynb &rarr; pdf** Run the following command to convert to a PDF:\n",
    "```\n",
    "jupyter nbconvert --to pdf lab6_teamname.ipynb\n",
    "```\n",
    "\n",
    "Unfortunately, `nbconvert` sometimes crashes with long notebooks. If that happens, here are a few options:\n",
    "\n",
    "\n",
    "**Option 2 (next best): ipynb &rarr; tex &rarr; pdf**:\n",
    "```\n",
    "jupyter nbconvert --to latex lab6_teamname.ipynb\n",
    "pdflatex lab6_teamname.tex\n",
    "```\n",
    "\n",
    "**Option 3: ipynb &rarr; html &rarr; pdf**:\n",
    "```\n",
    "jupyter nbconvert --to html lab6_teamname.ipynb\n",
    "# open lab6_teamname.html in browser and print to pdf\n",
    "```\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "- `nbconvert`: If you're using Anaconda for package management, \n",
    "```\n",
    "conda install -c anaconda nbconvert\n",
    "```\n",
    "- `pdflatex`: It comes with standard TeX distributions like TeXLive, MacTex, etc. Alternatively, you can upload the .tex and supporting files to Overleaf (free with Stanford address) and use it to compile to pdf.\n",
    "\n",
    "**Upload** your .ipynb and .pdf files to Gradescope. \n",
    "\n",
    "**Only one submission per team!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "STAT220/320 Lab 6: Decoding movement from motor cortex recordings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
