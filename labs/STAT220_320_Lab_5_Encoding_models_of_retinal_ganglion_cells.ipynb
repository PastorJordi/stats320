{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkInOPFnzoQK"
   },
   "source": [
    "# Lab 5: Encoding Models of Retinal Ganglion Cells\n",
    "\n",
    "**STATS320: Machine Learning Methods for Neural Data Analysis**\n",
    "\n",
    "_Stanford University. Winter, 2021._\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/slinderman/stats320/blob/main/labs/STAT220_320_Lab_5_Encoding_models_of_retinal_ganglion_cells.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Team Name:** _Your team name here_\n",
    "\n",
    "**Team Members:** _Names of everyone on your team here_\n",
    "\n",
    "*Due: 11:59pm Thursday, Feb 18, 2021 via GradeScope*\n",
    "\n",
    "---\n",
    "\n",
    "In this lab, you'll build generalized linear models (GLMs) and convolutional neural network (CNN) models of retinal ganglion cell (RGC) responses to visual stimuli. You'll use PyTorch to implement the models and fit them to a dataset  kindly provided by the [Baccus Lab](https://baccuslab.sites.stanford.edu/) (Stanford University), which they studied in the \"Deep Retina\" paper [(McIntosh et al, 2016)](https://arxiv.org/abs/1702.01825).\n",
    "\n",
    "### References\n",
    "McIntosh, Lane T., Niru Maheswaranathan, Aran Nayebi, Surya Ganguli, and Stephen A. Baccus. “Deep Learning Models of the Retinal Response to Natural Scenes.” Advances in Neural Information Processing (NeurIPS), 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GKupsmKxxvx"
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jn6yXI4rXhof"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import trange\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.distributions import Poisson\n",
    "\n",
    "# Specify that we want our tensors on the GPU and in float32\n",
    "device = torch.device('cuda')\n",
    "dtype = torch.float32\n",
    "\n",
    "# Helper function to convert between numpy arrays and tensors\n",
    "to_t = lambda array: torch.tensor(array, device=device, dtype=dtype)\n",
    "from_t = lambda tensor: tensor.to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "X0H23MgeD03y"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions for plotting (run this cell!)\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "# initialize a color palette for plotting\n",
    "palette = sns.xkcd_palette([\"windows blue\",\n",
    "                            \"red\",\n",
    "                            \"medium green\",\n",
    "                            \"dusty purple\",\n",
    "                            \"orange\",\n",
    "                            \"amber\",\n",
    "                            \"clay\",\n",
    "                            \"pink\",\n",
    "                            \"greyish\"])\n",
    "\n",
    "def plot_stimulus_weights(glm):\n",
    "    num_neurons = glm.num_neurons\n",
    "    max_delay = glm.max_delay\n",
    "\n",
    "    fig, axs = plt.subplots(num_neurons, 3, figsize=(8, 4 * num_neurons), \n",
    "                            gridspec_kw=dict(width_ratios=[1, 1.9, .1]))\n",
    "\n",
    "    temporal_weights = from_t(glm.temporal_conv.weight[:, 0])\n",
    "    bias = from_t(glm.temporal_conv.bias)\n",
    "    spatial_weights = from_t(glm.spatial_conv.weight)\n",
    "    spatial_weights = spatial_weights.reshape(num_neurons, 50, 50)\n",
    "\n",
    "    # normalize and flip the spatial weights\n",
    "    for n in range(num_neurons):\n",
    "        # Flip if spatial weight peak is negative\n",
    "        if np.allclose(spatial_weights[n].min(), \n",
    "                    -abs(spatial_weights[n]).max()):\n",
    "            spatial_weights[n] = -spatial_weights[n]\n",
    "            temporal_weights[n] = -temporal_weights[n]\n",
    "\n",
    "        # Normalize\n",
    "        scale = np.linalg.norm(spatial_weights[n])\n",
    "        spatial_weights[n] /= scale\n",
    "        temporal_weights[n] *= scale\n",
    "\n",
    "    # Set the same limits for each neuron\n",
    "    vlim = abs(spatial_weights).max()\n",
    "    ylim = abs(temporal_weights).max()\n",
    "    \n",
    "    for n in range(num_neurons):\n",
    "        axs[n, 0].plot(np.arange(-max_delay+1, 1) * 10, temporal_weights[n])\n",
    "        axs[n, 0].set_ylim(-ylim, ylim)\n",
    "        axs[n, 0].plot(np.arange(-max_delay+1, 1) * 10, np.zeros(max_delay), ':k')\n",
    "        if n < num_neurons - 1:\n",
    "            axs[n, 0].set_xticklabels([])\n",
    "        else:\n",
    "            axs[n, 0].set_xlabel(\"$\\Delta t$ [ms]\")\n",
    "\n",
    "        im = axs[n, 1].imshow(spatial_weights[n], \n",
    "                              vmin=-vlim, vmax=vlim, cmap=\"RdBu\")\n",
    "        axs[n, 1].set_axis_off()\n",
    "        axs[n, 1].set_title(\"neuron {}\".format(n + 1))\n",
    "        plt.colorbar(im, cax=axs[n, 2])\n",
    "\n",
    "\n",
    "def plot_coupling_weights(glm):\n",
    "    # Get the weights and flip them to get time after spike\n",
    "    W = from_t(glm.coupling_conv.weight)\n",
    "    W = W[:, :, ::-1]\n",
    "    wlim = abs(W).max()\n",
    "    dt = 10 * np.arange(W.shape[2])\n",
    "\n",
    "    fig, axs = plt.subplots(num_neurons, num_neurons, figsize=(12, 12), \n",
    "                            sharex=True, sharey=True)\n",
    "    for i in range(num_neurons):\n",
    "        for j in range(num_neurons):\n",
    "            axs[i, j].plot(dt, 0 * dt, ':k')\n",
    "            axs[i, j].plot(dt, W[i, j])\n",
    "            axs[i, j].set_ylim(-wlim, wlim)\n",
    "            axs[i, j].set_title(\"${} \\\\to {}$\".format(j, i))\n",
    "\n",
    "            if i == num_neurons - 1:\n",
    "                axs[i, j].set_xlabel(\"$\\Delta t$ [ms]\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_cnn_subunits_1(cnn):\n",
    "    num_subunits = cnn.num_subunits_1\n",
    "    max_delay = cnn.max_delay\n",
    "\n",
    "    fig, axs = plt.subplots(num_subunits, 3, figsize=(8, 4 * num_subunits), \n",
    "                            gridspec_kw=dict(width_ratios=[1, 1.9, .1]))\n",
    "\n",
    "    temporal_weights = from_t(cnn.temporal_conv.weight[:, 0])\n",
    "    bias = from_t(cnn.temporal_conv.bias)\n",
    "    spatial_weights = from_t(cnn.spatial_conv.weight)\n",
    "    spatial_weights = spatial_weights[:, 0, :, :]\n",
    "\n",
    "    # normalize and flip the spatial weights\n",
    "    for n in range(num_subunits):\n",
    "        # Flip if spatial weight peak is negative\n",
    "        if np.allclose(spatial_weights[n].min(), \n",
    "                    -abs(spatial_weights[n]).max()):\n",
    "            spatial_weights[n] = -spatial_weights[n]\n",
    "            temporal_weights[n] = -temporal_weights[n]\n",
    "\n",
    "        # Normalize\n",
    "        scale = np.linalg.norm(spatial_weights[n])\n",
    "        spatial_weights[n] /= scale\n",
    "        temporal_weights[n] *= scale\n",
    "\n",
    "    # Set the same limits for each neuron\n",
    "    vlim = abs(spatial_weights).max()\n",
    "    ylim = abs(temporal_weights).max()\n",
    "    \n",
    "    for n in range(num_subunits):\n",
    "        axs[n, 0].plot(np.arange(-max_delay+1, 1) * 10, temporal_weights[n])\n",
    "        axs[n, 0].set_ylim(-ylim, ylim)\n",
    "        axs[n, 0].plot(np.arange(-max_delay+1, 1) * 10, np.zeros(max_delay), ':k')\n",
    "        if n < num_subunits - 1:\n",
    "            axs[n, 0].set_xticklabels([])\n",
    "        else:\n",
    "            axs[n, 0].set_xlabel(\"$\\Delta t$ [ms]\")\n",
    "\n",
    "        im = axs[n, 1].imshow(spatial_weights[n], \n",
    "                              vmin=-vlim, vmax=vlim, cmap=\"RdBu\")\n",
    "        axs[n, 1].set_axis_off()\n",
    "        axs[n, 1].set_title(\"subunit 1,{}\".format(n + 1))\n",
    "        plt.colorbar(im, cax=axs[n, 2])\n",
    "\n",
    "def plot_cnn_subunits2(cnn):\n",
    "    cnn_filters_2 = from_t(cnn.layer2.weight)\n",
    "\n",
    "    fig, axs = plt.subplots(cnn.num_subunits_2, \n",
    "                            cnn.num_subunits_1,\n",
    "                            figsize=(4 * cnn.num_subunits_2,\n",
    "                                    4 * cnn.num_subunits_1),\n",
    "                            sharex=True, sharey=True)\n",
    "    vlim = abs(cnn_filters_2).max()\n",
    "    for i in range(cnn.num_subunits_2):\n",
    "        for j in range(cnn.num_subunits_1):\n",
    "            axs[i, j].imshow(cnn_filters_2[i, j], \n",
    "                            vmin=-vlim, vmax=vlim, cmap=\"RdBu\")\n",
    "            \n",
    "            axs[i, j].set_title('subunit 1,{} $\\\\to$ 2,{}'.format(j+1,i+1))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "z9HtrRrE4ox9"
   },
   "outputs": [],
   "source": [
    "#@title Implement `train_model` function (run this cell!)\n",
    "def train_model(model, \n",
    "                train_dataset, \n",
    "                val_dataset,\n",
    "                objective,\n",
    "                regularizer=None,\n",
    "                num_epochs=100, \n",
    "                lr=0.1,\n",
    "                momentum=0.9,\n",
    "                lr_step_size=25,\n",
    "                lr_gamma=0.9):\n",
    "    # progress bars\n",
    "    pbar = trange(num_epochs)\n",
    "    pbar.set_description(\"---\")\n",
    "    inner_pbar = trange(len(train_dataset))\n",
    "    inner_pbar.set_description(\"Batch\")\n",
    "\n",
    "    # data loaders for train and validation\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=1)\n",
    "    dataloaders = dict(train=train_dataloader, val=val_dataloader)\n",
    "\n",
    "    # use standard SGD with a decaying learning rate\n",
    "    optimizer = optim.SGD(model.parameters(), \n",
    "                          lr=lr, \n",
    "                          momentum=momentum)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, \n",
    "                                    step_size=lr_step_size, \n",
    "                                    gamma=lr_gamma)\n",
    "    \n",
    "    # Keep track of the best model\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_loss = 1e8\n",
    "\n",
    "    # Track the train and validation loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in pbar:\n",
    "        for phase in ['train', 'val']:\n",
    "            # set model to train/validation as appropriate\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                inner_pbar.reset()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            # track the running loss over batches\n",
    "            running_loss = 0\n",
    "            running_size = 0\n",
    "            for datapoint in dataloaders[phase]:\n",
    "                stim_t = datapoint['stimulus'].squeeze(0)\n",
    "                spikes_t = datapoint['spikes'].squeeze(0)\n",
    "                if phase == \"train\":\n",
    "                    with torch.set_grad_enabled(True):\n",
    "                        optimizer.zero_grad()\n",
    "                        # compute the model output and loss\n",
    "                        output_t = model(stim_t, spikes_t)\n",
    "                        loss_t = objective(output_t, spikes_t)\n",
    "                        # only add the regularizer in the training phase\n",
    "                        if regularizer is not None:\n",
    "                            loss_t += regularizer(model)\n",
    "\n",
    "                        # take the gradient and perform an sgd step\n",
    "                        loss_t.backward()\n",
    "                        optimizer.step()\n",
    "                    inner_pbar.update(1)\n",
    "                else:\n",
    "                    # just compute the loss in validation\n",
    "                    output_t = model(stim_t, spikes_t)\n",
    "                    loss_t = objective(output_t, spikes_t)\n",
    "\n",
    "                assert torch.isfinite(loss_t)\n",
    "                running_loss += loss_t.item()\n",
    "                running_size += 1\n",
    "            \n",
    "            # compute the train/validation loss and update the best\n",
    "            # model parameters if this is the lowest validation loss yet\n",
    "            running_loss /= running_size\n",
    "            if phase == \"train\":\n",
    "                train_losses.append(running_loss)\n",
    "            else:\n",
    "                val_losses.append(running_loss)\n",
    "                if running_loss < best_loss:\n",
    "                    best_loss = running_loss\n",
    "                    best_model_wts = deepcopy(model.state_dict())\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.set_description(\"Epoch {:03} Train {:.4f} Val {:.4f}\"\\\n",
    "                             .format(epoch, train_losses[-1], val_losses[-1]))\n",
    "        pbar.update(1)\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return np.array(train_losses), np.array(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdRq6bR4h5Sz"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "Load the data from the HDF5 file.\n",
    "- Each file contains a `train` and `test` group.\n",
    "- Each group contains:\n",
    "    - `time`: length `frames` array of timestamps\n",
    "    - `stimulus`: a `frames x 50 x 50` video taken at ~100Hz\n",
    "    - `response`: a group with\n",
    "        - `binned`: `cells x frames` array of spike counts (for the training data) or rates (for the test data) in each bin\n",
    "        - `firing_rate_xms` where `x` is 5, 10, or 20 milliseconds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vn8xa-Md9oG"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget -nc https://www.dropbox.com/s/gmgus2rm4sks6b8/whitenoise.h5\n",
    "# !wget -nc https://www.dropbox.com/s/gj8jf50v7dzy4ew/naturalscene.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BK5GG1_2eLsZ"
   },
   "outputs": [],
   "source": [
    "# Load the white noise data\n",
    "f = h5py.File(\"whitenoise.h5\", mode='r')\n",
    "frame_rate = 100\n",
    "times = f['train']['time'][:]\n",
    "stimulus = f['train']['stimulus'][:]\n",
    "spikes = f['train']['response']['binned'][:].T\n",
    "test_times = f['test']['time'][:]\n",
    "test_stimulus = f['test']['stimulus'][:] / 128\n",
    "test_rates = f['test']['response']['binned'][:, :-1].T\n",
    "\n",
    "# Get the size of the training data\n",
    "num_frames, height, width = stimulus.shape\n",
    "_, num_neurons = spikes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJo1E3q6KeR2"
   },
   "source": [
    "## Generic function to train a Pytorch model.\n",
    "\n",
    "We've slightly modified the `train_model` function from the previous lab. The changes are:\n",
    "- This function takes in both a `log_likelihood` and a `log_prior` function and minimizes the negative of their sum. The `log_prior` allows us to specify model-specific prior distributions, which we use to regularize the weights.\n",
    "- This function keeps track of the model with the best validation loss over the course of the training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h83mfn1O8dOy"
   },
   "source": [
    "# Part 1: Plot the data\n",
    "\n",
    "Always visualize your data first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcszNvfvQw3m"
   },
   "source": [
    "## Problem 1a: Plot a slice of the spike train\n",
    "\n",
    "Write a function to `imshow` a slice of the data.\n",
    "Add a colorbar and label your axes!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7KHHJrysLwI"
   },
   "outputs": [],
   "source": [
    "# Plot a few seconds of the spike train\n",
    "def plot_spike_train(spikes, t_start, t_stop, figsize=(12, 6)):\n",
    "    \"\"\"\n",
    "    `imshow` a window of the spike count matrix.\n",
    "\n",
    "    spikes:  time x neuron spike count matrix\n",
    "    t_start: time (in seconds) of the start of the window\n",
    "    t_stop:  time (in seconds) of the end of the window\n",
    "    figsize: width and height of the figure in inches\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    #\n",
    "    \n",
    "    #\n",
    "    ###\n",
    "\n",
    "plot_spike_train(spikes, 0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgEAmHRB0JfA"
   },
   "source": [
    "## Problem 1b: Compute the baseline firing rate for each neuron\n",
    "\n",
    "Print the mean firing rate for each neuron (on the training data) in spikes per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_N1KmLQ1Eaj"
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Compute the firing rates\n",
    "# YOUR CODE BELOW\n",
    "#\n",
    "print(\"Mean firing rates:\")\n",
    "for n in range(num_neurons):\n",
    "    print(\"  neuron {}: {:.4f} spk/sec\".format(...)\n",
    "#\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFdCd4yoQt1Y"
   },
   "source": [
    "## Plot a few frames of the stimulus\n",
    "\n",
    "Plot the 0th, 10th, 20th, and 30th frames of stimulus in grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGYNySW4sJPl"
   },
   "outputs": [],
   "source": [
    "# Plot a few frames of stimulus\n",
    "def plot_stimulus(stimulus, frame_inds, n_cols=4, panel_size=4):\n",
    "    num_frames = len(frame_inds)\n",
    "    n_rows = int(np.ceil(num_frames / n_cols))\n",
    "    fig, axs = plt.subplots(\n",
    "        n_rows, n_cols, figsize=(n_cols * panel_size, n_rows * panel_size))\n",
    "    for ax, ind in zip(axs.ravel(), frame_inds):\n",
    "        ax.imshow(stimulus[ind], cmap=\"Greys\")\n",
    "        ax.set_title(\"Stimulus Frame {}\".format(ind))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    for ax in axs.ravel()[len(frame_inds):]:\n",
    "        ax.set_visible(False)\n",
    "    \n",
    "plot_stimulus(stimulus, [0, 10, 20, 30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTjRhwb8Q83L"
   },
   "source": [
    "## Problem 1c: Compute and plot the spike triggered average\n",
    "\n",
    "The spike triggered average for neuron $n$ is the average stimulus in the lead-up to a spike by that neuron.\n",
    "\n",
    "Formally, let $A_n \\in \\mathbb{R}^{D \\times P_H \\times P_W}$ denote the STA for neuron $n$. It's defined as,\n",
    "\\begin{align}\n",
    "A_{n,d,i,j} = \\frac{1}{S_{n,d}} \\sum_{t=d+1}^T x_{t-d,i,j} \\mathbb{I}[y_{t,n} >0]\n",
    "\\end{align}\n",
    "where $S_{n,d} = \\sum_{t=d+1}^T \\mathbb{I}[y_{t,n} >0]$ is the number of spikes on neuron $n$, accounting for edge effects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fdmKcetfh8D"
   },
   "outputs": [],
   "source": [
    "def compute_sta(neuron, stimulus, spikes, max_delay=25):\n",
    "    \"\"\"\n",
    "    Compute the spike triggered average.\n",
    "\n",
    "    neuron: int index of the neuron \n",
    "    stimulus: (frames x height x width) array of stimulus\n",
    "    spikes: (frames x neurons) array of spike counts\n",
    "    max_delay: number of preceding frames (D) in the STA\n",
    "\n",
    "    returns: max_delay x height x width STA\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    #\n",
    "    sta = ...\n",
    "    #\n",
    "    ###\n",
    "    return sta\n",
    "\n",
    "def plot_sta(neuron, sta, n_cols=5):\n",
    "    max_delay = sta.shape[0]\n",
    "    n_rows = int(np.ceil(max_delay / n_cols))\n",
    "\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(4 * n_rows, 4 * n_cols))\n",
    "    vmin = sta.min()\n",
    "    vmax = sta.max()\n",
    "    for d, ax in enumerate(axs.ravel()):\n",
    "        ax.imshow(sta[d], vmin=vmin, vmax=vmax)\n",
    "        ax.set_axis_off()\n",
    "        ax.set_title(\"neuron {}, {}ms pre\".format(neuron + 1, d*10))\n",
    "    for ax in axs.ravel()[max_delay:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "n = 0\n",
    "sta = compute_sta(n, stimulus, spikes)\n",
    "plot_sta(n, sta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZVrpQLkDy1c"
   },
   "source": [
    "## Finally, create PyTorch Datasets containing the stimuli and the spikes.\n",
    "Before moving onto the modeling sections, we'll split the training stimulus and spikes into batches of length 1000 frames (10 seconds of data). Then we'll randomly assign 20% of the batches to a validation dataset. We've written a simple dataset to get the training and validation batches. For stability, we normalize the stimulus to be binary rather than 0 or 128, as in the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwXE8V_b2mjN"
   },
   "outputs": [],
   "source": [
    "  class RGCDataset(Dataset):\n",
    "    def __init__(self, stimulus, spikes):\n",
    "        self.stimulus = stimulus\n",
    "        self.spikes = spikes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.stimulus.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Binarize the stimulus, move it and the spikes to the GPU,\n",
    "        # and package into a dictionary\n",
    "        return dict(stimulus=to_t(self.stimulus[idx]) / 128.0, \n",
    "                    spikes=to_t(self.spikes[idx]))\n",
    "\n",
    "def make_datasets(batch_size=1000):\n",
    "    n_batches = num_frames // batch_size\n",
    "    batched_stimulus = stimulus[:n_batches * batch_size]\n",
    "    batched_stimulus = batched_stimulus.reshape(n_batches, batch_size, height, width)\n",
    "    batched_spikes = spikes[:n_batches * batch_size]\n",
    "    batched_spikes = batched_spikes.reshape(n_batches, batch_size, num_neurons)\n",
    "\n",
    "    # Split into train and validation\n",
    "    train_stimulus, val_stimulus, train_spikes, val_spikes = \\\n",
    "        train_test_split(batched_stimulus, \n",
    "                        batched_spikes, \n",
    "                        train_size=0.8, \n",
    "                        random_state=0)\n",
    "\n",
    "\n",
    "    train_dataset = RGCDataset(train_stimulus, train_spikes)\n",
    "    val_dataset = RGCDataset(val_stimulus, val_spikes)\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "train_dataset, val_dataset = make_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npfHppegnItd"
   },
   "source": [
    "# Part 2: Fit a linear-nonlinear Poisson (LNP) model\n",
    "\n",
    "<center>\n",
    "<img src=https://github.com/slinderman/stats320/raw/main/assets/lnp.png width=500>\n",
    "</center>\n",
    "<small><it>Image credit: Jonathan Pillow, Cosyne Tutorial 2018.</it></small>\n",
    "\n",
    "Let's start with a simple linear-nonlinear-Poisson (LNP) model. In statistics, we would just call this a generalized linear model (GLM), but here we'll stick to the neuroscience lingo to be consistent with McIntosh et al. (2016). LNP models (and GLMs more generally) are natural models for count data, like spike counts. Whereas standard linear models could ouput negative means, these models are constrained to output non-negative expected spike counts. Moreover, since they use a Poisson noise model, the variance of the spike counts will grow with the mean, unlike in typical linear regression models.\n",
    "\n",
    "The basic LNP model is,\n",
    "\\begin{align}\n",
    "\\mathbb{E}[y_{tn} \\mid X] \n",
    "&= f \\left(\\sum_{d=1}^D \\sum_{i=1}^{P_H} \\sum_{j=1}^{P_W} x_{t-d,i,j} w_{n,d,i,j} \\right) \\\\\n",
    "&= f \\left( \\sum_{d=1}^D v_{n,d} \\left(\\sum_{i=1}^{P_H} \\sum_{j=1}^{P_W} x_{t-d,i,j} u_{n,i,j} \\right) \\right) \\\\\n",
    "&= f \\left( \\sum_{d=1}^D v_{n,d} \\tilde{x}_{n,t-d} \\right) \\\\\n",
    "&= f \\left( [\\tilde{x}_n \\star v_n]_t \\right)\n",
    "\\end{align}\n",
    "where $w_{n,d,i,j} = v_{n,d} u_{n,i,j}$ is the weight neuron $n$ gives to the simulus at pixel $i,j$ at $d$ frames preceding the current time. The variable $\\tilde{x}_{n} \\in \\mathbb{R}^T$ denotes the stimulus projected onto the spatial filter for neuron $n$, and $\\left( \\tilde{x}_n \\star v_n \\right)\\in \\mathbb{R}^T$ is the cross-correlation between the projected stimulus and the temporal filter. The mean function $f: \\mathbb{R} \\to \\mathbb{R}_+$ maps the output of the linear filter to a non-negative expected spike count.\n",
    "\n",
    "Once we compute $\\mathbb{E}[y_{tn} \\mid X]$ we compute the likelihood function based on a Poisson regression model:\n",
    "\\begin{align}\n",
    "\\log p(y_{tn} \\mid X; W_n) &= \\log \\mathrm{Po}(y_{tn} \\mid f \\left( [\\tilde{x}_n \\star v_n]_t \\right))\n",
    "\\end{align}\n",
    "\n",
    "Summing across samples in $t$ leads to the full likelihood for estimating the parameters for a given neuron. We can do this simultaneously across all neurons by summing over $n$ too, as gradient descent will independently update each neuron's parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "galqOTzc2D72"
   },
   "source": [
    "## Problem 2a: Implement the model\n",
    "\n",
    "Let's start by implementing the GLM model as a class that inherits from `nn.Module`. The  `forward` method returns the mean spike count for each time bin\n",
    "given the stimulus. In the loss function below (Problem 2b), we'll pass this output to the mean of a Poisson distribution.\n",
    "\n",
    "**Notes:**\n",
    "- As in Lab 2, you should first project the stimulus onto the spatial filters with a linear layer, then you can convolve with the temporal filters.\n",
    "- Even though the spatial projection is a linear layer, we'll call it `spatial_conv` since its a factor of a spatiotemporal convolution. This naming will also be consistent with our models below.\n",
    "- Both `spatial_conv` and `temporal_conv` include a learnable bias, by default. We only need one, so turn off the bias in the spatial layer.\n",
    "- `mean_function` specifies the mapping from the linear predictor to the expected spike count. We'll use an exponential function to be consistent with the lecture, but `F.softplus` is more common in practice. (It tends to be a little more stable during training.\n",
    "- We set the initial bias to a value that is roughly the log of the average spike count so that our initial means are in the right ballpark.\n",
    "- We'll add a small positive constant to the firing rate in the `forward` function to ensure that we don't get `log(0)` errors during training.\n",
    "- `forward` takes a keyword argument `spikes`. We won't use it in this model, but we need it here so that our training algorithm will work for this model as well as the later ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jpvF4v_0Iam"
   },
   "outputs": [],
   "source": [
    "class LNP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_neurons=num_neurons,\n",
    "                 height=height,\n",
    "                 width=width,\n",
    "                 max_delay=40,\n",
    "                 mean_function=torch.exp,\n",
    "                 initial_bias=0.05):\n",
    "        super(LNP, self).__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.max_delay = max_delay\n",
    "        self.mean_function = mean_function\n",
    "\n",
    "        ###\n",
    "        # YOUR CODE BELOW\n",
    "        #\n",
    "        self.spatial_conv = nn.Linear(...)\n",
    "        self.temporal_conv = nn.Conv1d(...)\n",
    "        #\n",
    "        ###\n",
    "\n",
    "        # Initialize the bias\n",
    "        torch.nn.init.constant_(self.temporal_conv.bias, np.log(initial_bias))\n",
    "\n",
    "    def forward(self, stimulus, spikes=None):\n",
    "        \"\"\"\n",
    "        stimulus: num_frames x height x width\n",
    "        spikes: num_frames x num_neurons (unused by this model)\n",
    "        \n",
    "        returns: num_frames x num_neurons tensor of expected spike counts\n",
    "        \"\"\"\n",
    "        ###\n",
    "        # YOUR CODE BELOW\n",
    "        #\n",
    "        x = stimulus\n",
    "        ...\n",
    "        #\n",
    "        ###\n",
    "        return 1e-4 + x\n",
    "\n",
    "\n",
    "def check_model_outputs(model):\n",
    "    out_t = model(train_dataset[0]['stimulus'],\n",
    "                  train_dataset[0]['spikes'])\n",
    "    assert out_t.shape == train_dataset[0]['spikes'].shape\n",
    "    assert torch.all(out_t > 0)\n",
    "\n",
    "# Construct an LNP model with random initial weights.\n",
    "# Fix the seed so that the tests below will work\n",
    "torch.manual_seed(0)\n",
    "lnp = LNP().to(device)\n",
    "check_model_outputs(lnp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8kdQJmogGI_"
   },
   "source": [
    "## Problem 2b: Implement the Poisson loss\n",
    "Compute the average negative log likelihood of the spikes (taking the mean over neurons and frames) given the expected spike counts (`rates_t`) ouput by the model. \n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(W) = -\\frac{1}{NT} \\sum_{n=1}^N \\sum_{t=1}^T \\log \\mathrm{Po}(y_{nt} \\mid \\lambda_{ntt})\n",
    "\\end{align}\n",
    "where $\\lambda_{nt} = f([X \\star W_n]_t)$ denotes the rates output by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwzhpHNQI73j"
   },
   "outputs": [],
   "source": [
    "def poisson_loss(rate_t, spikes_t):\n",
    "    \"\"\"Compute the average negative log-likelihood under a \n",
    "    Poisson spiking model.\n",
    "\n",
    "    rate_t:  T x N array of expected spike counts\n",
    "    spikes_t: T x N array of integer spikes\n",
    "    returns: average negative log likelihood (mean over all spikes)\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    avg_nll = ...\n",
    "    ###\n",
    "    return avg_nll\n",
    "    \n",
    "assert torch.isclose(\n",
    "    poisson_loss(lnp(train_dataset[0]['stimulus']), \n",
    "                 train_dataset[0]['spikes']),\n",
    "    torch.tensor(0.1319), atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFip8dPMQVXk"
   },
   "source": [
    "## Problem 2c: Important simple $\\ell_2$ weight regularization\n",
    "\n",
    "To the Poisson loss above, we'll add a regularization penalty on the squared $\\ell_2$ norm of the weights,\n",
    "\\begin{align}\n",
    "\\mathcal{R}(W) &= \\frac{\\alpha}{2} \\sum_{n=1}^N (\\|U_n\\|_F^2 + \\|V_n\\|_F^2)\n",
    "\\end{align}\n",
    "where $U_n$ and $V_n$ are the spatial and temporal weights, respectively, and $\\alpha$ is a scaling factor.\n",
    "\n",
    "Do not regularize the biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdP8kBeXBcY3"
   },
   "outputs": [],
   "source": [
    "def lnp_regularizer(model, alpha=1e-3):\n",
    "    \"\"\"\n",
    "    Implement an \\ell_2 penalty on the norm of the model weights,\n",
    "    as described above.\n",
    "\n",
    "    model: LNP instance\n",
    "    alpha: scaling parameter for the \\ell_2 penalty.\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    reg = ...\n",
    "    #\n",
    "    ###\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZvRQVcQhoIY"
   },
   "source": [
    "## Fit the LNP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bb0YF6BnhrWy"
   },
   "outputs": [],
   "source": [
    "# Construct an LNP model with random initial weights.\n",
    "torch.manual_seed(0)\n",
    "lnp = LNP().to(device)\n",
    "\n",
    "# Fit the LNP model\n",
    "print(\"Training LNP model. This should take about 3-4 minutes...\")\n",
    "train_losses, val_losses = \\\n",
    "    train_model(lnp, \n",
    "                train_dataset, \n",
    "                val_dataset, \n",
    "                poisson_loss,\n",
    "                lnp_regularizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjPchkKjEAXk"
   },
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OvCUpMQ9jQC"
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation curves\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].plot(train_losses, color=palette[0], label=\"train\")\n",
    "axs[0].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
    "axs[0].set_xlabel(\"epoch\")\n",
    "axs[0].set_ylabel(\"poisson loss\")\n",
    "axs[0].grid(True)\n",
    "axs[0].legend(loc=\"upper right\")\n",
    "\n",
    "axs[1].plot(train_losses, color=palette[0], label=\"train\")\n",
    "axs[1].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
    "axs[1].set_xlabel(\"epoch\")\n",
    "axs[1].set_ylabel(\"poisson loss\")\n",
    "axs[1].set_ylim(top=val_losses[20])\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03g-_aoE06tj"
   },
   "outputs": [],
   "source": [
    "plot_stimulus_weights(lnp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xABSxqSLWNI8"
   },
   "source": [
    "## Problem 2d: [Short Answer] Interpret the results\n",
    "\n",
    "Describe the outputs of the LNP model. Are they what you expected? How do the spatiotemporal filters relate to the STA from Problem 1c? Are they mathematically related?\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2uyT_J3up4u"
   },
   "source": [
    "# Part 3: Fit a GLM with inter-neuron couplings\n",
    "\n",
    "<center>\n",
    "<img src=https://github.com/slinderman/stats320/raw/main/assets/glm.png width=300>\n",
    "</center>\n",
    "<small><it>Image credit: Jonathan Pillow, Cosyne Tutorial 2018.</it></small>\n",
    "\n",
    "Now add inter-neuron couplings to the basic model above. For historical reasons, the LNP with inter-neuron couplings is what some neuroscientists call a GLM, even though they're both instances of generalized linear models! Again, we're just going to stick to the notation of McIntosh et al (2016) for this lab anyway.  \n",
    "\n",
    "The new model is,\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[y_{nt}] \n",
    "&= f \\left( \\tilde{x}_n \\star v_n +  \\sum_{m=1}^N \\sum_{d=1}^D y_{m,t-d} g_{m,n,d} \\right)\n",
    "\\end{align}\n",
    "where $G \\in \\mathbb{R}^{N \\times N \\times D}$ is a weight matrix of self-couplings.\n",
    "\n",
    "Recall from lecture that you can implement the couplings as a convolution of $Y$ and $G$. \n",
    "\n",
    "**Note:** as above, the `coupling_conv` will have a bias by default. Get rid of it. You don't need it since there's already a bias in the `temporal_conv`.\n",
    "\n",
    "**IMPORTANT:** Make sure your output only depends on spike counts up to but _not including_ time $t$!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xL9nCdv3yuTx"
   },
   "source": [
    "## Problem 3a: Implement the coupled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "McoqFbRkp4s2"
   },
   "outputs": [],
   "source": [
    "class GLM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_neurons=num_neurons,\n",
    "                 height=height,\n",
    "                 width=width,\n",
    "                 max_delay=40,\n",
    "                 initial_bias=0.05,\n",
    "                 mean_function=torch.exp):\n",
    "        super(GLM, self).__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.max_delay = max_delay\n",
    "        self.mean_function = mean_function\n",
    "\n",
    "        ###\n",
    "        # YOUR CODE BELOW\n",
    "        #\n",
    "        self.spatial_conv = nn.Linear(...)\n",
    "        self.temporal_conv = nn.Conv1d(...)\n",
    "        self.coupling_conv = nn.Conv1d(...)\n",
    "        #\n",
    "        ###\n",
    "\n",
    "        # Initialize the bias\n",
    "        torch.nn.init.constant_(self.temporal_conv.bias, np.log(initial_bias))\n",
    "\n",
    "    def forward(self, stimulus, spikes):\n",
    "        \"\"\"\n",
    "        stimulus: num_frames x height x width\n",
    "        spikes: num_frames x num_neurons (unused by this model)\n",
    "        \n",
    "        returns: num_frames x num_neurons tensor of expected spike counts\n",
    "        \"\"\"\n",
    "        ###\n",
    "        # YOUR CODE BELOW\n",
    "        #\n",
    "        x, y = stimulus, spikes\n",
    "        ...\n",
    "        #\n",
    "        ###\n",
    "        return 1e-4 + x\n",
    "\n",
    "# Construct a coupled GLM model with random initial weights.\n",
    "torch.manual_seed(0)\n",
    "glm = GLM(num_neurons=9).to(device)\n",
    "check_model_outputs(glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLjycnQuy8Nj"
   },
   "source": [
    "## Problem 3b: Implement a regularizer for the coupled GLM weights\n",
    "\n",
    "Put an $\\ell_2$ penalty on the weights of the `spatial_conv`, `temporal_conv`, and `coupling_conv`. No need to regularize the bias. Scale the regularization by $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_bmk5eqsvCF"
   },
   "outputs": [],
   "source": [
    "def glm_regularizer(model, alpha=1e-3):\n",
    "    \"\"\"\n",
    "    Implement an \\ell_2 penalty on the norm of the model weights,\n",
    "    as described above.\n",
    "\n",
    "    model: GLM instance\n",
    "    alpha: scaling parameter for the \\ell_2 penalty.\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    reg = ...\n",
    "    #\n",
    "    ###\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYywYbtitPQZ"
   },
   "source": [
    "## Fit the GLM model with couplings between neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbC-Y1BhtOkb"
   },
   "outputs": [],
   "source": [
    "# Construct a coupled GLM model with random initial weights.\n",
    "torch.manual_seed(0)\n",
    "glm = GLM().to(device)\n",
    "\n",
    "# Fit the model\n",
    "print(\"Training coupled GLM. This should take about 4 minutes...\")\n",
    "train_losses, val_losses = \\\n",
    "    train_model(glm, \n",
    "                train_dataset, \n",
    "                val_dataset, \n",
    "                poisson_loss,\n",
    "                glm_regularizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOV3xBHAgkHU"
   },
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7RtWo53CMi9"
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation curves\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].plot(train_losses, color=palette[0], label=\"train\")\n",
    "axs[0].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
    "axs[0].set_xlabel(\"epoch\")\n",
    "axs[0].set_ylabel(\"poisson loss\")\n",
    "axs[0].grid(True)\n",
    "axs[0].legend(loc=\"upper right\")\n",
    "\n",
    "axs[1].plot(train_losses, color=palette[0], label=\"train\")\n",
    "axs[1].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
    "axs[1].set_xlabel(\"epoch\")\n",
    "axs[1].set_ylabel(\"poisson loss\")\n",
    "axs[1].set_ylim(top=val_losses[20])\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEviXKwBep5m"
   },
   "outputs": [],
   "source": [
    "plot_stimulus_weights(glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jG-oC8mmevo_"
   },
   "outputs": [],
   "source": [
    "plot_coupling_weights(glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjk8WqT5W5gB"
   },
   "source": [
    "## Problem 3c: [Short Answer] Interpret the results\n",
    "\n",
    "Did adding the coupling weights change the spatiotemporal stimulus filters in any perceptible way? Do you see any interesting structure in the coupling weights? What other regularization strategies could you have applied to the coupling weights?\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSpKiGqFv8U6"
   },
   "source": [
    "# Part 4: Convolutional neural network model\n",
    "\n",
    "<center>\n",
    "<img src=https://github.com/slinderman/stats320/raw/main/assets/cnn.png width=500>\n",
    "</center>\n",
    "<small><it>Image credit: McIntosh et al (NeurIPS, 2016).</it></small>\n",
    "\n",
    "Finally, we'll implement a convolutional neural network like the one proposed in McIntosh et al (2016). (See above.) We'll make some slight modifications though, so that the model doesn't take so long to fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faWvElPLYqgD"
   },
   "source": [
    "## Problem 4a: Implement the convolutional model\n",
    "\n",
    "Implement the following model:\n",
    "1. **Apply a rank-1 spatiotemporal filter to the video:**\n",
    "  \n",
    "  a. First convolve with 2D receptive fields of size `rf_size_1` and `num_subunits_1` output channels. You do not need to pad the edges since the neurons respond primarily to the center of the video. Your output should be `T x N1 x H1 x W1` where `T` is the number of frames, `N1` is the number of subunits, and `H1,W1` are the height and width after 2D convolution without padding.\n",
    "  \n",
    "  b. Then convolve each subunit and pixel with a temporal filter, to get another `T x N1 x H1 x W1` output.\n",
    "\n",
    "  c. Apply a rectifying nonlinearity (`F.relu`).\n",
    "\n",
    "2. **Spatial convolution and mixing**\n",
    "\n",
    "  a. Apply a spatial convolution of size `rf_size_2` with `num_subunits_2` output channels. This layer mixes the subunits from the first layer to obtain a representation that is, hopefully, somewhat similar to that of intermediate cells in the retina. \n",
    "  \n",
    "  b. Apply another rectifying nonlinearity (`F.relu`). The output should be `T x N2 x H2 x W2` where `N2` is the number of subunits in the second layer and `H2,W2` are the size of the image after convolution without padding.\n",
    "\n",
    "3. **Predict expected spike counts**\n",
    "\n",
    "  a. Apply a linear read-out to the `N2 x H2 x W2` representation and pass through the mean function to obtain a `T x N` tensor of expected spike counts, where `N` is the number of neurons.\n",
    "\n",
    "\n",
    "**Notes:** The modifications we made are\n",
    "- We used slightly larger receptive field sizes. This actually speeds things up since, with valid padding, we end up with fewer \"pixels\" in subsequent layers.\n",
    "- We used a smaller number of subunits (4/4 as opposed to 8/16). This is a smaller dataset (only 9 neurons) and we seemed to overfit with more layers. \n",
    "- We use an exponential mean function to be consistent with the models above. Again, a softplus is more common in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-DUYEogxe-8"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_neurons=num_neurons,\n",
    "                 height=height,\n",
    "                 width=width,\n",
    "                 rf_size_1=21,\n",
    "                 rf_size_2=15,\n",
    "                 max_delay=40,\n",
    "                 num_subunits_1=4,\n",
    "                 num_subunits_2=4,\n",
    "                 initial_bias=0.05,\n",
    "                 mean_function=torch.exp):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.max_delay = max_delay\n",
    "        self.rf_size_1 = rf_size_1\n",
    "        self.num_subunits_1 = num_subunits_1\n",
    "        self.rf_size_2 = rf_size_2\n",
    "        self.num_subunits_2 = num_subunits_2\n",
    "        self.mean_function = mean_function\n",
    "        \n",
    "        ###\n",
    "        # YOUR CODE BELOW\n",
    "        #\n",
    "        self.spatial_conv = nn.Conv2d(...)\n",
    "        self.temporal_conv = nn.Conv1d(...)\n",
    "        \n",
    "        # Spatial convolution over the subunits in layer 1\n",
    "        self.layer2 = nn.Conv2d(...)\n",
    "        \n",
    "        # Fully connected layer at the end\n",
    "        self.layer3 = nn.Linear(...)\n",
    "        #\n",
    "        ###\n",
    "\n",
    "        # Initialize the bias\n",
    "        torch.nn.init.constant_(self.layer3.bias, np.log(initial_bias))\n",
    "        \n",
    "    def forward(self, stimulus, spikes=None):\n",
    "        \"\"\"\n",
    "        stimulus: num_frames x height x width\n",
    "        spikes: num_frames x num_neurons (unused by this model)\n",
    "        \n",
    "        returns: num_frames x num_neurons tensor of expected spike counts\n",
    "        \"\"\"\n",
    "        ###\n",
    "        # YOUR CODE BELOW\n",
    "        #\n",
    "        x = stimulus\n",
    "        ...\n",
    "        #\n",
    "        ###\n",
    "        return 1e-4 + x\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "cnn = CNN().to(device)\n",
    "check_model_outputs(cnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKOZi-lZLREY"
   },
   "source": [
    "## Problem 4b: Regularize the weights\n",
    "\n",
    "Put an $\\ell_2$ penalty on the weights of `spatial_conv`, `temporal_conv`, `layer2`, and `layer3`. Scale the regularize by $\\alpha$, as in the preceding sections. No need to regularize the biases. We found that a smaller value of $\\alpha$ was helpful, so here we default to `1e-5`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLSd4T5rHKyL"
   },
   "outputs": [],
   "source": [
    "# Regularize the weights of the CNN\n",
    "def cnn_regularizer(model, alpha=1e-5):\n",
    "    \"\"\"\n",
    "    Implement an \\ell_2 penalty on the norm of the model weights,\n",
    "    as described above.\n",
    "\n",
    "    model: CNN instance\n",
    "    alpha: scaling parameter for the \\ell_2 penalty.\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # YOUR CODE BELOW\n",
    "    reg = ...\n",
    "    #\n",
    "    ###\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNJj40mULrFa"
   },
   "source": [
    "## Fit the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mg9epbov_2jQ"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "cnn = CNN().to(device)\n",
    "\n",
    "print(\"Fitting the CNN model. This should take about 12 minutes.\")\n",
    "train_losses, val_losses = \\\n",
    "    train_model(cnn, \n",
    "                train_dataset, \n",
    "                val_dataset,\n",
    "                poisson_loss,\n",
    "                cnn_regularizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBBKfnSyENd8"
   },
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxIFl9tGSVz7"
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation curves\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].plot(train_losses, color=palette[0], label=\"train\")\n",
    "axs[0].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
    "axs[0].set_xlabel(\"epoch\")\n",
    "axs[0].set_ylabel(\"poisson loss\")\n",
    "axs[0].grid(True)\n",
    "axs[0].legend(loc=\"upper right\")\n",
    "\n",
    "axs[1].plot(train_losses, color=palette[0], label=\"train\")\n",
    "axs[1].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
    "axs[1].set_xlabel(\"epoch\")\n",
    "axs[1].set_ylabel(\"poisson loss\")\n",
    "axs[1].set_ylim(top=val_losses[10])\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Euh6Qpao6qe2"
   },
   "source": [
    "## Plot the subunit weights for the CNN\n",
    "\n",
    "First we'll plot the spatiotemporal filters of the first layer of subunits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drsWoCfJ4_yV"
   },
   "outputs": [],
   "source": [
    "plot_cnn_subunits_1(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPqMs_u5MkYM"
   },
   "source": [
    "## Plot the spatial weights for the second layer of subunits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7G8BFgI4vIE"
   },
   "outputs": [],
   "source": [
    "plot_cnn_subunits2(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N6nOINb8E9w"
   },
   "source": [
    "## Problem 4c: Predict test firing rates\n",
    "\n",
    "Finally, take the fitted models from Parts 2-4 and evaluate them on test data. \n",
    "\n",
    "The test data consists of _expected_ spike counts rather than spike counts. That's because they showed the same visual stimulus many times and computed the average response.  If our models are working well, they should output a similar firing rate in response to that same visual stimulus. \n",
    "\n",
    "Compute the predicted rates given the test stimulus using the LNP, GLM, and the CNN. Then plot them for a slice of the test data alongside the true test rates\n",
    "\n",
    "**Note:** technically the coupled GLM from Part 3 expects preceding spikes as input, but here we'll give it the rates as input instead. (We don't have test spikes to feed in.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgfQzkXs8VK3"
   },
   "outputs": [],
   "source": [
    "# Move the test stimulus and measured rates to the GPU\n",
    "test_stimulus_t = to_t(test_stimulus)\n",
    "test_rates_t = to_t(test_rates)\n",
    "\n",
    "###\n",
    "# YOUR CODE BELOW\n",
    "#\n",
    "lnp_test_rates = ...\n",
    "glm_test_rates = ...\n",
    "cnn_test_rates = ...\n",
    "\n",
    "# Plot a slice of the true and predicted firing rates\n",
    "slc = slice(250, 500)\n",
    "fig, axs = plt.subplots(num_neurons, 1, figsize=(8, 16), sharex=True)\n",
    "for n in range(num_neurons):\n",
    "    ...\n",
    "#\n",
    "###\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQEng0FgNBzG"
   },
   "source": [
    "## Problem 4d: Model comparison\n",
    "\n",
    "Make a bar plot of the mean squared error between the true and predicted rates for each model. As a baseline, compute the mean squared error of a constant-rate model with rate equal to the expected spike count under the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBDTFUTm_HlN"
   },
   "outputs": [],
   "source": [
    "###\n",
    "# YOUR CODE BELOW\n",
    "#\n",
    "mse_const = ...\n",
    "mse_lnp = ...\n",
    "mse_glm = ...\n",
    "mse_cnn = ...\n",
    "#\n",
    "###\n",
    "\n",
    "# Make a bar plot\n",
    "plt.bar(0, mse_const, color='gray', ec='k')\n",
    "plt.bar(1, mse_lnp, color=palette[0], ec='k')\n",
    "plt.bar(2, mse_glm, color=palette[1], ec='k')\n",
    "plt.bar(3, mse_cnn, color=palette[2], ec='k')\n",
    "plt.xticks([0, 1, 2, 3], [\"Const.\", \"LNP\", \"GLM\", \"CNN\"])\n",
    "plt.ylabel(\"Test MSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsadGfnrCO5o"
   },
   "source": [
    "# Part 5: Discussion\n",
    "\n",
    "You've now developed and fit three encoding models for these retinal ganglion cell responses, and hopefully you've developed some intuition for how these models work!\n",
    "\n",
    "Let's end by discussing some of the decisions that go into building and checking these models. Write a few paragraphs summarizing your key takeaways. Here are a few suggestions of things to consider:\n",
    "\n",
    "1. All three models were fit with a Poisson loss. What other loss functions could we use? What is overdispersion of count data? Is this an issue here?\n",
    "\n",
    "2. The CNN was loosely motivated as an approximation to the layers of photoreceptors, bipolar cells, etc. that precede retinal ganglion cells. Of course, the actual circuitry is more complicated. What could you imagine adding to this model to make it more realistic?\n",
    "\n",
    "3. In our hands, the CNN outperformed the LNP and GLM. Though it's tempting to just say the CNN is a more flexible model, notice that the CNN does not have coupling filters and it compresses the input substantially before the final read-out layer. Given the results above, what follow-up experiments would you do to further understand the root of these performance differences?\n",
    "\n",
    "4. We didn't ask you to do a thorough hyperparameter search. If you were to do one, what are the key parameters you would vary to try to improve model performance?\n",
    "\n",
    "5. We fit all of these models to RGC responses to a binary white noise stimulus. Would you expect your results to change if the cells had been shown a movie with natural scenes instead?\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjisiMLhQxaN"
   },
   "source": [
    "# Submission Instructions\n",
    "\n",
    "\n",
    "**Formatting:** check that your code does not exceed 80 characters in line width. You can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
    "\n",
    "Download your notebook in .ipynb format and use the following commands to convert it to PDF. \n",
    "\n",
    "**Option 1 (best case): ipynb &rarr; pdf** Run the following command to convert to a PDF:\n",
    "```\n",
    "jupyter nbconvert --to pdf lab5_teamname.ipynb\n",
    "```\n",
    "\n",
    "Unfortunately, `nbconvert` sometimes crashes with long notebooks. If that happens, here are a few options:\n",
    "\n",
    "\n",
    "**Option 2 (next best): ipynb &rarr; tex &rarr; pdf**:\n",
    "```\n",
    "jupyter nbconvert --to latex lab5_teamname.ipynb\n",
    "pdflatex lab5_teamname.tex\n",
    "```\n",
    "\n",
    "**Option 3: ipynb &rarr; html &rarr; pdf**:\n",
    "```\n",
    "jupyter nbconvert --to html lab5_teamname.ipynb\n",
    "# open lab5_teamname.html in browser and print to pdf\n",
    "```\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "- `nbconvert`: If you're using Anaconda for package management, \n",
    "```\n",
    "conda install -c anaconda nbconvert\n",
    "```\n",
    "- `pdflatex`: It comes with standard TeX distributions like TeXLive, MacTex, etc. Alternatively, you can upload the .tex and supporting files to Overleaf (free with Stanford address) and use it to compile to pdf.\n",
    "\n",
    "**Upload** your .ipynb and .pdf files to Gradescope. \n",
    "\n",
    "**Only one submission per team!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "STAT220/320 Lab 5: Encoding models of retinal ganglion cells.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
