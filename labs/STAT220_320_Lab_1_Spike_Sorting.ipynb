{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhZTZAxw1_2_"
   },
   "source": [
    "# Lab 1: Spike Sorting\n",
    "\n",
    "**STATS320: Machine Learning Methods for Neural Data Analysis**\n",
    "\n",
    "_Stanford University. Winter, 2021._\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/slinderman/stats320/blob/main/labs/STAT220_320_Lab_1_Spike_Sorting.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Team Name:** _Your team name here_\n",
    "\n",
    "**Team Members:** _Names of everyone on your team here_\n",
    "\n",
    "*Due: 11:59pm Thursday, Jan 21, 2021 via GradeScope (see below)*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In this lab you'll build a basic spike sorting algorithm for electrophysiological recordings like those you might collect with a Neuropixels probe. The model is inspired by [Kilosort](https://github.com/MouseLand/Kilosort), a popular spike sorting algorithm. We'll fit the model to synthetic data simulated with \"eMouse,\" a Kilosort utility for testing spike sorting algorithms on realistic data with ground truth spike times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7jMey-AZShJ"
   },
   "source": [
    "## Instructions\n",
    "Make a copy of this notebook and share it amongst yourselves. Fill in your names and your chosen team name above! You won't be graded on your team name, but you might be judged. (Just kidding!)\n",
    "\n",
    "Complete and run the code cells below to preprocess the data, implement the spike sorting model described in class, and produce some plots.\n",
    "\n",
    "The problems in Parts 1-3 ask you to fill in a few lines of code. Some problems are one-liners, which we denote by\n",
    "```\n",
    "result = ...     # YOUR CODE HERE\n",
    "```\n",
    "It's ok if you split your answer into more than one line. Other problems will almost surely need a few lines of code, which we denote by\n",
    "```\n",
    "###\n",
    "# This block should do a thing.\n",
    "# YOUR CODE BELOW\n",
    "#\n",
    "\n",
    "result = ...\n",
    "#\n",
    "###\n",
    "```\n",
    "We have a few asserts in the code so that you can check you're getting the expected result. Don't change the output variable names or the subsequent code won't run!\n",
    "\n",
    "Part 4 asks you to discuss the results. Respond to these questions in text cells.\n",
    "\n",
    "### Submission\n",
    "At the end, you'll submit both a PDF and a .ipynb file via GradeScope. It's due at **11:59pm Thursday, Jan 21.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dyv2VRJq2kGU"
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PWc6KZ91vya"
   },
   "outputs": [],
   "source": [
    "# First, import necessary libraries.\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from scipy.io import loadmat \n",
    "import scipy.signal as signal\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm.auto import trange\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrWkNIrX2Cx8"
   },
   "source": [
    "## Download the synthetic data\n",
    "This data was simulated with [eMouse](https://github.com/MouseLand/Kilosort/tree/main/eMouse_drift), a Kilosort utility for generating semi-synthetic data.  It's only semi-synthetic because the spike waveforms are taken from real recordings with manually curated spikes.  The spike timing and noise are all drawn from known distributions though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9wJuTuQ2slC"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget -nc https://www.dropbox.com/s/tq0auevkswaa09y/sim_binary.imec.ap.bin\n",
    "!wget -nc https://www.dropbox.com/s/j0xcqk7v9n0fzc7/eMouseGroundTruth.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAWjwoNx23Cd"
   },
   "outputs": [],
   "source": [
    "# Load the data. It's stored a binary file of int16's\n",
    "sample_freq = 30000             # sampling frequency\n",
    "spike_width = 81                # width of a spike, in samples\n",
    "plot_slice = slice(0, 6000)     # 200ms window of frames to plot\n",
    "num_channels = 64               # number of channels on the probe\n",
    "\n",
    "raw_data = np.fromfile(\"sim_binary.imec.ap.bin\", dtype=np.int16).reshape(-1, num_channels)\n",
    "raw_data = raw_data.T           # Transpose to be consisten with notes\n",
    "_, num_samples = raw_data.shape\n",
    "timestamps = np.arange(num_samples) / sample_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-SacjXDyIC6",
    "outputId": "3d11a5cf-be3b-42a8-a63c-b2c4d452b8f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 3600000)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "vUPzL_Go3J8G"
   },
   "outputs": [],
   "source": [
    "#@title Define some helper functions for plotting (run this cell)\n",
    "def plot_data(timestamps,\n",
    "              data, \n",
    "              plot_slice=plot_slice, \n",
    "              true_labels=None, \n",
    "              true_times=None,\n",
    "              unit_channels=None,\n",
    "              spike_width=spike_width,\n",
    "              scale=10,\n",
    "              figsize=(12, 9),\n",
    "              cmap=\"jet\"):\n",
    "    n_channels, n_samples = data.shape\n",
    "    cmap = get_cmap(cmap) if isinstance(cmap, str) else cmap\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(timestamps[plot_slice], \n",
    "             data.T[plot_slice] - scale * np.arange(n_channels), \n",
    "             '-k', lw=1)\n",
    "    \n",
    "    if not any(x is None for x in [true_labels, true_times, unit_channels]):\n",
    "        # Plot the ground truth spikes and assignments\n",
    "        n_units = true_labels.max()\n",
    "        in_slice = (true_times >= plot_slice.start) & (true_times < plot_slice.stop)\n",
    "        labels = true_labels[in_slice]\n",
    "        times = true_times[in_slice]\n",
    "        for i in range(n_units):\n",
    "            i_channels = unit_channels[i]\n",
    "            for t in times[labels == i + 1]:\n",
    "                window = slice(t - spike_width // 2, t + spike_width // 2)\n",
    "                plt.plot(timestamps[window], \n",
    "                         data.T[window, i_channels] - scale * np.arange(n_channels)[i_channels],\n",
    "                         color=cmap(i / (n_units-1)),\n",
    "                         alpha=0.5,\n",
    "                         lw=2)\n",
    "                \n",
    "    plt.yticks(-scale * np.arange(1, n_channels+1, step=2), \n",
    "            np.arange(1, n_channels+1, step=2) + 1)\n",
    "    plt.xlabel(\"time [s]\")\n",
    "    plt.ylabel(\"channel\")\n",
    "    plt.xlim(timestamps[plot_slice.start], timestamps[plot_slice.stop])\n",
    "    plt.ylim(-scale * n_channels, scale)\n",
    "\n",
    "\n",
    "def plot_templates(templates, \n",
    "                   indices,\n",
    "                   scale=0.1,\n",
    "                   n_cols=8,\n",
    "                   panel_height=6,\n",
    "                   panel_width=1.25,\n",
    "                   colors=('k',),\n",
    "                   label=\"neuron\"):\n",
    "    n_subplots = len(indices)\n",
    "    n_cols = min(n_cols, n_subplots)\n",
    "    n_rows = int(np.ceil(n_subplots / n_cols))\n",
    "\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, \n",
    "                            figsize=(panel_width * n_cols, panel_height * n_rows),\n",
    "                            sharex=True, sharey=True)\n",
    "    \n",
    "    n_units, n_channels, spike_width = templates.shape\n",
    "    timestamps = np.arange(-spike_width // 2, spike_width//2) / sample_freq\n",
    "    for i, (ind, ax) in enumerate(zip(indices, np.ravel(axs))):\n",
    "        color = colors[i % len(colors)]\n",
    "        ax.plot(timestamps * 1000, \n",
    "                templates[ind].T - scale * np.arange(n_channels), \n",
    "                '-', color=color, lw=1)\n",
    "        \n",
    "        ax.set_title(\"{} {:d}\".format(label, ind + 1))\n",
    "        ax.set_xlim(timestamps[0] * 1000, timestamps[-1] * 1000)\n",
    "        ax.set_yticks(-scale * np.arange(n_channels+1, step=4))\n",
    "        ax.set_yticklabels(np.arange(n_channels+1, step=4) + 1)\n",
    "        ax.set_ylim(-scale * n_channels, scale)\n",
    "\n",
    "        if i // n_cols == n_rows - 1:\n",
    "            ax.set_xlabel(\"time [ms]\")\n",
    "        if i % n_cols == 0:\n",
    "            ax.set_ylabel(\"channel\")\n",
    "\n",
    "        # plt.tight_layout(pad=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M91Qlwkg2HL5"
   },
   "source": [
    "## Plot the raw data\n",
    "It's a 64-dimensional time series of voltage measurements for each channel in units of microvolts ($\\mu$V). Here we plot a 200ms window of data. The `scale` parameter specifies the spacing between channels in the same units as the data.  For this synthetic data, the noise is on the order of 10-20$\\mu$V, and the spikes are on the order of 100$\\mu$V. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpCFGIKdPPnI"
   },
   "outputs": [],
   "source": [
    "plot_data(timestamps, raw_data, scale=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3TkWR3cE_Vk"
   },
   "source": [
    "# Part 1: Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lof-Hw_m4-ho"
   },
   "source": [
    "## Problem 1a: Bandpass filtering\n",
    "First we will bandpass filter each channel from 300Hz to 2kHz to isolate spiking content. We'll use a 10th order Butterworth filter.\n",
    "\n",
    "Use `signal.butter` and `signal.sosfilt` to do this (note that `signal` is an alias for `scipy.signal`; see above). Try calling `help(signal.butter)` or Googling it for more information on the function signature and outputs. The `sample_freq` is specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3SF1JvmE-Uh"
   },
   "outputs": [],
   "source": [
    "# Construct a Butterworth bandpass filter.\n",
    "sos = signal.butter(..., output='sos')  # YOUR CODE HERE\n",
    "\n",
    "# This function evaluates the filter response\n",
    "# at a grid of input frequencies from 0 to the Nyquist frequency\n",
    "# (1/2 the sampling frequency). The response is given\n",
    "# as a complex number for each input frequency, where the square \n",
    "# of the magnitude is the power at that frequency.\n",
    "freqs, response = signal.sosfreqz(sos, fs=sample_freq)\n",
    "\n",
    "# convert the response to decibels and truncate lower end.\n",
    "# (see, e.g., https://en.wikipedia.org/wiki/Decibel)\n",
    "response_db = 20 * np.log10(np.maximum(np.abs(response), 1e-5))\n",
    "\n",
    "# Plot the response.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(freqs, response_db, lw=2)\n",
    "plt.vlines([300, 2000], *plt.ylim(), colors='r', ls=':')\n",
    "plt.ylim(-40, 5)\n",
    "plt.grid(True)\n",
    "plt.yticks([0, -20, -40, -60])\n",
    "plt.ylabel('Gain [dB]')\n",
    "plt.title('Butterworth filter frequency response')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TT15FSAFEYA"
   },
   "source": [
    "The next cell applies the filter to the raw data using `signal.sosfilt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rk9bj_AoE3-K"
   },
   "outputs": [],
   "source": [
    "filtered_data = signal.sosfilt(sos, raw_data, axis=1)\n",
    "\n",
    "# Plot the result. It should be much cleaner than the raw data.\n",
    "plot_data(timestamps, filtered_data, scale=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KgZkJXIIwnO"
   },
   "source": [
    "## Problem 1b: Decorrelate and standardize the signal\n",
    "\n",
    "Bandpass filtering effectively centered the data for us (think about why that is), but there are still instantaneous noise correlations between the channels that will invalidate our modeling assumptions below.  We'll try to counteract that here.\n",
    "\n",
    "Let $x \\in \\mathbb{R}^C$ denote a random variable denoting a vector of voltage measurements across the $C$ channels. Let $\\mathrm{Cov}(x) = \\Sigma$ denote the (symmetric) covariance matrix of $x$, and define $\\tilde{x} = \\Sigma^{-1/2} x$. Then $\\mathrm{Cov}(\\tilde{x}) = \\Sigma^{-1/2} \\Sigma (\\Sigma^{-1/2})^\\top = I$. Multiplying by the inverse square root of the covariance yields a random variable with uncorrelated channels and unit variance along each channel.\n",
    "\n",
    "We'll use an empirical estimate of the covariance matrix instead. Let $\\hat{\\Sigma} = \\frac{1}{T} \\sum_{t=1}^T x_t x_t^\\top$ denote the empirical covariance matrix, where $x_t$ is the vector of voltage measurements at time $t$ across all channels. $\\hat{\\Sigma} = U S V^T$ denote its signal value decomposition.  Since the covariance matrix is symmetric, $U = V$.  The inverse square root is given by,\n",
    "\\begin{align}\n",
    "\\hat{\\Sigma}^{-1/2} = V S^{-1/2} V^T.\n",
    "\\end{align}\n",
    "\n",
    "We will call the transformed data $\\tilde{x}_t = \\hat{\\Sigma}^{-1/2} x_t$ the `data` for short, since it's what we'll be working with from here on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpJvO68zJFUa"
   },
   "outputs": [],
   "source": [
    "cov = ...   # YOUR CODE HERE\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cov)\n",
    "plt.xlabel(\"channels\")\n",
    "plt.ylabel(\"channels\")\n",
    "plt.colorbar(label=\"covariance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7numqb5hK5mh"
   },
   "outputs": [],
   "source": [
    "# Use the SVD to construct the transformation matrix\n",
    "U, S, VT = np.linalg.svd(cov)\n",
    "inverse_sqrt_cov = ...  # YOUR CODE HERE\n",
    "\n",
    "# Multiply the filtered data by the inverse square root of the covariance\n",
    "data = inverse_sqrt_cov @ filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2xgKT-oMBZc"
   },
   "outputs": [],
   "source": [
    "# Plot the standardized data. \n",
    "# Note that the scale is an order of magnitude smaller!\n",
    "plot_data(timestamps, data, scale=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsttRqnqzHfv"
   },
   "outputs": [],
   "source": [
    "# Cleanup: remove the filtered data since we no longer need it.\n",
    "del filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AT6QxKjdYs4E"
   },
   "source": [
    "# Part 2: Find putative spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waOJVPrwM3nw"
   },
   "source": [
    "## Problem 2a: Find negative peaks in the data\n",
    "\n",
    "We're looking for peaks in the negative voltage for each channel. The peaks should stand out from the noise---a reasonable threshold is 4 standard deviations---and they should be well separted---a reasonable distance is 5ms.  Use the `signal.find_peaks` function to extract negative peaks for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2fHBfZCZJvY"
   },
   "outputs": [],
   "source": [
    "# Use scipy.signal.find_peaks to find candidate spike times.\n",
    "distance_sec = 0.005    # time between spikes (in seconds)\n",
    "height = 4              # standard deviations to define a spike\n",
    "per_ch_spike_inds = []  # list of arrays of spike indices for each channel\n",
    "\n",
    "for ch in trange(num_channels):\n",
    "    # Find the peaks in the negative voltage data for channel `ch`.\n",
    "    ch_spike_inds, _ = signal.find_peaks(...)   # YOUR CODE HERE\n",
    "\n",
    "    per_ch_spike_inds.append(ch_spike_inds)\n",
    "\n",
    "plot_data(timestamps, data)\n",
    "for ch, ch_spike_inds in enumerate(per_ch_spike_inds):\n",
    "    plt.plot(timestamps[ch_spike_inds], data[ch, ch_spike_inds] - 10 * ch, 'r.',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vzmqhn4Z_2MC"
   },
   "source": [
    "## Problem 2b: Combine spike times that are nearly coincident across channels\n",
    "\n",
    "We'll do this in a few steps:\n",
    "\n",
    "1. Count the number of spikes in each time bin with `np.histogram` applied to the concatenated `per_channel_spike_inds`.\n",
    "2. Smooth the `total_spike_counts` with `gaussian_filter_1d` using a filter width of 0.1ms.\n",
    "3. Plot the smoothed histogram and eyeball a lower bound to declare a spike.\n",
    "4. Use `find_peaks` again, this time on the smoothed total spike counts and with a distance threshold of 1ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OE5phfYc7wN"
   },
   "outputs": [],
   "source": [
    "# Count total number of spikes across all channels in each sample.\n",
    "bins = np.arange(num_samples + 1)\n",
    "total_spike_counts, _ = np.histogram(...)   # YOUR CODE HERE\n",
    "\n",
    "# Do a little Gaussian smoothing to allow for jitter in spike time across channels\n",
    "total_spike_counts = gaussian_filter1d(total_spike_counts.astype(float), 0.0001 * sample_freq)\n",
    "\n",
    "# Eyeball a threshold and find peaks.\n",
    "# RUN WITH NP.INF FIRST AND THEN\n",
    "# EYEBALL THE PLOT TO SET A HEIGHT THRESHOLD\n",
    "min_height = np.inf \n",
    "\n",
    "spike_inds, _ = signal.find_peaks(total_spike_counts, \n",
    "                                  height=min_height, \n",
    "                                  distance=0.001 * sample_freq)\n",
    "num_spikes = len(spike_inds)\n",
    "print(\"Found\", num_spikes, \"putative spikes.\")\n",
    "\n",
    "# Plot the smoothed spike counts and extracted spike indices.\n",
    "plt.figure(figsize=(12, 3))\n",
    "in_slice = (spike_inds >= plot_slice.start) & (spike_inds < plot_slice.stop) \n",
    "plt.plot(timestamps[plot_slice], total_spike_counts[plot_slice])\n",
    "plt.plot(timestamps[spike_inds[in_slice]], total_spike_counts[spike_inds[in_slice]], 'ro')\n",
    "plt.xlim([timestamps[plot_slice.start], timestamps[plot_slice.stop]])\n",
    "plt.xlabel(\"time [ms]\")\n",
    "plt.ylabel(\"smoothed spike rate and extracted peaks\")\n",
    "\n",
    "# Plot the data and overlay the spike times to sanity check your results.\n",
    "plot_data(timestamps, data, scale=10)\n",
    "plt.vlines(timestamps[spike_inds[in_slice]], *plt.ylim(), color='r', lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQnazYfj3iEJ"
   },
   "outputs": [],
   "source": [
    "assert num_spikes == 32115, \"Play with the threshold to ensure you get this number of spikes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bXXyQVfHQhC"
   },
   "source": [
    "## Collect the spikes into an array\n",
    "\n",
    "For each of the `spike_inds` found above, extract a window of `spike_width` time samples from `data`, centered on the spike and insert it into the `spikes` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oeYkgn1qrrkk"
   },
   "outputs": [],
   "source": [
    "spikes = np.zeros((num_spikes, num_channels, spike_width))\n",
    "for i, ind in enumerate(spike_inds):\n",
    "    spikes[i] = ... # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXx1GZIdif1B"
   },
   "outputs": [],
   "source": [
    "# We can use our template plotting code to visualize the spikes too.\n",
    "plot_templates(spikes, indices=np.arange(16), scale=10, label=\"spike\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCivGuA3iZES"
   },
   "source": [
    "# Part 3: Write a simple spike sorting algorithm\n",
    "\n",
    "Recall our simple spike sorting model from lecture.\n",
    "Let\n",
    "- $S$ denote the number of (putative) spikes\n",
    "- $C$ denote the number of channels on the probe.\n",
    "- $D$ denote the maximum duration (in frames) of a spike.\n",
    "- $N$ denote the (unknown) number of units (i.e. neurons).\n",
    "- $K \\leq \\min\\{C, D\\}$ denote the dimensionality (i.e. rank) of a single unit's spike waveform.\n",
    "\n",
    "Our preprocessing steps above yielded an array of spike waveforms, which we denote by $Y \\in \\mathbb{R}^{S \\times C \\times D}$.\n",
    "\n",
    "Our latent variables are,\n",
    "- $n_s \\in \\{1, \\ldots, N\\}$, the neuron label assigned to spike $s$. \n",
    "- $a_s \\in \\mathbb{R}_+$, the amplitude of spike $s$. \n",
    "\n",
    "And our parameters are,\n",
    "- $W \\in \\mathbb{R}^{N \\times C \\times D}$, the normalized waveform of spikes by neuron $n$.\n",
    "- $\\pi \\in \\Delta_N$, a prior distribution on neurons.\n",
    "\n",
    "For simplicity, we will treat the following as hyperparameters. Let,\n",
    "- $\\lambda_n \\in \\mathbb{R}_+$, the rate (inverse scale) of spike amplitudes on neuron $n$.\n",
    "- $\\sigma^2 \\in \\mathbb{R}_+$, the noise level, which we assume to be constant across channels. \n",
    "\n",
    "We will set $\\lambda_n = 0.01$ to minimally shrink the amplitude estimates, and we set $\\sigma^2 = 1$ based on the standardization we did in preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "it7vNqYIJv_h"
   },
   "source": [
    "## Problem 3a: write a helper function to project onto $\\mathcal{S}_K$.\n",
    "\n",
    "One of the key pieces of our algorithm is solving the following constrained optimization over $\\mathcal{S}_K$,\n",
    "\\begin{align}\n",
    "X^\\star &= \\mathrm{argmin}_{X \\in \\mathcal{S}_K} \\mathrm{Tr}(A^\\top X)\n",
    "\\end{align}\n",
    "for a target matrix $A \\in \\mathbb{R}^{C \\times D}$. This is equivalent to projecting $A$ onto the set $\\mathcal{S}_K$. In lecture we showed that we can solve this problem with the SVD $A = U \\mathrm{diag}(\\varsigma) V$. Then, \n",
    "\\begin{align}\n",
    "X^\\star &= U_{:K} \\mathrm{diag} \\left(\\tfrac{\\varsigma_{:K}}{\\|\\varsigma_{:K}\\|_2} \\right) V_{:K}^\\top,\n",
    "\\end{align}\n",
    "where $U_{:K}$ and $V_{:K}$ are the first $K$ columns of $U$ and $V$, and where $\\varsigma_{:K}$ are the first $K$ singular values in $\\varsigma$.\n",
    "\n",
    "Write a helper function to compute this projection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oW8ftBbqXSlh"
   },
   "outputs": [],
   "source": [
    "def project_templates(targets, rank):\n",
    "    \"\"\"A helper function to project a target template\n",
    "    onto the space of rank-K, unit norm matrices via\n",
    "    truncted SVD.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    targets:    an NxCxD array of target matrices\n",
    "    rank:       an integer K <= min(C, D)\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    templates:  a NxCxD array where templates[n] is the projection\n",
    "                of targets[n] onto \\mathcal{S}_K, the set of rank-K,\n",
    "                unit norm matrices.\n",
    "    \"\"\"\n",
    "    assert rank <= min(*targets.shape[-2:])\n",
    "    \n",
    "    ###\n",
    "    # Compute the optimal templates using the SVD of the targets.\n",
    "    #\n",
    "    # YOUR CODE BELOW\n",
    "    U, Sigma, VT = np.linalg.svd(targets, full_matrices=False)\n",
    "    ...\n",
    "    templates = ...\n",
    "    #\n",
    "    ###\n",
    "    \n",
    "    assert np.allclose(np.sum(templates**2, axis=(-1,-2)), 1)\n",
    "    return templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjKHYnGRkZ4J"
   },
   "source": [
    "## Initialize the model parameters\n",
    "\n",
    "Initialize the model parameters as follows:\n",
    "- Fix `num_neurons` $N=100$ as an upper bound on the number of neurons\n",
    "- Fix `template_rank` $K=3$ as the rank of the templates\n",
    "- Set `amplitude_rates` $\\lambda_n = 0.01$ for a broad prior on amplitudes $a_s$\n",
    "- Set `noise_std` $\\sigma = 1$, since we have standardized the data.\n",
    "\n",
    "- Initialize `templates` $W$ by randomly choosing $N$ spike waveforms from $Y$ and projecting them onto $\\mathcal{S}_K$ using the function you write in problem 3a.\n",
    "- Set `neuron_probs` $\\pi$ to the uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xshp0mtmiYOo"
   },
   "outputs": [],
   "source": [
    "# We'll give our variables human-readable names\n",
    "\n",
    "# Set the hyperparameters\n",
    "num_neurons = 100                                               # N\n",
    "template_rank = 3                                               # K\n",
    "amplitude_rates = 0.01 * np.ones(num_neurons)                   # \\lambda_n\n",
    "noise_std = 1.0                                                 # \\sigma\n",
    "\n",
    "# Initialize colors to plot each neuron\n",
    "npr.seed(0)\n",
    "colors = get_cmap(\"jet\")(npr.rand(num_neurons))\n",
    "\n",
    "# Initialize the model parameters\n",
    "#\n",
    "# Set uniform neuron probabilities (1/N, ..., 1/N)\n",
    "neuron_probs = np.ones(num_neurons) / num_neurons               # \\pi\n",
    "\n",
    "# Initialize the templates W by projecting randomly \n",
    "# chosen spike indices.\n",
    "# Make sure the templates are low rank and normalized!\n",
    "inds = npr.choice(num_spikes, size=num_neurons, replace=False)\n",
    "templates = project_templates(spikes[inds], template_rank)      # W\n",
    "\n",
    "# Initialize the latent variables.\n",
    "# (These will be overwritten below, but it's helpful to \n",
    "#  specification here.)\n",
    "labels = npr.choice(num_neurons, size=num_spikes)               # n_s\n",
    "amplitudes = np.ones(num_spikes)                                # a_s\n",
    "\n",
    "# Pack them into dictionaries\n",
    "hypers = dict(\n",
    "    num_neurons=num_neurons,\n",
    "    template_rank=template_rank,\n",
    "    amplitude_rates=amplitude_rates,\n",
    "    noise_std=noise_std\n",
    ")\n",
    "\n",
    "params = dict(\n",
    "    templates=templates,\n",
    "    neuron_probs=neuron_probs,\n",
    ")\n",
    "\n",
    "latents = dict(\n",
    "    labels=labels,\n",
    "    amplitudes=amplitudes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2ylmXI1RiQh"
   },
   "outputs": [],
   "source": [
    "# Plot the initial templates\n",
    "plot_templates(params[\"templates\"], np.arange(16), scale=0.1, colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7LmeyhqOSA3"
   },
   "source": [
    "## Problem 3b: Write a function to compute the log probability\n",
    "\n",
    "For the likelihood, we modeled the observed spike waveforms as randomly scaled versions of a neuron's normalized waveform plus additive noise.\n",
    "\\begin{align}\n",
    "    p(Y \\mid W, \\{n_s, a_s\\}_{s=1}^S, \\sigma^2) \n",
    "    &= \\prod_{s=1}^S \\prod_{c=1}^C \\prod_{d=1}^D \\mathcal{N} \\left(y_{scd} \\mid a_s \\cdot w_{n_s cd}, \\, \\sigma^2 \\right).\n",
    "\\end{align}\n",
    "\n",
    "Taking the log and expanding the Gaussian density,\n",
    "\\begin{align}\n",
    "    \\log p(Y \\mid W, \\{n_s, a_s\\}_{s=1}^S, \\sigma^2) \n",
    "    &= \n",
    "    \\sum_{s=1}^S \\sum_{c=1}^C \\sum_{d=1}^D \n",
    "    \\left[-\\frac{1}{2} \\log 2 \\pi \\sigma^2 -\\frac{1}{2\\sigma^2} (y_{scd} - a_s \\cdot w_{n_s cd})^2 \\right] \\\\\n",
    "    &= \n",
    "    -\\frac{SCD}{2} \\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2} \\sum_{s=1}^S \\|Y_s - a_s W_{n_s}\\|_F^2 \\\\\n",
    "    &= \n",
    "    -\\frac{SCD}{2} \\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2} \\sum_{n=1}^N \\sum_{s:n_s = n} \\|Y_s - a_s W_{n}\\|_F^2 \\\\\n",
    "    &= \n",
    "    -\\frac{SCD}{2} \\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2} \\sum_{n=1}^N \\sum_{s:n_s = n} \\sum_{c=1}^C \\sum_{d=1}^D (y_{scd} - a_s w_{ncd})^2\n",
    "\\end{align}\n",
    "\n",
    "We completed the model with priors on the latent variables and parameters. We assumed,\n",
    "\\begin{align}\n",
    "n_s &\\sim \\pi \\\\\n",
    "a_s \\mid n_s &\\sim \\mathrm{Exp}(\\lambda_{n_s})\n",
    "\\end{align}\n",
    "where $\\pi \\in \\Delta_N$ is a distribution on neurons and $\\lambda_n$ is the rate parameter (i.e. the inverse scale) of an exponential distribution on amplitudes. Thus,\n",
    "\\begin{align}\n",
    "\\log p(\\{n_s, a_s\\}_{s=1}^S) = \\sum_{s=1}^S \\log \\pi_{n_s} + \\log \\lambda_{n_s} - \\lambda_{n_s} a_s.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "We assumed uniform priors on $\\pi$ and $W_n$, so they will not factor into our log probability calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgtyPWLxOWW1"
   },
   "outputs": [],
   "source": [
    "def log_probability(spikes, latents, params, hypers):\n",
    "    \"\"\"Evaluate the average log joint probability of the spikes, \n",
    "    latents, and params.\n",
    "    \"\"\"\n",
    "    S, C, D = spikes.shape\n",
    "    N = hypers[\"num_neurons\"]\n",
    "    sigma = hypers[\"noise_std\"]\n",
    "    lmbdas = hypers[\"amplitude_rates\"]\n",
    "\n",
    "    ns = latents[\"labels\"]\n",
    "    amps = latents[\"amplitudes\"]\n",
    "    W = params[\"templates\"]\n",
    "    pi = params[\"neuron_probs\"]\n",
    "    \n",
    "    ### \n",
    "    # Compute the log joint probability.\n",
    "    # First compute the log likelihood p(Y | \\{n_s, a_s\\}, W, \\sigma).\n",
    "    # Then compute the log prior p(\\{n_s, a_s\\}).\n",
    "    # \n",
    "    # YOUR CODE BELOW\n",
    "    #\n",
    "    log_lkhd = ...\n",
    "\n",
    "    # compute the log prior probability \n",
    "    # (consider using stats.expon.logpdf)\n",
    "    log_prior = ...\n",
    "\n",
    "    #\n",
    "    ###\n",
    "\n",
    "    # The log joint is the log likelihood plus the log prior\n",
    "    log_joint = log_lkhd + log_prior\n",
    "    \n",
    "    # return the average log probability \n",
    "    return log_joint / spikes.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82Eg7TnURY_k"
   },
   "outputs": [],
   "source": [
    "# Try it! \n",
    "lp = log_probability(spikes, latents, params, hypers)\n",
    "assert np.allclose(lp, -1.5218722, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uTmqeq8msVq"
   },
   "source": [
    "## Problem 3c: Write a function to update for the latent variables\n",
    "\n",
    "In lecture we derived a coordinate ascent algorithm for MAP estimation of the latent variables and parameters. We showed that for fixed parameters, the optimal spike assignment ($n_s$) and amplitude $a_s$ are,\n",
    "\\begin{align}\n",
    "n_s &= \\mathrm{argmax} \\mathcal{L}(n) \\\\\n",
    "a_s &= a_s^\\star(n_s) \n",
    "\\end{align}\n",
    "where\n",
    "\\begin{align}\n",
    "    a_s^\\star(n) &= \\max\\left\\{0, \\, \\mathrm{Tr}(Y_s^\\top W_{n}) - \\sigma^2 \\lambda_{n} \\right\\} \\\\\n",
    "    \\mathcal{L}(n) &=\n",
    "    \\frac{1}{2 \\sigma^2}\\left( a_s^\\star(n) \\right)^2  + \\log \\pi_{n}.\n",
    "\\end{align}\n",
    "Let's implement this update. Remember that $\\mathrm{Tr}(Y_s^\\top W_n) = \\sum_{cd} Y_{scd} W_{ncd}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTnJagPumpCY"
   },
   "outputs": [],
   "source": [
    "def update_latent_variables(spikes, params, hypers):\n",
    "    \"\"\"Solve for the optimal labels and amplitudes \\{n_s, a_s\\}_{s=1}^S\n",
    "    given the data, the current parameters, and the hyperparameters.\n",
    "\n",
    "    Returns: a dictionary with keys\n",
    "        \"labels\"        a 1d array of spike assignments n_s \n",
    "        \"amplitudes\"    a 1d array of non-negative amplitudes a_s\n",
    "    \"\"\"\n",
    "    Y = spikes\n",
    "    S, C, D = Y.shape\n",
    "    lmbda = hypers[\"amplitude_rates\"]\n",
    "    sigma = hypers[\"noise_std\"]\n",
    "    W = params[\"templates\"]\n",
    "    pi = params[\"neuron_probs\"]\n",
    "\n",
    "    # compute the optimal amplitudes for each spike and assignment\n",
    "    # the output should be a S by N array\n",
    "    amplitudes = np.einsum(..., Y, W)   # YOUR CODE HERE\n",
    "\n",
    "    # threshold the amplitudes\n",
    "    amplitudes = np.maximum(...)    # YOUR CODE HERE\n",
    "\n",
    "    # compute the score \\mathcal{L}(n) for each spike and neuron\n",
    "    # the output should be a S by N array\n",
    "    lp = ...    # YOUR CODE HERE\n",
    "    \n",
    "    # find the most likely neuron labels\n",
    "    labels = np.argmax(lp, axis=1)\n",
    "\n",
    "    # get the corresponding amplitude\n",
    "    amplitudes = amplitudes[np.arange(S), labels]\n",
    "    \n",
    "    # pack the results into a dictionary and return\n",
    "    return dict(labels=labels, amplitudes=amplitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTwcJC13uh1r"
   },
   "outputs": [],
   "source": [
    "# Test it! Run one update of the latent variables \n",
    "new_latents = update_latent_variables(spikes, params, hypers)\n",
    "\n",
    "# Plot a histogram of neuron labels\n",
    "plt.hist(new_latents[\"labels\"], np.arange(num_neurons), ec='k')\n",
    "plt.xlabel(\"neuron\")\n",
    "plt.ylabel(\"num. assigned spikes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAmC22Rww6MJ"
   },
   "source": [
    "## Problem 3d: Write a function to update the templates\n",
    "\n",
    "In lecture we showed that the optimal waveforms are the truncated SVD of the average spike waveform,\n",
    "\\begin{align}\n",
    "    \\bar{Y}_n &\\propto \\sum_{s:n_s = n} a_s Y_s,\n",
    "\\end{align} \n",
    "which we call the `targets` in the code below.\n",
    "\n",
    "Compute this matrix for each neuron $n$ using the current assignments, then use your `project_templates` function to find\n",
    "\\begin{align}\n",
    "    W_n^\\star &= \\mathrm{argmax}_{W_n \\in \\mathcal{S}_K} \\mathrm{Tr}(\\bar{Y}_n^\\top W_n).\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvS-C85Bw4XW"
   },
   "outputs": [],
   "source": [
    "def update_templates(spikes, latents, params, hypers):\n",
    "    # Unpack the parameters\n",
    "    S, C, D = spikes.shape\n",
    "    N = hypers[\"num_neurons\"]\n",
    "    K = hypers[\"template_rank\"]\n",
    "    ns = latents[\"labels\"]\n",
    "    amps = latents[\"amplitudes\"]\n",
    "\n",
    "    # compute the target for each neuron\n",
    "    targets = np.zeros((N, C, D))\n",
    "    for n in range(N):\n",
    "        targets[n] = ...    # YOUR CODE HERE (np.einsum might be helpful again)\n",
    "\n",
    "    # solve the constrained optimization problem\n",
    "    templates = project_templates(targets, K)\n",
    "\n",
    "    # Update the parameters\n",
    "    new_params = deepcopy(params)\n",
    "    new_params[\"templates\"] = templates\n",
    "    return new_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akXYFxaPz8ha"
   },
   "outputs": [],
   "source": [
    "# Try it! Since we set the neuron assignments randomly, \n",
    "# the templates should all look like the average spike waveform.\n",
    "new_params = update_templates(spikes, latents, params, hypers)\n",
    "plot_templates(new_params[\"templates\"], np.arange(16), colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDR-rW075929"
   },
   "source": [
    "## Problem 3e: Write a function to update the neuron probabilities $\\pi$\n",
    "\n",
    "In lecture you worked out that,\n",
    "\\begin{align}\n",
    "\\pi_n &= \\frac{1}{S} \\sum_{s=1}^S \\mathbb{I}[n_s = n].\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOj8rokG59Do"
   },
   "outputs": [],
   "source": [
    "def update_neuron_probs(spikes, latents, params, hypers):\n",
    "    \"\"\"Update the neuron assignment probabilites $\\pi$, holding\n",
    "    the latent variables fixed.\n",
    "    \"\"\"\n",
    "    num_spikes = spikes.shape[0]\n",
    "    labels = latents[\"labels\"]\n",
    "\n",
    "    # Find the optimal neuron probabilities\n",
    "    pi = ...    # YOUR CODE HERE (try np.bincount)\n",
    "    \n",
    "    # Return a new copy of the parameters\n",
    "    new_params = deepcopy(params)\n",
    "    new_params[\"neuron_probs\"] = pi\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIIj4bOyHstE"
   },
   "source": [
    "## Optional: Update the amplitude rates $\\lambda_n$\n",
    "See the lecture slides for a final form. If you write a function fo this step, call it in `update_parameters` below. In practice, you should get fine results even with $\\lambda_n$ fixed to 0.01, as it is currently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7S7Ewz660Iz"
   },
   "source": [
    "## Put it all together\n",
    "\n",
    "Now we can write a function to perform MAP estimation.\n",
    "\n",
    "**No coding necessary for this block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvMB0ZCSGP4q"
   },
   "outputs": [],
   "source": [
    "def update_parameters(spikes, latents, params, hypers):\n",
    "    params = update_templates(spikes, latents, params, hypers)\n",
    "    params = update_neuron_probs(spikes, latents, params, hypers)\n",
    "    return params\n",
    "\n",
    "def map_estimate(spikes, \n",
    "                 initial_latents,\n",
    "                 initial_params, \n",
    "                 hypers, \n",
    "                 max_iter=10,\n",
    "                 tol=1e-3):\n",
    "    \"\"\"Find the MAP estimate of the parameters and latent\n",
    "    variables via coordinate ascent.\n",
    "    \"\"\"\n",
    "    # Initialize the algorithm\n",
    "    latents = initial_latents\n",
    "    params = initial_params\n",
    "    lps = [log_probability(spikes, latents, params, hypers)]\n",
    "\n",
    "    pbar = trange(max_iter)\n",
    "    pbar.set_description(\"LP: {:.4f}\".format(lps[0]))\n",
    "    for itr in pbar:\n",
    "        # compute the most likely latent variables given current parameters\n",
    "        latents = update_latent_variables(spikes, params, hypers)\n",
    "        \n",
    "        # compute the most likely parameters given the latent variables\n",
    "        params = update_parameters(spikes, latents, params, hypers)\n",
    "\n",
    "        # compute the new log probability\n",
    "        lps.append(log_probability(spikes, latents, params, hypers))\n",
    "        \n",
    "        # update the progress bar\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(\"LP: {:.4f}\".format(lps[-1]))\n",
    "        \n",
    "        # check for convergence\n",
    "        if abs(lps[-1] - lps[-2]) < tol:\n",
    "            print(\"Convergence detected!\")\n",
    "            break\n",
    "\n",
    "    return np.array(lps), latents, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wK_N0YuwHxLN"
   },
   "outputs": [],
   "source": [
    "lps, latents, new_params = map_estimate(spikes, latents, params, hypers)\n",
    "\n",
    "# Plot the training curve\n",
    "plt.plot(lps)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"log probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYMZtGjpJQp9"
   },
   "outputs": [],
   "source": [
    "# Plot some of the learned templates\n",
    "plot_templates(new_params[\"templates\"], np.arange(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arRcIiEKJndo"
   },
   "outputs": [],
   "source": [
    "# Plot the amplitude distributions for a few neurons\n",
    "def plot_amplitudes(latents, n_rows=2, n_cols=8):\n",
    "    labels = latents[\"labels\"]\n",
    "    amps = latents[\"amplitudes\"]\n",
    "\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, sharex=True, sharey=True, \n",
    "                            figsize=(12, 6))\n",
    "    for ind, ax in enumerate(np.ravel(axs)):\n",
    "        row, col = np.unravel_index(ind, (n_rows, n_cols))\n",
    "        ax.hist(amps[labels==ind], 20, density=True, edgecolor='k', alpha=0.9)\n",
    "    \n",
    "        if row == n_rows - 1:\n",
    "            ax.set_xlabel(\"amplitude\")\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(\"density\")\n",
    "        ax.set_title(\"neuron {}\".format(ind))\n",
    "\n",
    "plot_amplitudes(latents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMOXPBJPiOoS"
   },
   "source": [
    "You'll probably see some bimodal amplitude histograms. These suggest that some labels actually correspond to multiple true neurons.  In the next section we'll do some basic evaluation of the model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Jv-ZeB_nBAD"
   },
   "source": [
    "# Part 4: Evaluation\n",
    "\n",
    "Compare the inferred spike assignments to the ground truth. \n",
    "\n",
    "**There is no coding necessary for this part. Instead, you will be asked to discuss ways that you could further evaluate the spike sorting algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIIVmdzGnxuz"
   },
   "outputs": [],
   "source": [
    "# Load the ground truth labels\n",
    "ground_truth = loadmat(\"eMouseGroundTruth.mat\", squeeze_me=True)\n",
    "true_labels = ground_truth[\"gtClu\"]\n",
    "true_inds = ground_truth[\"gtRes\"]\n",
    "num_true_neurons = true_labels.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJrwUodRVQ6d"
   },
   "source": [
    "## See how close our inferred spike times are to the true times \n",
    "\n",
    "We've written some basic code to check how close the inferred spikes are to the true spike times.  Note that the true spike times are defined as the start of the template, whereas ours are defined as the center. Even after accounting for that, there could be a little offset since we haven't forced our templates to peak in the middle, but we should expect that the time to the nearest true spike should be on the order of a spike width, say 1ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg81xW5lrpvJ"
   },
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(1).fit(true_inds.reshape(-1, 1))\n",
    "dists, nearest_inds = nn.kneighbors((spike_inds - spike_width // 2).reshape(-1, 1))\n",
    "dists, nearest_inds = dists.ravel(), nearest_inds.ravel()\n",
    "dists_sec = dists.ravel() / sample_freq\n",
    "nearest_times = true_inds[nearest_inds.ravel()]\n",
    "\n",
    "plt.hist(dists_sec[dists_sec < 0.005], 50, ec='k')\n",
    "plt.xlabel(\"time to nearest true spike\")\n",
    "plt.ylabel(\"count\")\n",
    "print(\"number of inferred spikes more than 5ms away from a true spike: \", np.sum(dists_sec > 0.005))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MRl4ZB6V2pu"
   },
   "source": [
    "## See how well our inferred labels line up with the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WSWBe9knAkt"
   },
   "outputs": [],
   "source": [
    "# First get the label of the nearest true spike\n",
    "nearest_labels = true_labels[nearest_inds]\n",
    "num_neurons_true = len(np.unique(true_labels))\n",
    "\n",
    "# Then compute the overlap matrix, which counts the number of times \n",
    "# a spike with true label i was assigned label j by our algorithm.\n",
    "def compute_overlap(labels, true_labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    unique_true_labels = np.unique(true_labels)\n",
    "\n",
    "    overlap = np.zeros((len(unique_true_labels), len(unique_labels)))\n",
    "    for i, label_i in enumerate(unique_true_labels):\n",
    "        for j, label_j in enumerate(unique_labels):\n",
    "            overlap[i, j] = np.sum((true_labels == label_i) & (labels == label_j))\n",
    "    return overlap, unique_labels, unique_true_labels\n",
    "\n",
    "overlap, _, _ = compute_overlap(latents[\"labels\"], nearest_labels)\n",
    "\n",
    "plt.imshow(overlap, cmap=\"Greys\")\n",
    "plt.xlabel(\"inferred label\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.colorbar(label=\"count\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07DlhtihWHFe"
   },
   "source": [
    "## The labels are all shuffled! Use a simple heuristic to align them.\n",
    "We permute the inferred labels by finding the true label with the most overlap and then sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nILDqcvwuQ-"
   },
   "outputs": [],
   "source": [
    "# Permute the inferred labels to align with the true labels\n",
    "permutation = np.argsort(np.argmax(overlap, axis=0))\n",
    "\n",
    "plt.imshow(overlap[:, permutation], cmap=\"Greys\")\n",
    "plt.xlabel(\"inferred label\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.colorbar(label=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7BYOjwrWYQp"
   },
   "source": [
    "We should see a nice diagonal band, indicating that a close correspondence between true and inferred labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tz5_ukMyWoQc"
   },
   "source": [
    "## Problem 4a: Discuss evaluation methods for spike sorting.\n",
    "\n",
    "These sanity checks suggest that the algorithm is finding real spike clusters, at least in this synthetic data. Discuss other ways you could evaluate the algorithm:\n",
    "\n",
    "- How surprised should we be by these overlap matrices? Can you think of ways to get a reasonable baseline?\n",
    "\n",
    "- In real data we don't have the ground truth spike times or assignments. What types of checks would you do to assess model performance?\n",
    "\n",
    "*Answer below this line*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzInbfW95cTt"
   },
   "source": [
    "## Problem 4b: Discuss limitations of this modeling approach\n",
    "\n",
    "- How realistic are these modeling assumptions? Are there any assumptions that are clearly not appropriate for this data?\n",
    "\n",
    "- What are the possible effects of such model misspecification? \n",
    "\n",
    "- What were some of the key hyperparameters and how you would expect different hyperparameter settings to affect your results?\n",
    "\n",
    "*Answer below this line*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrDUgYDE5txS"
   },
   "source": [
    "# Submission instructions\n",
    "Create a GradeScope account with your Stanford email address , if you don't already have one. You should have access to the STATS320 course on GradeScope if you're enrolled in STATS320, STATS220, CS339N, or NBIO220. Let us know if you don't see it!\n",
    "\n",
    "- Print to PDF and download an .ipynb file. \n",
    "- Submit both on GradeScope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1awPKubDxYr"
   },
   "source": [
    "# Errata\n",
    "\n",
    "- On Problem 3b, we changed the assert to `assert np.allclose(lp, -1.5218722, atol=1e-4)` in order to account for slight numerical discrepancies you may get with different implementations."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "STAT220/320 Lab 1: Spike Sorting.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
