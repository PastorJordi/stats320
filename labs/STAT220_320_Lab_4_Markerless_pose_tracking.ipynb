{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkK1N7zSL-eZ"
   },
   "source": [
    "# Lab 4: Markerless pose tracking\n",
    "\n",
    "**STATS320: Machine Learning Methods for Neural Data Analysis**\n",
    "\n",
    "_Stanford University. Winter, 2021._\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/slinderman/stats320/blob/main/labs/STAT220_320_Lab_4_Markerless_pose_tracking.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Team Name:** _Your team name here_\n",
    "\n",
    "**Team Members:** _Names of everyone on your team here_\n",
    "\n",
    "*Due: 11:59pm Thursday, Feb 11, 2021 via GradeScope*\n",
    "\n",
    "---\n",
    "\n",
    "This lab develops convolutional neural networks (CNNs) for pose tracking in behavioral videos. The model is inspired by [DeepLabCut](http://www.mackenziemathislab.org/deeplabcut) (DLC), a popular tool for estimating keypoint locations from raw image data by leveraging features of pretrained neural networks for image classification. \n",
    "\n",
    "We'll start with a simpler approach and build a CNN that frames pose tracking as a logistic regression problem. We convolve the input image with a template for each keypoint to get log-odds, and then pass them through the logistic function to get probabilities of the keypoints being present at each pixel. \n",
    "\n",
    "The simple model forms a baseline for our deep neural network. We'll download a pretrained resnet50 and hijack an intermediate layer to obtain features for pose tracking. We'll skip some of the bells and whistles of DLC and just train a simple linear classifier on top of these features. As you'll see, the simple CNN makes for a pretty strong baseline. We'll see if we can eek out some improvement with the resnet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMI793dUci57"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WguM-P7UmrJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.auto import trange\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms.functional\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Specify that we want our tensors on the GPU and in float32\n",
    "device = torch.device('cuda')\n",
    "dtype = torch.float32\n",
    "\n",
    "# Helper function to convert between numpy arrays and tensors\n",
    "to_t = lambda array: torch.tensor(array, device=device, dtype=dtype)\n",
    "from_t = lambda tensor: tensor.to(\"cpu\").detach().numpy()\n",
    "\n",
    "# Helper to round integer to odd number\n",
    "to_odd = lambda x: int(np.ceil(x) // 2 * 2 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "WFjghjJ2vPwh"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions for plotting\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from tempfile import NamedTemporaryFile\n",
    "import base64\n",
    "\n",
    "# Set some plotting defaults\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# initialize a color palette for plotting\n",
    "palette = sns.xkcd_palette([\"windows blue\",\n",
    "                            \"red\",\n",
    "                            \"medium green\",\n",
    "                            \"dusty purple\",\n",
    "                            \"orange\",\n",
    "                            \"amber\",\n",
    "                            \"clay\",\n",
    "                            \"pink\",\n",
    "                            \"greyish\"])\n",
    "\n",
    "def gradient_cmap(colors, nsteps=256, bounds=None):\n",
    "    # Make a colormap that interpolates between a set of colors\n",
    "    ncolors = len(colors)\n",
    "    if bounds is None:\n",
    "        bounds = np.linspace(0,1,ncolors)\n",
    "\n",
    "    reds = []\n",
    "    greens = []\n",
    "    blues = []\n",
    "    alphas = []\n",
    "    for b,c in zip(bounds, colors):\n",
    "        reds.append((b, c[0], c[0]))\n",
    "        greens.append((b, c[1], c[1]))\n",
    "        blues.append((b, c[2], c[2]))\n",
    "        alphas.append((b, c[3], c[3]) if len(c) == 4 else (b, 1., 1.))\n",
    "\n",
    "    cdict = {'red': tuple(reds),\n",
    "             'green': tuple(greens),\n",
    "             'blue': tuple(blues),\n",
    "             'alpha': tuple(alphas)}\n",
    "\n",
    "    cmap = LinearSegmentedColormap('grad_colormap', cdict, nsteps)\n",
    "    return cmap\n",
    "\n",
    "cmaps = [\n",
    "    gradient_cmap([np.array([1, 1, 1, 0]), \n",
    "                   np.concatenate([color, [1]])])\n",
    "    for color in palette\n",
    "]\n",
    "\n",
    "\n",
    "_VIDEO_TAG = \"\"\"<video controls>\n",
    " <source src=\"data:video/x-m4v;base64,{0}\" type=\"video/mp4\">\n",
    " Your browser does not support the video tag.\n",
    "</video>\"\"\"\n",
    "\n",
    "def _anim_to_html(anim, fps=20):\n",
    "    # todo: todocument\n",
    "    if not hasattr(anim, '_encoded_video'):\n",
    "        with NamedTemporaryFile(suffix='.mp4') as f:\n",
    "            anim.save(f.name, fps=fps, extra_args=['-vcodec', 'libx264'])\n",
    "            video = open(f.name, \"rb\").read()\n",
    "        anim._encoded_video = base64.b64encode(video)\n",
    "\n",
    "    return _VIDEO_TAG.format(anim._encoded_video.decode('ascii'))\n",
    "\n",
    "def _display_animation(anim, fps=30, start=0, stop=None):\n",
    "    plt.close(anim._fig)\n",
    "    return HTML(_anim_to_html(anim, fps=fps))\n",
    "\n",
    "def play(movie, fps=30, speedup=1, fig_height=6,\n",
    "         show_time=False):\n",
    "    # First set up the figure, the axis, and the plot element we want to animate\n",
    "    T, Py, Px = movie.shape[:3]\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(fig_height * Px/Py, fig_height))\n",
    "    im = plt.imshow(movie[0], interpolation='None', cmap=plt.cm.gray)\n",
    "\n",
    "    if show_time:\n",
    "        tx = plt.text(0.75, 0.05, 't={:.3f}s'.format(0), \n",
    "                    color='white',\n",
    "                    fontdict=dict(size=12),\n",
    "                    horizontalalignment='left',\n",
    "                    verticalalignment='center', \n",
    "                    transform=ax.transAxes)\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        im.set_data(movie[i * speedup])\n",
    "        if show_time: \n",
    "            tx.set_text(\"t={:.3f}s\".format(i * speedup / fps))\n",
    "        return im, \n",
    "\n",
    "    # call the animator.  blit=True means only re-draw the parts that have changed.\n",
    "    anim = animation.FuncAnimation(fig, animate, \n",
    "                                   frames=T // speedup, \n",
    "                                   interval=1, \n",
    "                                   blit=True)\n",
    "    plt.close(anim._fig)\n",
    "\n",
    "    # return an HTML video snippet\n",
    "    print(\"Preparing animation. This may take a minute...\")\n",
    "    return HTML(_anim_to_html(anim, fps=30))\n",
    "\n",
    "#@title Helper functions to play movies with overlays { display-mode: \"form\" }\n",
    "def play_overlay(movie, probs, fps=30, speedup=1, fig_height=6,\n",
    "                 show_time=False, prob_clip=0.1):\n",
    "    # First set up the figure, the axis, and the plot element we want to animate\n",
    "    T, Py, Px = movie.shape[:3]\n",
    "    assert probs.shape[0] == T\n",
    "    num_keypoints = probs.shape[1]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(fig_height * Px/Py, fig_height))\n",
    "    im = plt.imshow(movie[0], interpolation='None', cmap=plt.cm.gray)\n",
    "    \n",
    "    p_ims = []\n",
    "    for k, (name, color, cmap) in enumerate(zip(keypoint_names, palette, cmaps)):\n",
    "        p_ims.append(plt.imshow(probs[0, k], \n",
    "                     extent=(0, Px, Py, 0), \n",
    "                     cmap=cmap, vmin=0, vmax=prob_clip))\n",
    "        \n",
    "\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        im.set_data(movie[i * speedup])\n",
    "        for k, p_im in enumerate(p_ims):\n",
    "            p_im.set_data(probs[i * speedup, k])\n",
    "        return [im] + p_ims\n",
    "\n",
    "    # call the animator.  blit=True means only re-draw the parts that have changed.\n",
    "    anim = animation.FuncAnimation(fig, animate, \n",
    "                                   frames=T // speedup, \n",
    "                                   interval=1, \n",
    "                                   blit=True)\n",
    "    plt.close(anim._fig)\n",
    "\n",
    "    # return an HTML video snippet\n",
    "    print(\"Preparing animation. This may take a minute...\")\n",
    "    return HTML(_anim_to_html(anim, fps=30))\n",
    "\n",
    "def plot_example(image,\n",
    "                 coords,\n",
    "                 targets,\n",
    "                 title=None,\n",
    "                 downsample=1,\n",
    "                 dilation=1,\n",
    "                 panel_size=5, \n",
    "                 plot_grid=False):\n",
    "    \n",
    "    from scipy.ndimage import binary_dilation\n",
    "    figsize = (panel_size * (num_keypoints + 1), panel_size)\n",
    "    fig, axs = plt.subplots(1, 1 + num_keypoints, figsize=figsize)\n",
    "    \n",
    "    if image.ndim == 2:\n",
    "        axs[0].imshow(image, cmap=\"Greys_r\")\n",
    "    elif image.ndim == 3:\n",
    "        axs[0].imshow(image)\n",
    "    else:\n",
    "        raise Exception(\"image must be 2d grayscale or 3d rgb\")\n",
    "    for k in range(num_keypoints):\n",
    "        axs[0].plot(coords[k, 0] / downsample, \n",
    "                    coords[k, 1] / downsample, \n",
    "                    marker='o', color=palette[k], ls='none',\n",
    "                    label=keypoint_names[k])\n",
    "        \n",
    "    axs[0].legend(loc=\"lower right\", fontsize=8)\n",
    "    axs[0].set_axis_off()\n",
    "    if title: axs[0].set_title(title)\n",
    "    \n",
    "    for k, target in enumerate(targets):\n",
    "        axs[k + 1].imshow(binary_dilation(target, np.ones((dilation, dilation))))\n",
    "        if plot_grid:\n",
    "            height, width = target.shape\n",
    "            axs[k + 1].hlines(-0.5 + np.arange(height + 1), -0.5, width, 'w', lw=0.5)\n",
    "            axs[k + 1].vlines(-0.5 + np.arange(width + 1), -0.5, height, 'w', lw=0.5)\n",
    "        axs[k + 1].set_title(\"Target: {}\".format(keypoint_names[k]))\n",
    "        axs[k + 1].set_axis_off()\n",
    "\n",
    "def plot_image_and_probs(image, probs, probs_max=1.0, panel_size=4):\n",
    "    # Plot the image, the predictions for each keypoint, and a colorbar\n",
    "    num_keypoints = probs.shape[0]\n",
    "    if image.ndim == 3: image = image.mean(0)\n",
    "\n",
    "    figsize = (panel_size * (1 + num_keypoints + .1), panel_size)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = GridSpec(nrows=1, ncols=1 + num_keypoints + 1, figure=fig,\n",
    "                  width_ratios=[1] * (1 + num_keypoints) + [0.1],)\n",
    "    \n",
    "    # Plot the (grayscale) image\n",
    "    ax = plt.subplot(gs[0])\n",
    "    ax.imshow(image, cmap=\"Greys_r\")\n",
    "    ax.axis('off')\n",
    "    ax.set_title(\"input image $X$\")\n",
    "\n",
    "    # Plot the probabilities for each keypoint\n",
    "    for k in range(num_keypoints):\n",
    "        ax = plt.subplot(gs[k + 1])\n",
    "        im = ax.imshow(probs[k], vmin=0, vmax=probs_max)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(\"{} probs\".format(keypoint_names[k]))\n",
    "\n",
    "    # Plot a colorbar\n",
    "    cbax = plt.subplot(gs[-1])\n",
    "    plt.colorbar(im, cax=cbax)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_model_weights(basic_model, panel_size=4):\n",
    "    weights = basic_model.final_conv.weight\n",
    "    num_keypoints = weights.shape[0]\n",
    "    vlim = from_t(abs(weights).max())\n",
    "    \n",
    "    figsize = (panel_size * (num_keypoints + .1), panel_size)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = GridSpec(nrows=1, ncols=num_keypoints + 1, figure=fig,\n",
    "                  width_ratios=[1] * (num_keypoints) + [0.1])\n",
    "\n",
    "    for k in range(num_keypoints):\n",
    "        ax = plt.subplot(gs[k])\n",
    "        im = ax.imshow(from_t(weights[k, 0]), vmin=-vlim, vmax=vlim, cmap=\"RdBu\")\n",
    "        ax.set_title(\"weights: {}\".format(keypoint_names[k]))\n",
    "        ax.set_axis_off()\n",
    "\n",
    "    cbax = plt.subplot(gs[-1])\n",
    "    plt.colorbar(im, cax=cbax)\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEQCJDMrIEXi"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "The dataset was downloaded from the [DeepLabCut github repo](https://github.com/DeepLabCut/DeepLabCut/) and lightly preprocessed with [this notebook](https://colab.research.google.com/drive/1-1MTX16Azx-lYr4ivzxxWwQvJXs4YKrM?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMXtvNESIUM6"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget -nc https://www.dropbox.com/s/0uferm5tj5va17o/lab4_train_data.pkl\n",
    "!wget -nc https://www.dropbox.com/s/55wvvwbnwapfj1o/lab4_val_data.pkl\n",
    "!wget -nc https://www.dropbox.com/s/4x5hyjxk626a7w7/lab4_test_data.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlZcaF4bZfBB"
   },
   "outputs": [],
   "source": [
    "keypoint_names = [\"Hand\", \"Finger\", \"Tongue\", \"Joystick1\", \"Joystick2\"]\n",
    "num_keypoints = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1UII81jUrpr"
   },
   "outputs": [],
   "source": [
    "# Load the training, validation, and test data\n",
    "with open(\"lab4_train_data.pkl\", \"rb\") as f:\n",
    "    train_images, train_coords, train_targets = pickle.load(f)\n",
    "\n",
    "with open(\"lab4_val_data.pkl\", \"rb\") as f:\n",
    "    val_images, val_coords, val_targets = pickle.load(f)\n",
    "\n",
    "with open(\"lab4_test_data.pkl\", \"rb\") as f:\n",
    "    test_images = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH2YJqvJcyo3"
   },
   "source": [
    "## Plot a few training examples\n",
    "\n",
    "Each example consists of an image and a set of binary masks, one for each keypoint. To create the masks, we first made a one-hot matrix with a 1 at the position of the keypoint. Then we performed a [binary dilation](https://en.wikipedia.org/wiki/Dilation_(morphology)#Binary_dilation) to make that target and make it visible in these plots. We'll do the same thing when we train the models below: a larger target offers some room for error in the pose tracking models and makes them easier to train.\n",
    "\n",
    "Notice a few things:\n",
    "- The training examples are not all the same size. Some are cropped and zoomed. This should help the model generalize to new test data.\n",
    "- Some examples are missing a few keypoints. For example, the tongue isn't present in all frames.\n",
    "- Since the training examples are different sizes, we'll have to be a bit careful during training. In particular, we'll use a batch size of 1 since it's not so straightforward to pack the training examples into a tensor. (instead, `type(train_images) == list`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egT4sXZReg_P"
   },
   "outputs": [],
   "source": [
    "for index in [0, 10, 20, 30, 40]:\n",
    "    plot_example(train_images[index], \n",
    "                 train_coords[index], \n",
    "                 train_targets[index],\n",
    "                 dilation=30,\n",
    "                 title=\"Training Image {}\".format(index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwkYM9lIeY29"
   },
   "source": [
    "## Play the test movie\n",
    "\n",
    "We'll test the models on this 8 second clip of the mouse reaching for a joystick and licking a spout. Here, all the frames are ordered and the same size, so it makes sense to show a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icdUN5UfFb74"
   },
   "outputs": [],
   "source": [
    "play(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLlnYs-hJeOb"
   },
   "source": [
    "# Part 1: Groundwork for fitting models \n",
    "\n",
    "Before writing the PyTorch models, let's first lay some groundwork for model fitting and evaluation. One of the nice features of PyTorch is its data loading framework. We'll first write a `Dataset` class for loading and preprocessing the keypoint data. Then we'll write a generic function for training models via stochastic gradient descent. These will come in handy in Parts 2 and 3 of the lab.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mmSUj1MViKE"
   },
   "source": [
    "## Make a PyTorch Dataset class for keypoint data\n",
    "\n",
    "We follow the [PyTorch data loading tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) to make a `Dataset` object for this data. As the comments below say, the dataset is a lightweight wrapper that implements the `iterable` interface. It has a `__getitem__` function that returns requested datapoints (typically we request just one at a time).  Each datapoint is packaged into a dictionary with an `image` key and a `targets` key, which contain $X$ and $Y$ in the notation shown in lecture. \n",
    "\n",
    "Notes:\n",
    "- Before transformation, $X$ is actually shape `(P_H, P_W, 3)`, where 3 is the RGB color axis. \n",
    "- The getter applies a given list of transformations, one of which will permute the order of the axes so that the color channel comes first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM8lEc0WJhLq"
   },
   "outputs": [],
   "source": [
    "class KeypointDataset(Dataset):\n",
    "    \"\"\"Dataset of keypoints in the DeepLabCut (DLC) format.\"\"\"\n",
    "    def __init__(self, images, targets=None, transforms=[]):\n",
    "        \"\"\"\n",
    "        A PyTorch dataset is a lightweight wrapper for an iterable\n",
    "        dataset. It exposes a __getitem__ function that returns an\n",
    "        example at the specified index (or a set of examples if a \n",
    "        list of indices is requested). Here, each example will be \n",
    "        a dictionary with an `image` key and a `target` key. The\n",
    "        getter will optionally apply a sequence of transformations\n",
    "        to each example. More on this below.\n",
    "\n",
    "        images: list (or iterable) of arrays, each H_n x W_n x 3 \n",
    "            where 3 denotes the number of color channels (RGB). \n",
    "            The arrays must be type uint8 (values 0...255).\n",
    "\n",
    "        targets: [optional] list (or iterable) of arrays, each \n",
    "            K x H_n x W_n where K denotes the number of keypoints. \n",
    "            Each H_n x W_n slice should be a one hot matrix indicating \n",
    "            the location of the corresponding keypoint.\n",
    "\n",
    "        transforms: [optional] list (or iterable) of Transform objects \n",
    "            to call before returning an example. See below.\n",
    "        \"\"\"\n",
    "        # convert uint8 images (values 0...255) to float32\n",
    "        assert all([im.dtype == np.uint8 for im in images]), \\\n",
    "            \"Expected images to be uint8's.\"\n",
    "        self.images = [im.astype(np.float32) / 256 for im in images]\n",
    "        \n",
    "        # Check that the targets are the right size and shape\n",
    "        self.targets = targets\n",
    "        if targets is not None:\n",
    "            assert len(targets) == len(images)\n",
    "            assert all([tgt.shape[0] == num_keypoints for tgt in targets])\n",
    "            assert all([img.shape[:2] == tgt.shape[-2:] \n",
    "                        for img, tgt in zip(images, targets)])\n",
    "\n",
    "        # Store any transforms that the user requests\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert indices to list if necessary\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Pull out specified images\n",
    "        image = self.images[idx]\n",
    "\n",
    "        # Extract the keypoints if present, o/w return -1\n",
    "        if self.targets is not None:\n",
    "            targets = self.targets[idx]\n",
    "        else:\n",
    "            targets = -1 * np.ones_like(idx)\n",
    "\n",
    "        # Package output into a dictionary\n",
    "        datapoint = dict(image=image, targets=targets)\n",
    "\n",
    "        # Apply the specified transforms\n",
    "        for transform in self.transforms:\n",
    "            datapoint = transform(datapoint)\n",
    "\n",
    "        return datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5cBItXHZMnO"
   },
   "source": [
    "## Define transformations to apply to each datapoint\n",
    "\n",
    "As you see above, the getter in the `KeypointDataset` object can apply a sequence of transformations. Here we'll define four such transformations. Each is an object with a `__call__` function, which takes as input a datapoint and outputs a transformed datapoint. As above, the datapoints are dictionaries with an `image` key and a `targets` key.\n",
    "\n",
    "We'll implement the first two transformations for you, so you can see how they work. The first two are:\n",
    "1. Convert the `image` and `targets` to tensors and move them to the GPU.\n",
    "2. Normalize the color channels of the image so that they have the mean and standard deviation expected by pretrained PyTorch models.\n",
    "\n",
    "You'll implement two more transformations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xKzFVlsYrYJ"
   },
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors on the GPU.\"\"\"\n",
    "    def __call__(self, datapoint):\n",
    "        # swap image color axis because\n",
    "        # numpy image: H x W x 3 \n",
    "        # torch image: 3 X H X W\n",
    "        image = datapoint['image'].transpose((2, 0, 1))\n",
    "        return dict(image=to_t(image), targets=to_t(datapoint['targets']))\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize the images to have the mean and standard deviation \n",
    "    expected by pretrained pytorch models.\n",
    "    \n",
    "    See: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "    \"\"\"\n",
    "    def __call__(self, datapoint):\n",
    "        image = torchvision.transforms.functional.normalize(\n",
    "            datapoint['image'], \n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225])\n",
    "        return dict(image=image, targets=datapoint['targets'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fq1OYM1W8dc"
   },
   "source": [
    "## Problem 1a: Implement a downsampling transformation\n",
    "\n",
    "Write a transformation object called `Downsample` that downsamples the image and target by a specified factor. This will make our code run faster, albeit with slightly lower precision.\n",
    "\n",
    "You may need to consult the documentation in [`torch.nn.functional`](https://pytorch.org/docs/stable/nn.functional.html), which is imported as `F` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6SNI6WA4No_"
   },
   "outputs": [],
   "source": [
    "class Downsample(object):\n",
    "    \"\"\"Downsample the image using avg_pool2d\n",
    "    and the target (if given) with max_pool2d.\"\"\"\n",
    "    def __init__(self, downsample_factor=1):\n",
    "        self.downsample_factor = downsample_factor\n",
    "        \n",
    "    def __call__(self, datapoint):\n",
    "        \"\"\"\n",
    "        Downsample the image and (if present) the targets \n",
    "        by `self.downsample_factor`.\n",
    "\n",
    "        datapoint: dict with keys `image` and `targets`.\n",
    "            Assume both are PyTorch tensors and their shapes\n",
    "            are (3, H, W) and (K, H, W), respectively.\n",
    "        \"\"\"\n",
    "        image = datapoint['image']\n",
    "\n",
    "        ### \n",
    "        # Downsample image by a factor of `self.downsample_factor`\n",
    "        # using `F.avg_pool2d`.\n",
    "        #\n",
    "        # YOUR CODE BELOW\n",
    "        image = F.avg_pool2d(...)\n",
    "        \n",
    "        # Downsample targets by max pooling, if present.\n",
    "        # (If no targets are given, they will be a 1d array of -1's.)\n",
    "        targets = datapoint['targets']\n",
    "        if targets.ndim > 1:\n",
    "            targets = F.max_pool2d(...)\n",
    "        #\n",
    "        ###\n",
    "        \n",
    "        return dict(image=image, targets=targets)\n",
    "\n",
    "# Test by comparing to manual downsampling.\n",
    "def test_downsample():\n",
    "    dummy_image = np.arange(300).reshape(3, 10, 10)\n",
    "    dummy_image_ds = dummy_image.reshape(3, 5, 2, 5, 2).mean(axis=(2, 4))\n",
    "    \n",
    "    dummy_target = np.zeros((5, 10, 10))\n",
    "    dummy_target[0, 4, 4] = 1\n",
    "    dummy_target[1, 5, 5] = 1\n",
    "    dummy_target[2, 6, 6] = 1\n",
    "    dummy_target[3, 7, 7] = 1\n",
    "    dummy_target[4, 8, 8] = 1\n",
    "    dummy_target_ds = dummy_target.reshape(5, 5, 2, 5, 2).max(axis=(2, 4))\n",
    "    \n",
    "    dummy_datapoint = dict(image=to_t(dummy_image), targets=to_t(dummy_target))\n",
    "    dummy_output = Downsample(2)(dummy_datapoint)\n",
    "    assert from_t(dummy_output['image']).shape == dummy_image_ds.shape\n",
    "    assert np.allclose(from_t(dummy_output['image']), dummy_image_ds)\n",
    "    assert from_t(dummy_output['targets']).shape == dummy_target_ds.shape\n",
    "    assert np.allclose(from_t(dummy_output['targets']), dummy_target_ds)\n",
    "\n",
    "test_downsample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5Y-Y9j3XN42"
   },
   "source": [
    "## Problem 1b: Implement a transformation to dilate the targets\n",
    "\n",
    "Dilate the targets to make them a bit bigger. This offers more positive examples for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5y1lQQrg4epH"
   },
   "outputs": [],
   "source": [
    "class DilateTargets(object):\n",
    "    \"\"\"Dilate the binary target by convolving with ones.\"\"\"\n",
    "    def __init__(self, dilation_factor=1):\n",
    "        self.dilation_factor = dilation_factor\n",
    "        assert dilation_factor % 2 == 1, \"dilation_factor must be odd\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Dilate the target by a factor of `self.dilation_factor`\n",
    "        by convolving by a square matrix of ones of that size.\n",
    "\n",
    "        datapoint: dict with keys `image` and `targets`.\n",
    "            Assume both are PyTorch tensors and their shapes\n",
    "            are (3, H, W) and (K, H, W), respectively.\n",
    "        \"\"\"\n",
    "        targets = sample['targets']\n",
    "        shape = targets.shape\n",
    "        assert targets.ndim > 1, \"DilateTargets called without targets!\"\n",
    "\n",
    "        ###\n",
    "        # Convolve each keypoint (input channel) with a square of ones.\n",
    "        # Pad the convolution so that the output is the same shape as the\n",
    "        # input and the center of the dilated targets are on the ones in \n",
    "        # the original targets.\n",
    "        #  \n",
    "        # Hint: you may find the `groups` keyword argument of conv2d helpful.\n",
    "        # \n",
    "        # YOUR CODE BELOW\n",
    "        ...\n",
    "        targets = F.conv2d(...)\n",
    "        ...\n",
    "        #\n",
    "        ###\n",
    "        \n",
    "        assert targets.shape == shape\n",
    "        return dict(image=sample['image'], targets=targets)\n",
    "\n",
    "#  Test by comparing to dilation with scipy.ndimage.\n",
    "def test_dilate():\n",
    "    from scipy.ndimage import binary_dilation\n",
    "    dummy_target = np.zeros((3, 10, 10))\n",
    "    dummy_target[0, 4, 4] = 1\n",
    "    dummy_target[1, 5, 5] = 1\n",
    "    dummy_target[2, 6, 6] = 1\n",
    "    dummy_target_dil = np.array(\n",
    "        [binary_dilation(tgt, structure=np.ones((3, 3)))\n",
    "        for tgt in dummy_target])\n",
    "    dummy_datapoint = dict(image=None, targets=to_t(dummy_target))\n",
    "    dummy_output = DilateTargets(3)(dummy_datapoint)\n",
    "    assert from_t(dummy_output['targets']).shape == dummy_target_dil.shape\n",
    "    assert np.allclose(from_t(dummy_output['targets']), dummy_target_dil)\n",
    "\n",
    "test_dilate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnYL8OJXVWnD"
   },
   "source": [
    "## Problem 1c: Initialize datasets for the training, validation, and test data\n",
    "\n",
    "Make `KeypointDataset` objects for the training, validation, and test data. In all cases, apply the following (ordered) set of transformations:\n",
    "1. Convert to tensors on the GPU with `ToTensor`\n",
    "2. Normalize the color channels with `Normalize`\n",
    "3. Downsample the images (and targets) by a factor of 2 with `Downsample`. This will speed up all your analyses at the cost of a little lower resolution.\n",
    "\n",
    "Finally, for the training and validation data only:\n",
    "4. Apply `DilateTargets` with a dilation factor of 21 (equivalent to dilating by 42 pixels in the original image). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUi-qZZ6aOVv"
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Make the training, validation, and test datasets as described aboe.\n",
    "# \n",
    "# YOUR CODE BELOW\n",
    "downsample = 2\n",
    "dilate = 21\n",
    "\n",
    "train_dataset = ...\n",
    "val_dataset = ...\n",
    "# The test dataset doesn't have targets so omit the dilation.\n",
    "test_dataset = ...\n",
    "#\n",
    "###\n",
    "\n",
    "# Plot the training examples from above but after transformation\n",
    "for index in [0, 10, 20, 30, 40]:\n",
    "    plot_example(from_t(train_dataset[index]['image'][0]), \n",
    "                train_coords[index], \n",
    "                from_t(train_dataset[index]['targets']),\n",
    "                downsample=downsample,\n",
    "                title=\"Train Image {}\".format(index))\n",
    "\n",
    "# Test\n",
    "assert train_dataset[0]['image'].shape == (3, 373, 416)\n",
    "assert train_dataset[0]['targets'].shape == (5, 373, 416)\n",
    "assert train_dataset[0]['targets'].sum() == 1764\n",
    "assert val_dataset[0]['image'].shape == (3, 235, 320)\n",
    "assert val_dataset[0]['targets'].shape == (5, 235, 320)\n",
    "assert val_dataset[0]['targets'].sum() == 1764\n",
    "assert test_dataset[0]['image'].shape == (3, 373, 416)\n",
    "assert test_dataset[0]['targets'] == -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NP6jA7KC95vO"
   },
   "source": [
    "## Problem 1d: Comment this function for training Pytorch models\n",
    "\n",
    "We've written a generic model training function following [this example](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html). \n",
    "\n",
    "**The code is written. You just have to write comments for this function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFfTaNY22yAO"
   },
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                train_dataset, \n",
    "                val_dataset,\n",
    "                loss_func,  \n",
    "                num_epochs=200, \n",
    "                lr=1.0,\n",
    "                momentum=0.9,\n",
    "                lr_step_size=100,\n",
    "                lr_gamma=0.1):\n",
    "\n",
    "    pbar = trange(num_epochs)\n",
    "    pbar.set_description(\"---\")\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=1)\n",
    "    dataloaders = dict(train=train_dataloader, val=val_dataloader)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), \n",
    "                          lr=lr, \n",
    "                          momentum=momentum)\n",
    "    \n",
    "    scheduler = lr_scheduler.StepLR(optimizer, \n",
    "                                    step_size=lr_step_size, \n",
    "                                    gamma=lr_gamma)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "        \n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0\n",
    "            running_size = 0\n",
    "            for datapoint in dataloaders[phase]:\n",
    "                image_t = datapoint['image']\n",
    "                targets_t = datapoint['targets']\n",
    "                if phase == \"train\":\n",
    "                    with torch.set_grad_enabled(True):\n",
    "                        optimizer.zero_grad()\n",
    "                        output_t = model(image_t)\n",
    "                        loss_t = loss_func(output_t, targets_t)\n",
    "                        loss_t.backward()\n",
    "                        optimizer.step()\n",
    "                else:\n",
    "                    output_t = model(image_t)\n",
    "                    loss_t = loss_func(output_t, targets_t)\n",
    "\n",
    "                assert loss_t >= 0\n",
    "                running_loss += loss_t.item()\n",
    "                running_size += 1\n",
    "            \n",
    "            running_loss /= running_size\n",
    "            losses = train_losses if phase == \"train\" else val_losses\n",
    "            losses.append(running_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        pbar.set_description(\"Epoch {:03} Train {:.4f} Val {:.4f}\"\\\n",
    "                             .format(epoch, train_losses[-1], val_losses[-1]))\n",
    "        pbar.update(1)\n",
    "\n",
    "    return np.array(train_losses), np.array(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jeZcQy5iyT9"
   },
   "source": [
    "# Part 2: Implement basic pose tracking with a CNN\n",
    "\n",
    "Our \"basic model\" will use a CNN to implement a template matching/logistic regression model. \n",
    "\n",
    "First, notation: let\n",
    "- $X \\in \\mathbb{R}^{P_H \\times P_W}$ denote the a (lightly preprocessed) input image. Its height is $P_H$ pixels and its width is $P_W$ pixels.\n",
    "- $Y_{k} \\in \\{0,1\\}^{P_H \\times P_W}$ denote the binary target for keypoint $k$.\n",
    "- $W_k \\in \\mathbb{R}^{P_h \\times P_w}$ denote the template (i.e. *weights*) for keypoint $k$. It is $P_h < P_H$ by $P_w < P_W$ pixels.\n",
    "- $b_k \\in \\mathbb{R}$ denote a bias for keypoint $k$.\n",
    "\n",
    "\n",
    "The \"basic\" model treats keypoint estimation as a logistic regression problem. We convolve the image with weights to obtain the log-odds (i.e. *logits*) of keypoint $k$ for image $n$,\n",
    "\\begin{align}\n",
    "U_{k} &= (X \\star W_k) + b_k.\n",
    "\\end{align}\n",
    "Note that $U_{k}  \\in \\mathbb{R}^{P_H \\times P_W}$ is the same shape as the input image.  Its entries specify the log-odds of keypoint $k$ being found at each pixel. \n",
    "Finally, map the logits through a logistic function to get probabilities, and plug them into the Bernoulli likelihood function,\n",
    "\\begin{align}\n",
    "p(Y_{k} \\mid X, W_k) &= \\prod_{i=1}^{P_H} \\prod_{j=1}^{P_W} \\mathrm{Bern}(y_{kij} \\mid \\sigma(u_{kij})).\n",
    "\\end{align}\n",
    "We'll maximize the (log) likelihood of a image as a function of $W_k$, \n",
    "\\begin{align}\n",
    "\\mathcal{L}(W_k) &= \\sum_{i=1}^{P_H} \\sum_{j=1}^{P_W} \\log \\mathrm{Bern}(y_{kij} \\mid \\sigma(u_{kij})) \\\\\n",
    "&= \\sum_{i=1}^{P_H} \\sum_{j=1}^{P_W} y_{kij} u_{kij} - \\log (1 + \\exp \\{u_{kij}\\}) \n",
    "\\end{align}\n",
    "as we derived in class.\n",
    "\n",
    "Technically, we'll actually minimize the negative average log likelihood via stochastic gradient descent by grabbing one image at a time, evaluating the loss function for that image, taking the gradient of the loss (automatically), and updating our weights by taking a step in the direction of the gradient.\n",
    "\n",
    "**Practicalities.**\n",
    "A few practical issues should be mentioned:\n",
    "- To speed training and gain some statistical efficiency, our basic model will downsample the images before applying the convolution and then upsample the logits to the full resolution before evaluating the loss. You can think of this as a form of weight sharing.\n",
    "- Likewise, our basic model will operate on grayscale images by taking the mean over the color channels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEzx9HKeXa4H"
   },
   "source": [
    "## Problem 2a: Implement the basic model\n",
    "\n",
    "A PyTorch model inherits from `nn.Module`. It has trainable parameters, which are encapsulated in its class variables. For example, the `BasicPoseTracker` model below has a `self.conv` class variable, which is an instance of `nn.Conv2d`, an object that implements a 2D convolution. (We've said this before, but remember convolutions in deep learning are cross-correlations in the real world.) The `Conv2d` object has inside of it a set of parameters, which the training algorithm above will modify, in place, via stochastic gradient descent. In particular, the `Conv2d` object has a stack of weights and a set of biases, one for each keypoint. \n",
    "\n",
    "PyTorch models implement a `forward` function, which takes an input and produces an output. Here, the input is a batch of images `B x 3 x P_H x P_W`, but really the batch will just be shape `1` since different images may be different sizes. The output will be a `B x K x P_H x P_W` set of log-odds, or \"logits,\" one for each image, keypoint, and pixel. \n",
    "\n",
    "In the basic model, the `foward` function should perform the following operations:\n",
    "1. Convert the image to grayscale by taking the mean over the color axis to produce a `B x 1 x P_H x P_W` output.\n",
    "2. Downsample by the downsampling factor specified in the constructor. We'll allow fractional downsampling, which is best implemented with the `F.interpolate` function.\n",
    "3. Apply the convolution to the downsampled image to get logits for each keypoint.\n",
    "4. Upsample the logits back to the original image size, again using `F.interpolate`.\n",
    "\n",
    "The forward function returns the upsampled logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zc4yKkD5i8EH"
   },
   "outputs": [],
   "source": [
    "class BasicPoseTracker(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_keypoints, \n",
    "                 downsample=1,\n",
    "                 patch_size=121):\n",
    "        super(BasicPoseTracker, self).__init__()\n",
    "        self.num_keypoints = num_keypoints\n",
    "        assert downsample >= 1.0, \"downsample factor must be at least 1\"\n",
    "        self.downsample = downsample\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # compute the effective patch size in the downsampled image\n",
    "        # round to the nearest odd number so that the convolution is centered.\n",
    "        self.eff_patch_size = to_odd(patch_size / downsample)\n",
    "        \n",
    "        ###\n",
    "        # Convolve (in neural net lingo; cross-correlate in stats lingo)\n",
    "        # the input with a stack of weights. The weights are parameters of \n",
    "        # the nn.Conv2d layer, initialized below. Its dimensions are 1 \n",
    "        # (the number of input channels after grayscale conversion) by \n",
    "        # K (the number of keypoints) by h by w (the effective height and width \n",
    "        # of the patch). Pad the input with (h // 2) and (w // 2) zeros so that\n",
    "        # the output is the same size as the input. Note that by default the\n",
    "        # convolution layer also has a learnable bias, which we want.\n",
    "        # \n",
    "        # YOUR CODE BELOW\n",
    "        self.final_conv = nn.Conv2d(...)\n",
    "        #\n",
    "        ###\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Run the \"forward pass\" of the model to produce logits for\n",
    "        a batch of images `x`.\n",
    "\n",
    "        x: (1 x 3 x H x W) \"batch\" of images, where the batch size\n",
    "           is always 1.\n",
    "\n",
    "        returns: (1 x K x H x W) batch of logits for each keypoint.\n",
    "        \"\"\"\n",
    "        original_size = x.shape[-2:]\n",
    "        ###\n",
    "        # Complete the steps below, which iteratively update \n",
    "        # `x` by applying a sequence of functions to it.\n",
    "        #\n",
    "        # YOUR CODE BELOW\n",
    "        \n",
    "        # 1. Convert to grayscale by taking a mean over \n",
    "        #    the color channels (axis=1), but make sure its still 4D. \n",
    "        x = ...\n",
    "        \n",
    "        # 2. Further downsample the image by the factor specified above.\n",
    "        #    Catch warnings in the F.interpolate function.\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            x = F.interpolate(...)\n",
    "        \n",
    "        # 3. Apply your convolution layer\n",
    "        x = ...\n",
    "        \n",
    "        # 4. Interpolate back to the original size\n",
    "        #    Catch warnings in the interpolate function.\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            x = F.interpolate(...)\n",
    "        #\n",
    "        ###\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CssqpCXDBcPi"
   },
   "source": [
    "## Instantiate the model and move it to the GPU\n",
    "Fix the random seed so the weights will be reproducible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syyCN_JSBfVv"
   },
   "outputs": [],
   "source": [
    "# Construct a basic pose tracker with random initial weights.\n",
    "torch.manual_seed(0)\n",
    "basic_model = BasicPoseTracker(num_keypoints, \n",
    "                               downsample=4,\n",
    "                               patch_size=101)\n",
    "\n",
    "# Move the model to the GPU\n",
    "basic_model = basic_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqJmBwDQcZ0F"
   },
   "source": [
    "## Problem 2b: Plot model predictions\n",
    "\n",
    "Write a function to compute the keypoint probabilities output by the model.\n",
    "\n",
    "Since the constructor initialized the `Conv2d` layer with random weights, the initial predictions should be terrible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUttQxsU3tE0"
   },
   "outputs": [],
   "source": [
    "def plot_model_predictions(model, datapoint):\n",
    "    \"\"\"Plot the image and the model's predictions \n",
    "    (i.e. probabilities per pixel) for each of the \n",
    "    keypoints.\n",
    "    \"\"\"\n",
    "    \n",
    "    ###\n",
    "    # Compute the model probabilities $\\sigma(U)$.\n",
    "    # It should be a K x H x W array.\n",
    "    #\n",
    "    # YOUR CODE BELOW\n",
    "    image_t = datapoint['image']\n",
    "    \n",
    "    ...\n",
    "    probs_t = \n",
    "    #\n",
    "    ###\n",
    "    assert probs_t.shape[0] == num_keypoints\n",
    "    assert probs_t.shape[1:] == image_t.shape[1:]\n",
    "    plot_image_and_probs(from_t(image_t), from_t(probs_t))\n",
    "\n",
    "# Plot the probabilities output under the initial, random parameters    \n",
    "plot_model_predictions(basic_model, train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kP3X3xx3eYv_"
   },
   "source": [
    "## Problem 2c [Math] Log-sum-exp trick\n",
    "The log likelihood for a single image, as given above, is\n",
    "\\begin{align}\n",
    "\\mathcal{L}(W_k) &= \n",
    "\\sum_{i=1}^{P_H} \\sum_{j=1}^{P_W} y_{kij} u_{kij} - \\log (1 + \\exp \\{u_{kij}\\}) \n",
    "\\end{align}\n",
    "Show that this is equivalent to,\n",
    "\\begin{align}\n",
    "\\mathcal{L}(W_k) &= \n",
    "\\sum_{i=1}^{P_H} \\sum_{j=1}^{P_W} y_{kij} u_{kij} - m_{kij} - \\log (\\exp\\{-m_{kij}\\} + \\exp \\{u_{kij} - m_{kij}\\}) \n",
    "\\end{align}\n",
    "for any $m_{kij} \\in \\mathbb{R}$. \n",
    "\n",
    "Set $m_{kij} = \\max \\{0, u_{kij}\\}$. Why does this lead to a more numerically stable computation?\n",
    "\n",
    "Answer below this line.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jypVTxw-gN8g"
   },
   "source": [
    "## Problem 2d: Implement the Bernoulli loss \n",
    "PyTorch has a built-in loss function that implements the negative Bernoulli log likelihood above. They call it the binary cross-entropy loss, and there are versions that take in either probabilities ([`BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss)) or logits ([`BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)).  It's a good exercise to write it from scratch at least once though.\n",
    "\n",
    "Implement the `bernoulli_loss` function below. It should return the average negative log likelihood over all keypoints and pixels. I.e.\n",
    "\\begin{align}\n",
    "-\\frac{1}{K P_H P_W} \\sum_{k=1}^K \\mathcal{L}(W_k).\n",
    "\\end{align}\n",
    "Use the log-sum-exp trick derived above to implement this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyxxaEZI2GxD"
   },
   "outputs": [],
   "source": [
    "def bernoulli_loss(logits_t, targets_t):\n",
    "    \"\"\"Compute the Bernoulli loss as described in the comment above.\n",
    "\n",
    "    logits_t:  K x P_H x P_W array of logits ($u_{kij}$)\n",
    "    targets_t: K x P_H x P_W array of binary targets ($y_{kij}$)\n",
    "\n",
    "    returns: scalar average negative log likelihood.\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Compute the average negative log likelihood.\n",
    "    # \n",
    "    # YOUR CODE BELOW\n",
    "    avg_nll = ...\n",
    "    #\n",
    "    ###\n",
    "    return avg_nll\n",
    "\n",
    "# Test on a single image\n",
    "image_t = train_dataset[0]['image']\n",
    "logits_t = basic_model(image_t.unsqueeze(0)).squeeze(0)\n",
    "targets_t = train_dataset[0]['targets']\n",
    "assert torch.isclose(bernoulli_loss(logits_t, targets_t), \n",
    "                     torch.tensor(0.9426), atol=1e-4)\n",
    "\n",
    "# Test that it works even with potential overflow\n",
    "logits_t = 1e3 * torch.ones_like(targets_t)\n",
    "assert torch.isfinite(bernoulli_loss(logits_t, targets_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37o9SvrpjXSW"
   },
   "source": [
    "## Train the basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Vk69ENq3BuL"
   },
   "outputs": [],
   "source": [
    "# Train the basic model\n",
    "train_losses, val_losses = train_model(basic_model, \n",
    "                                       train_dataset, \n",
    "                                       val_dataset, \n",
    "                                       bernoulli_loss)\n",
    "\n",
    "# Plot the training and validation curves\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].plot(train_losses, color=palette[0], label=\"train\")\n",
    "axs[0].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
    "axs[0].set_xlabel(\"epoch\")\n",
    "axs[0].set_ylabel(\"bernoulli loss\")\n",
    "axs[0].set_ylim(bottom=0)\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].plot(train_losses, color=palette[0], label=\"train\")\n",
    "axs[1].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
    "axs[1].set_xlabel(\"epoch\")\n",
    "axs[1].set_ylabel(\"bernoulli loss\")\n",
    "axs[1].set_ylim(bottom=0, val_losses[50])\n",
    "axs[1].grid(True)\n",
    "axs[1].legend(loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrATTeXhoxBs"
   },
   "source": [
    "## Look at model outputs\n",
    "\n",
    "Try the model on the same training image as above. If it's working, you should see the keypoint probabilities are close to one (yellow) at the keypoint and zero (blue) elsewhere.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqbIbI_J_BpA"
   },
   "outputs": [],
   "source": [
    "plot_model_predictions(basic_model, train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7-plmWOdKM1"
   },
   "source": [
    "## Problem 2e [Short Answer]: Interpret the inferred weights\n",
    "\n",
    "The probabilities are derived from the learned weights, plotted below. Are the what you expected? Can you make any sense of them?\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MelPamEGdJiF"
   },
   "outputs": [],
   "source": [
    "plot_model_weights(basic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Agh5-Xi0pis3"
   },
   "source": [
    "## Make a movie of the predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOoiLpS88pGj"
   },
   "outputs": [],
   "source": [
    "# Extract probabilities for each frame\n",
    "basic_probs = []\n",
    "for t in trange(len(test_dataset)):\n",
    "    ex = test_dataset[t]\n",
    "    logits_t = basic_model(ex[\"image\"].unsqueeze(0)).squeeze(0)\n",
    "    basic_probs.append(from_t(torch.sigmoid(logits_t)))\n",
    "basic_probs = np.array(basic_probs)\n",
    "\n",
    "play_overlay(test_images, basic_probs, prob_clip=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzesWqz17Zaa"
   },
   "source": [
    "# Part 3: Transfer learning from a pretrained ResNet\n",
    "\n",
    "Now we'll download a pretrained residual network from PyTorch's model zoo and hijack its intermediate layer features to predict keypoint locations.  This is called **transfer learning**: using a model trained for a different purpose (here, image classification) as the basis for a new task (here, keypoint detection).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJdyVlk3EPce"
   },
   "outputs": [],
   "source": [
    "# Download a pretrained resnet50 from pytorch's model zoo and copy it to the GPU\n",
    "pretrained_resnet50 = models.resnet50(pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIvexOgA9_R2"
   },
   "source": [
    "The ResNet source code can be found [here](https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html). Its `forward` function applies the following layers, with my comments added:\n",
    "```    \n",
    "def _forward_impl(self, x):\n",
    "    # See note [TorchScript super()]\n",
    "    x = self.conv1(x)         # 2d convolution over color channels \n",
    "    x = self.bn1(x)           # batch norm\n",
    "    x = self.relu(x)          # nonlinearity\n",
    "    x = self.maxpool(x)       # max pooling\n",
    "\n",
    "                              # Now for the residual network!\n",
    "                              # Each \"layer\" is a series of strided convolutions, \n",
    "                              # batch normalization, and nonlinearities. The \n",
    "                              # result is added to the (downsampled) input \n",
    "                              # before passing through the final nonlinearity to\n",
    "                              # produce an output with the specified number of \n",
    "                              # channels (planes) and downsampling factors.\n",
    "\n",
    "    x = self.layer1(x)        # outputs 256  channels @ 4x   downsampling\n",
    "    x = self.layer2(x)        # outputs 512  channels @ 8x   downsampling\n",
    "    x = self.layer3(x)        # outputs 1024 channels @ 16x  downsampling\n",
    "    x = self.layer4(x)        # outputs 2048 channels @ 32x  downsampling\n",
    "\n",
    "    x = self.avgpool(x)       # adaptive average pooling over pixels to get a 2048x1x1 output\n",
    "    x = torch.flatten(x, 1)   # flatten to remove trailing dimensions\n",
    "    x = self.fc(x)            # pass through a fully connected layer to make final predictions!\n",
    "\n",
    "    return x\n",
    "\n",
    "def forward(self, x):\n",
    "    return self._forward_impl(x)\n",
    "```\n",
    "\n",
    "We'll write our own torch `Module` that freezes the weights of the pretrained ResNet and then learns to predict binary presence or absence of a keypoint at each pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvLwrpvLF0jO"
   },
   "source": [
    "## Problem 3a: Implement the resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBVJ-EiJ-g88"
   },
   "outputs": [],
   "source": [
    "init_layer_names = ['conv1', 'bn1', 'relu', 'maxpool']\n",
    "bottleneck_layer_names = ['layer1', 'layer2', 'layer3', 'layer4']\n",
    "bottleneck_sizes = [256, 512, 1024, 2048]\n",
    "\n",
    "class ResnetPoseTracker(nn.Module):\n",
    "    def __init__(self, \n",
    "                 resnet,\n",
    "                 num_keypoints,\n",
    "                 num_bottleneck_layers,\n",
    "                 kernel_size):\n",
    "        \"\"\"A pose tracking model that uses a pretrained resnet to extract\n",
    "        features. \n",
    "\n",
    "        resnet: the pretrained resnet model\n",
    "\n",
    "        num_keypoints: the number of keypoints to predict\n",
    "\n",
    "        num_bottleneck_layers: the number of bottleneck layers to apply \n",
    "            before extracting features. Must be 1, 2, 3, or 4.\n",
    "\n",
    "        kernel_size: the size of the kernel in the final convolutional\n",
    "            layer. Equivalently, the number of \"super pixels\" to use in\n",
    "            making the final keypoint probability predictions. This is\n",
    "            analogous to the patch size in the basic model.\n",
    "        \"\"\"\n",
    "        super(ResnetPoseTracker, self).__init__()\n",
    "        assert num_bottleneck_layers >= 1 and num_bottleneck_layers <= 4\n",
    "        assert kernel_size % 2 == 1\n",
    "        \n",
    "        # Copy the specified layers from the pretrained resnet\n",
    "        # and fix their parameters by setting `requires_grad = False`\n",
    "        # If `setattr` and `getattr` are unfamiliar, take a minute\n",
    "        # to google them and understand what we're doing here.\n",
    "        for layer_name in init_layer_names + bottleneck_layer_names:\n",
    "            setattr(self, layer_name, deepcopy(getattr(resnet, layer_name)))\n",
    "            for param in getattr(self, layer_name).parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Store the number of bottleneck layers\n",
    "        self.num_bottleneck_layers = num_bottleneck_layers\n",
    "\n",
    "        ###\n",
    "        # Add a final conv layer applied across channels for each \"super pixel\".\n",
    "        # The number of input channels is determined by the output of the last\n",
    "        # bottleneck layer applied. There is an output channel for each keypoint \n",
    "        # and a kernel of specified size. Make sure you pad the convolution so \n",
    "        # the output size is the same as the input size.\n",
    "        #\n",
    "        # YOUR CODE BELOW\n",
    "        self.final_conv = nn.Conv2d(...)\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_size = x.shape[-2:]\n",
    "        \n",
    "        ###\n",
    "        # Implement the forward pass. Apply all the initial layers but only the \n",
    "        # specified number of bottleneck layers. Then apply your final \n",
    "        # convolution layer and interpolate to get logits the same size as the \n",
    "        # input image.\n",
    "        # \n",
    "        # YOUR CODE BELOW\n",
    "        ...\n",
    "        #\n",
    "        ###\n",
    "        assert x.shape[-2:] == original_size\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gBef6grF8MN"
   },
   "source": [
    "## Instantiate the resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8GEyKX7cbn90"
   },
   "outputs": [],
   "source": [
    "# Initialize the resnet pose model\n",
    "torch.manual_seed(0)\n",
    "resnet_model = ResnetPoseTracker(pretrained_resnet50, \n",
    "                                 num_keypoints, \n",
    "                                 num_bottleneck_layers=1, \n",
    "                                 kernel_size=3).to(device)\n",
    "\n",
    "# Plot the model predictions \n",
    "plot_model_predictions(resnet_model, train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGIplL7vGX1N"
   },
   "source": [
    "## Problem 3b: Plot the resnet features after each bottleneck layer\n",
    "\n",
    "First, write a function to compute the model predictions after a specified number of bottleneck layers. Then plot a few of the features for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iL8uUTJeGhRr"
   },
   "outputs": [],
   "source": [
    "def get_features(resnet_model, datapoint, num_layers):\n",
    "    \"\"\"\n",
    "    Get the pretrained resnet features at the specified\n",
    "    bottleneck layer.\n",
    "\n",
    "    image_t: (3 x H x W) input image tensor from dataset\n",
    "    bottleneck_layer: number 1...4 specifying with layer to output\n",
    "    \"\"\"\n",
    "    assert num_layers >= 1 and num_layers <= 4\n",
    "    image_t = datapoint[\"image\"]\n",
    "    \n",
    "    ###\n",
    "    # Run the image through the resnet up to the output of \n",
    "    # the specified bottleneck layer. Eg. if num_layers == 1,\n",
    "    # give the output after resnet.layer1\n",
    "    #\n",
    "    # YOUR CODE BELOW\n",
    "    ...\n",
    "    #\n",
    "    ###\n",
    "    return x\n",
    "\n",
    "for num_layers, layer_size in enumerate(bottleneck_sizes):\n",
    "    features = get_features(resnet_model, train_dataset[0], num_layers+1)\n",
    "    assert features.shape[0] == layer_size\n",
    "    \n",
    "    # Plot the first four feature channels\n",
    "    num_features = 5\n",
    "    fig, axs = plt.subplots(1, num_features, \n",
    "                            figsize=(4 * num_features, num_features), \n",
    "                            sharey=True)\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.imshow(from_t(features[i]), cmap=\"Greys_r\")\n",
    "        ax.set_title(\"layer {} feature {}/{}\".\\\n",
    "                     format(num_layers+1, i, layer_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9BHJiTNIw2D"
   },
   "source": [
    "## Problem 3c [Short Answer]: Discuss the pros/cons of number of layers\n",
    "\n",
    "When predicting keypoints, we could take the output of the model after any one of the layers. Discuss the considerations the pros and cons of deeper vs shallower layers.\n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkxcB4VUsuIN"
   },
   "source": [
    "## Train the resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wJNT6FCmEUl"
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses = train_model(resnet_model, \n",
    "                                       train_dataset, \n",
    "                                       val_dataset, \n",
    "                                       bernoulli_loss)\n",
    "\n",
    "# Plot the training and validation curves\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].plot(train_losses, color=palette[0], label=\"train\")\n",
    "axs[0].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
    "axs[0].set_xlabel(\"epoch\")\n",
    "axs[0].set_ylabel(\"bernoulli loss\")\n",
    "axs[0].set_ylim(bottom=0)\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].plot(train_losses, color=palette[0], label=\"train\")\n",
    "axs[1].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
    "axs[1].set_xlabel(\"epoch\")\n",
    "axs[1].set_ylabel(\"bernoulli loss\")\n",
    "axs[1].set_ylim(0, val_losses[50])\n",
    "axs[1].grid(True)\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJNu52VeFvGx"
   },
   "source": [
    "## Plot the resnet predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBiYTxqf8GjY"
   },
   "outputs": [],
   "source": [
    "plot_model_predictions(resnet_model, train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwSGFSB4g_iT"
   },
   "source": [
    "## Make a movie of the predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQfEmDFMGeqJ"
   },
   "outputs": [],
   "source": [
    "# Extract probabilities for each frame\n",
    "resnet_probs = []\n",
    "for t in trange(len(test_dataset)):\n",
    "    ex = test_dataset[t]\n",
    "    logits_t = resnet_model(ex[\"image\"].unsqueeze(0)).squeeze(0)\n",
    "    resnet_probs.append(from_t(torch.sigmoid(logits_t)))\n",
    "resnet_probs = np.array(resnet_probs)\n",
    "\n",
    "play_overlay(test_images, resnet_probs, prob_clip=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtI5CdTLJR9U"
   },
   "source": [
    "# Part 4: Model comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAAtgzJeO8f1"
   },
   "source": [
    "## Problem 4a: Grid search\n",
    "So far, you've trained models for only one set of hyperparameters. Perform a grid search over these hyperparameters to see if you can find better settings. The relevant hyperparameters are:\n",
    "- Basic model:\n",
    "    - `downsampling`: how much to shrink the image before applying the template\n",
    "    - `patch_size`: how big to make the patch, before downsampling\n",
    "- Resnet model:\n",
    "    - `num_bottleneck_layers`: how many bottleneck layers to apply before taking the features\n",
    "    - `kernel_size` how many \"super pixels\" to draw from when making the predictions\n",
    "\n",
    "Plot the validation loss for each configuration in your grid search.\n",
    "\n",
    "If this question seems more vague than the rest, that's intentional. Use your best judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPrU2CKZGN16"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXxYdzvWOtKi"
   },
   "source": [
    "## Problem 4b [Short Answer]: Discussion\n",
    "\n",
    "What did you find in your analyses? Were you surprised by anything in the results? Was it harder or easier than you expected to implement these pose tracking models? Speculate on how you could further improve upon these models.  \n",
    "\n",
    "_Answer below this line_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edBZ0wQzOSzt"
   },
   "source": [
    "# Submission Instructions\n",
    "\n",
    "\n",
    "Download your notebook in .ipynb format and use the following commands to convert it to PDF. Unfortunately, `nbconvert` apparently crashes with long notebooks. Here are a few options:\n",
    "\n",
    "**Formatting:** check that your code does not exceed 80 characters in line width. You can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
    "\n",
    "**Option 1 (preferred): ipynb &rarr; tex &rarr; pdf**:\n",
    "```\n",
    "jupyter nbconvert --to latex lab4_teamname.ipynb\n",
    "pdflatex lab4_teamname.tex\n",
    "```\n",
    "\n",
    "**Option 2: ipynb &rarr; html &rarr; pdf**:\n",
    "```\n",
    "jupyter nbconvert --to html lab4_teamname.ipynb\n",
    "# open lab4_teamname.html in browser and print to pdf\n",
    "```\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "- `nbconvert`: If you're using Anaconda for package management, \n",
    "```\n",
    "conda install -c anaconda nbconvert\n",
    "```\n",
    "- `pdflatex`: It comes with standard TeX distributions like TeXLive, MacTex, etc. Alternatively, you can upload the .tex and supporting files to Overleaf (free with Stanford address) and use it to compile to pdf.\n",
    "\n",
    "**Upload** your .ipynb and .pdf files to Gradescope. \n",
    "\n",
    "**Only one submission per team!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dihXYBFV0-Ld"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "STAT220/320 Lab 4: Markerless pose tracking.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
